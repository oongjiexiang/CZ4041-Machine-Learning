{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55d5848",
   "metadata": {
    "papermill": {
     "duration": 0.005759,
     "end_time": "2023-04-27T10:45:15.762503",
     "exception": false,
     "start_time": "2023-04-27T10:45:15.756744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "Source: https://www.kaggle.com/competitions/elo-merchant-category-recommendation/data\n",
    "\n",
    "### Table of Contents\n",
    "- [Libraries](#libraries)\n",
    "- [Utils](#utils)\n",
    "- [Datasets](#datasets)\n",
    "- [Machine Learning](#custom-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836af62",
   "metadata": {
    "papermill": {
     "duration": 0.004112,
     "end_time": "2023-04-27T10:45:15.771328",
     "exception": false,
     "start_time": "2023-04-27T10:45:15.767216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Libraries <a id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f53a323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T10:45:15.782716Z",
     "iopub.status.busy": "2023-04-27T10:45:15.781535Z",
     "iopub.status.idle": "2023-04-27T10:45:19.048451Z",
     "shell.execute_reply": "2023-04-27T10:45:19.047026Z"
    },
    "papermill": {
     "duration": 3.276188,
     "end_time": "2023-04-27T10:45:19.051943",
     "exception": false,
     "start_time": "2023-04-27T10:45:15.775755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "# Tools\n",
    "import math\n",
    "import datetime\n",
    "from typing import List, Union\n",
    "\n",
    "# ML Tools\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# CONSTANTS\n",
    "SEED = 123\n",
    "TEST_PERC = 0.05\n",
    "INPUT_ELO_DIR = '/kaggle/input/elo-merchant-category-recommendation'\n",
    "INPUT_PREPROCESSED_DIR = '/kaggle/input/cz4041-preprocessed'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import unittest\n",
    "import threading\n",
    "\n",
    "np.random.seed(400)\n",
    "random.seed(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f35e5",
   "metadata": {
    "papermill": {
     "duration": 0.004465,
     "end_time": "2023-04-27T10:45:19.061356",
     "exception": false,
     "start_time": "2023-04-27T10:45:19.056891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utils <a id=\"utils\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2f68f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T10:45:19.073706Z",
     "iopub.status.busy": "2023-04-27T10:45:19.073245Z",
     "iopub.status.idle": "2023-04-27T10:45:19.144405Z",
     "shell.execute_reply": "2023-04-27T10:45:19.143321Z"
    },
    "papermill": {
     "duration": 0.080969,
     "end_time": "2023-04-27T10:45:19.147156",
     "exception": false,
     "start_time": "2023-04-27T10:45:19.066187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarizeDF(df:DataFrame)->DataFrame:\n",
    "    \"\"\"This function shows a basic summary of the given dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas DataFrame\n",
    "    This specifies the dataframe to be summarized.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame: This is a table of summary of the given dataset.\n",
    "    \"\"\"    \n",
    "    variables, dtypes, count, unique, missing, pc_missing = [], [], [], [], [], []\n",
    "    \n",
    "    for item in df.columns:\n",
    "        variables.append(item)\n",
    "        dtypes.append(df[item].dtype)\n",
    "        count.append(len(df[item]))\n",
    "        unique.append(len(df[item].unique()))\n",
    "        missing.append(df[item].isna().sum())\n",
    "        pc_missing.append(round((df[item].isna().sum() / len(df[item])) * 100, 2))\n",
    "\n",
    "    output = pd.DataFrame({\n",
    "        'column_name': variables, \n",
    "        'dtype': dtypes,\n",
    "        'count': count,\n",
    "        'unique': unique,\n",
    "        'missing': missing, \n",
    "        'percentage_missing_data': pc_missing\n",
    "    })    \n",
    "        \n",
    "    return output\n",
    "\n",
    "def preprocess_data(df:DataFrame=None)->DataFrame:\n",
    "    \"\"\"This function preprocess the data into a specific form for the computation.\n",
    "    Given a DataFrame (df), impute with mode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas DataFrame\n",
    "    This specifies the data to be preprocessed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame: This specifies the preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise Exception(\"Expected a DataFrame, no DataFrame supplied.\")\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    for col in df.columns[df.isnull().any()]:\n",
    "        df_copy[col].fillna(df_copy['card_id'].map(df_copy.groupby('card_id')[col].apply(lambda x: x.mode().iloc[0] if not x.isnull().all() else np.nan)).fillna(df_copy[col].mode().iloc[0]), inplace=True)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "def feature_engineering(df:DataFrame=None)->DataFrame:\n",
    "    \"\"\"This function perform feature engineering on the input Data\"\"\"\n",
    "    \n",
    "    def get_new_columns(name:str, aggs:list)->list: # Nested function for feature engineering\n",
    "        \"\"\"This function creates new column names for the aggregation of the features.\"\"\"\n",
    "        return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    \n",
    "    # Make copy of df\n",
    "    df_historical_transactions_copy = df.copy()\n",
    " \n",
    "    # Convert DT columns to Pandas DT\n",
    "    df_historical_transactions_copy['purchase_date'] = pd.to_datetime(df_historical_transactions_copy['purchase_date'])\n",
    " \n",
    "    # Feature Engineer columns from purchase_date\n",
    "    df_historical_transactions_copy['year'] = df_historical_transactions_copy['purchase_date'].dt.year\n",
    "    df_historical_transactions_copy['weekofyear'] = df_historical_transactions_copy['purchase_date'].dt.isocalendar().week\n",
    "    df_historical_transactions_copy['month'] = df_historical_transactions_copy['purchase_date'].dt.month\n",
    "    df_historical_transactions_copy['dayofweek'] = df_historical_transactions_copy['purchase_date'].dt.dayofweek\n",
    "    df_historical_transactions_copy['weekend'] = (df_historical_transactions_copy.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df_historical_transactions_copy['hour'] = df_historical_transactions_copy['purchase_date'].dt.hour\n",
    " \n",
    "    # Encode Binary Features\n",
    "    df_historical_transactions_copy['authorized_flag'] = df_historical_transactions_copy['authorized_flag'].map({\"Y\":1, 'N':0})\n",
    "    df_historical_transactions_copy['category_1'] = df_historical_transactions_copy['category_1'].map({'Y':1, 'N':0})\n",
    " \n",
    "    # Feature Engineer Month Diff/Lag\n",
    "    df_historical_transactions_copy['month_diff'] = ((datetime.datetime.today() - df_historical_transactions_copy['purchase_date']).dt.days)//30\n",
    "    df_historical_transactions_copy['month_diff'] += df_historical_transactions_copy['month_lag']\n",
    "    \n",
    "    # Getting Centrality of the Data\n",
    "    aggs = {}\n",
    "    for col in ['month','hour','weekofyear','dayofweek','year', 'state_id','subsector_id']:\n",
    "        aggs[col] = ['nunique']\n",
    " \n",
    "    # Feature Engineering using Univariate Analysis\n",
    "    aggs['authorized_flag'] = ['sum', 'mean']\n",
    "    aggs['card_id'] = ['size']\n",
    "    aggs['category_1'] = ['sum', 'mean']\n",
    "    aggs['installments'] = ['sum','max','min','mean','var']\n",
    "    aggs['month_lag'] = ['max','min','mean','var']\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_diff'] = ['mean']\n",
    "    aggs['weekend'] = ['sum', 'mean']\n",
    " \n",
    "    for col in ['category_2','category_3']:\n",
    "        df_historical_transactions_copy[col+'_mean'] = df_historical_transactions_copy.groupby([col])['purchase_amount'].transform('mean')\n",
    "        aggs[col+'_mean'] = ['mean']    \n",
    " \n",
    "    new_columns = get_new_columns('hist',aggs)\n",
    "    \n",
    "    # Group Aggregations by card_id\n",
    "    df_historical_transactions_copy_group = df_historical_transactions_copy.groupby('card_id').agg(aggs)\n",
    " \n",
    "    # Remove Multilevel Indexing with New Column Names\n",
    "    df_historical_transactions_copy_group.columns = new_columns\n",
    "    \n",
    "    # Reset Index\n",
    "    df_historical_transactions_copy_group.reset_index(drop=False,inplace=True)\n",
    "    \n",
    "    # Cast variable to pandas Datetime\n",
    "    df_historical_transactions_copy_group['hist_purchase_date_max'] = pd.to_datetime(df_historical_transactions_copy_group['hist_purchase_date_max'])\n",
    "    df_historical_transactions_copy_group['hist_purchase_date_min'] = pd.to_datetime(df_historical_transactions_copy_group['hist_purchase_date_min'])\n",
    "\n",
    "    return df_historical_transactions_copy_group\n",
    "\n",
    "def merge_data(key:str=None, dfs:List[DataFrame]=None)->DataFrame:\n",
    "    \"\"\"This function takes in multiple dataframes and performs a left outer join on a key.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    key: str\n",
    "    This species the joining key.\n",
    "    \n",
    "    dfs: list of pandas DataFrame\n",
    "    This specifies the list of DataFrames to perform left outer join based on a key.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame: This specifies the resultant DataFrame from the merging operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanity Check\n",
    "    if key is None:\n",
    "        raise Exception(\"Expected a key, no key supplied.\")\n",
    "        \n",
    "    if not isinstance(key, str):\n",
    "        raise Exception(f\"Expected type str for key, {type(key)} was supplied.\")\n",
    "    \n",
    "    if dfs is None or not len(dfs) == 2:\n",
    "        raise Exception(\"Expected at least two DataFrame.\")\n",
    "        \n",
    "    if any(type(x) != DataFrame for x in dfs):\n",
    "        raise Exception(\"At least one DataFrame is not the correct DataType.\")\n",
    "        \n",
    "    # Iterate through DataFrames to perform merge operation\n",
    "    df_res = dfs[0]\n",
    "    \n",
    "    for df in dfs[1:]:\n",
    "        df_res = pd.merge(left=df_res, right=df, how='left', left_on=key, right_on=key)\n",
    "    \n",
    "    return df_res\n",
    "\n",
    "def zhenjie_miracle(df:DataFrame)->DataFrame:\n",
    "    \"\"\"This function perform feature engineering on purchase_max and purchase_min and one-hot encoding on the \n",
    "    input Data which must be a merged dataframe of train dataset and trans (hist and new) dataset after running \n",
    "    feature_engineering function\"\"\"\n",
    "    \n",
    "    # Make copy of df\n",
    "    df_historical_transactions_copy_group = df.copy()\n",
    "\n",
    "    brazil_holiday_list=[ \n",
    "            '01-01-17', '14-02-17', '28-08-17', '14-04-17', '16-04-17', '21-04-17',\n",
    "            '01-05-17', '15-06-17', '07-09-17', '12-10-17', '02-11-17', '15-11-17', \n",
    "            '24-12-17', '25-12-17', '31-12-17',\n",
    "            '01-01-18', '14-02-18', '28-08-18', '14-04-18', '16-04-18', '21-04-18',\n",
    "            '01-05-18', '15-06-18', '07-09-18', '12-10-18', '02-11-18', '15-11-18', \n",
    "            '24-12-18', '25-12-18', '31-12-18'\n",
    "      ]\n",
    "    df_historical_transactions_copy_group['purchase_max_is_holiday'] = df_historical_transactions_copy_group['hist_purchase_date_max'].isin(brazil_holiday_list).astype(int)\n",
    "    df_historical_transactions_copy_group['purchase_min_is_holiday'] = df_historical_transactions_copy_group['hist_purchase_date_min'].isin(brazil_holiday_list).astype(int)\n",
    "    \n",
    "    df_historical_transactions_copy_group_dummies = pd.get_dummies(df_historical_transactions_copy_group['feature_1'], prefix='feature_1', drop_first=True)\n",
    "    df_historical_transactions_copy_group = pd.concat([df_historical_transactions_copy_group, df_historical_transactions_copy_group_dummies], axis=1)\n",
    "    df_historical_transactions_copy_group_dummies = pd.get_dummies(df_historical_transactions_copy_group['feature_2'], prefix='feature_2', drop_first=True)\n",
    "    df_historical_transactions_copy_group = pd.concat([df_historical_transactions_copy_group, df_historical_transactions_copy_group_dummies], axis=1)\n",
    "\n",
    "    return df_historical_transactions_copy_group\n",
    "\n",
    "\n",
    "def pengaik_miracle(df:DataFrame=None)->DataFrame:\n",
    "    \n",
    "    \"\"\"This function perform feature engineering on average monthly purchase amount raw of positive month lags \n",
    "    over that of negative. It also performs feature engineering on the ratio of purchase amount raw of \n",
    "    month_lag=i/month_lag=i-1 for each card_id and returns the average ratio as a column\n",
    "    input Data which must be a concat dataframe of trans (hist and new) dataset \"\"\"\n",
    "    \n",
    "    transactions_copy = df.copy()\n",
    "    \n",
    "    # Reverse purchase_amount\n",
    "    transactions_copy['purchase_amount_raw'] = np.round(transactions_copy['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
    "\n",
    "    # Group transactions_copy by card_id and month_lag\n",
    "    grouped_transactions_copy = transactions_copy.groupby(['card_id', 'month_lag']).agg({'purchase_amount_raw': 'mean'}).reset_index()\n",
    "\n",
    "    # Separate transactions_copy into two groups based on month_lag\n",
    "    lag_le_0 = grouped_transactions_copy[grouped_transactions_copy['month_lag'] <= 0]\n",
    "    lag_gt_0 = grouped_transactions_copy[grouped_transactions_copy['month_lag'] > 0]\n",
    "\n",
    "    # Calculate the monthly average purchase amount for each group\n",
    "    lag_le_0_monthly_average_raw = lag_le_0.groupby('card_id')['purchase_amount_raw'].mean().reset_index().rename(columns={'purchase_amount_raw': 'monthly_average_purchase_amount_raw_for_month_lag_le_0'})\n",
    "    lag_gt_0_monthly_average_raw = lag_gt_0.groupby('card_id')['purchase_amount_raw'].mean().reset_index().rename(columns={'purchase_amount_raw': 'monthly_average_purchase_amount_raw_for_month_lag_gt_0'})\n",
    "\n",
    "    # Merge the new columns with the original transactions_copy dataframe\n",
    "    transactions_copy = transactions_copy.merge(lag_le_0_monthly_average_raw, on='card_id', how='left')\n",
    "    transactions_copy = transactions_copy.merge(lag_gt_0_monthly_average_raw, on='card_id', how='left')\n",
    "\n",
    "    transactions_copy['ratio_between_ave_monthly_purchase_raw_for_positive_and_negative'] = transactions_copy['monthly_average_purchase_amount_raw_for_month_lag_gt_0'] / transactions_copy['monthly_average_purchase_amount_raw_for_month_lag_le_0']\n",
    "\n",
    "    # Find the minimum month_lag for each card_id and set the index to 'card_id'\n",
    "    min_month_lag_per_card = transactions_copy.groupby('card_id', as_index=False)['month_lag'].min().set_index('card_id')\n",
    "\n",
    "    # Fill in missing month_lag values for each card_id\n",
    "    unique_card_ids = transactions_copy['card_id'].unique()\n",
    "    min_month_lag = transactions_copy['month_lag'].min()\n",
    "    max_month_lag = transactions_copy['month_lag'].max()\n",
    "\n",
    "    complete_data = []\n",
    "\n",
    "    for card_id in unique_card_ids:\n",
    "        # Use .loc[] accessor to look up the minimum month_lag for each card_id\n",
    "        card_min_month_lag = min_month_lag_per_card.loc[card_id]['month_lag']\n",
    "        for month_lag in range(card_min_month_lag, max_month_lag + 1):\n",
    "            complete_data.append([card_id, month_lag, 0])\n",
    "\n",
    "    complete_transactions_copy = pd.DataFrame(complete_data, columns=['card_id', 'month_lag', 'purchase_amount_raw'])\n",
    "\n",
    "    # Compute the purchase_amount_raw sum for each card_id and month_lag combination\n",
    "    grouped_transactions_copy = transactions_copy.groupby(['card_id', 'month_lag'], as_index=False)['purchase_amount_raw'].sum()\n",
    "\n",
    "    # Merge the complete_transactions_copy dataframe with the grouped_transactions_copy dataframe\n",
    "    merged_transactions_copy = pd.merge(complete_transactions_copy, grouped_transactions_copy, on=['card_id', 'month_lag'], how='left', suffixes=('', '_y'))\n",
    "    merged_transactions_copy['purchase_amount_raw'] = merged_transactions_copy['purchase_amount_raw_y'].fillna(merged_transactions_copy['purchase_amount_raw'])\n",
    "\n",
    "    # Calculate the ratio of purchase_amount_raw for each month_lag=i/month_lag=i-1\n",
    "    merged_transactions_copy['prev_month_purchase_amount'] = merged_transactions_copy.groupby('card_id')['purchase_amount_raw'].shift(1)\n",
    "    merged_transactions_copy['ratio'] = np.where(merged_transactions_copy['prev_month_purchase_amount'] != 0, merged_transactions_copy['purchase_amount_raw'] / merged_transactions_copy['prev_month_purchase_amount'], np.nan)\n",
    "\n",
    "    # Compute the average of these ratios for each card_id\n",
    "    average_ratios = merged_transactions_copy.groupby('card_id', as_index=False)['ratio'].mean()\n",
    "\n",
    "    # Handling division by zero cases by replacing np.inf with np.nan and then replacing np.nan with a suitable value (e.g., 1)\n",
    "    average_ratios['ratio'] = average_ratios['ratio'].replace([np.inf, -np.inf], np.nan).fillna(1)\n",
    "\n",
    "    # Merge average_ratios with transactions_copy DataFrame\n",
    "    feature_engineered_transactions_copy = transactions_copy.merge(average_ratios, on='card_id', how='left')\n",
    "\n",
    "    return feature_engineered_transactions_copy\n",
    "\n",
    "\n",
    "\n",
    "ModelRegressor = Union[LinearRegression, DecisionTreeRegressor, RandomForestRegressor]\n",
    "def feature_selection(approach:str=\"RFE\", \n",
    "                      k:int=10, \n",
    "                      train:DataFrame=None, \n",
    "                      test:DataFrame=None,\n",
    "                      model:ModelRegressor=None)->List[str]:\n",
    "    \"\"\"This function performs feature selection based on the user's choice of approach.\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "    >> features = feature_selection(approach=\"LGBM\", train=X_train, test=y_train)\n",
    "    \"\"\"\n",
    "    if approach == 'LGBM':\n",
    "        # LGTM Regressor to pick out important features\n",
    "        gbm = lgb.LGBMRegressor()\n",
    "        gbm.fit(train, test)\n",
    "\n",
    "        # Feature Important Viz\n",
    "        fea_imp_ = pd.DataFrame({'variable':train.columns, 'feature_importance':gbm.feature_importances_})\n",
    "        return list(fea_imp_[:k]['variable'])\n",
    "    if approach == 'RFE':\n",
    "        rfe = RFE(estimator=model, n_features_to_select=k)\n",
    "        rfe = rfe.fit(train, test)\n",
    "\n",
    "        # summarize the ranking of the attributes\n",
    "        fea_rank_ = pd.DataFrame({'variable': train.columns, 'feature_importance':rfe.ranking_})\n",
    "        return list(fea_rank_[:k]['variable'])\n",
    "\n",
    "    \n",
    "def build_train_test_sets(df:DataFrame=None, \n",
    "                          features:List[str]=None, \n",
    "                          target:str=None, \n",
    "                          verbose:int=0, \n",
    "                          **kwargs:dict)->List[DataFrame]:\n",
    "    \"\"\"This function splits the given dataframe into train and test data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        This specifies the source DataFrame.\n",
    "\n",
    "        features: list of str\n",
    "        This list containing a str-type elements specifies the name of the features.\n",
    "\n",
    "        target: str\n",
    "        This specifies the target variable.\n",
    "        \n",
    "        verbose: int-type\n",
    "            This species the verbosity of the function.\n",
    "    \n",
    "    Kwargs\n",
    "        A dict mapping the corresponding parameters for scikit learn model selection. \n",
    "        \n",
    "        {\"test_size\": 0.05, \"seed\": None}\n",
    "    \n",
    "        If a key from the keys argument is missing from the settings, the default will be used.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list: This species the list containing X_train, X_test, y_test and y_train DataFrame.\n",
    "    \"\"\"\n",
    "    # Default \n",
    "    model_params = {\n",
    "        'test_size': 0.05,\n",
    "        \"seed\": None\n",
    "    }\n",
    "    \n",
    "    # Sanity Check\n",
    "    if df is None:\n",
    "        raise ValueError(\"Expected a DataFrame, no DataFrame supplied.\")\n",
    "        \n",
    "    if features is None:\n",
    "        raise Exception(\"Expected a features list, no features list supplied.\")\n",
    "    \n",
    "    if not isinstance(features, list):\n",
    "        raise Exception(f\"Expected list datatype for features, {type(features)} was supplied.\")\n",
    "        \n",
    "    if not isinstance(target, str):\n",
    "        raise Exception(f\"Expected str datatype for target, {type(target)} was supplied.\")\n",
    "        \n",
    "    # Check for Kwargs\n",
    "    if \"test_size\" in kwargs:\n",
    "        model_params['test_size'] = kwargs[\"test_size\"]\n",
    "    if \"seed\" in kwargs:\n",
    "        model_params['seed'] = kwargs[\"seed\"]\n",
    "        \n",
    "    seed = \"No Seed\" if model_params[\"seed\"] is None else model_params['seed']\n",
    "    if verbose != 0:\n",
    "        print(f\"***Parameters for Model Selection***\\ntest_size {model_params['test_size']}\\nseed: {seed}\\n\")\n",
    "    \n",
    "    if seed != \"No Seed\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[features], df[[target]], test_size=model_params['test_size'])\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[features], df[[target]], test_size=model_params['test_size'], random_state=model_params['seed'])\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def train_eval_model(model:ModelRegressor,  X_train, X_test, y_train, y_test, name)->DataFrame:\n",
    "    \"\"\"This function trains and evaluates the model.\n",
    "    By default, the score used it RMSE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Args:\n",
    "        model: model\n",
    "        This species the model to use for training and evaluation.\n",
    "       \n",
    "    Kwargs\n",
    "        A dict mapping the corresponding parameters for training and test data. \n",
    "        \n",
    "        {\"X_train\": ..., \"X_test\": ..., \"y_train\": ..., \"y_test\": ...}\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    dataframe: This specifies resets from testing the model.\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Pred\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Eval\n",
    "    print(f\"{name} Score:\", math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def Bayesian_Optimization(objective_function, parameters_dict, n_init_random_explorations=10, n_iter = 50):\n",
    "    \"\"\"Find the hyperparameters that maximizes a given objective (e.g test result)\n",
    "    Parameters\n",
    "    ----------\n",
    "    Args:\n",
    "        objective_function: function\n",
    "        function that outputs a value, which BO will try to maximize. \n",
    "\n",
    "        parameters_dict: Dictionary\n",
    "        Contain hyperparameters that you want to optimize. Key is hyperparameter name, value is (min, max) value of that hyperparameters\n",
    "        \n",
    "        n_init_random_explorations: int\n",
    "        Number of random sets of hyperparameters to try. <n_init_random_explorations> random hp sets are explored before <n_iter> systematic explorations are run\n",
    "        \n",
    "        n_iter: int\n",
    "        Number of iterations to run\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict: Best set of hyperparameters\n",
    "\n",
    "\n",
    "    Usage\n",
    "    -------\n",
    "    The key in pbounds must match the parameters in objective_function!!!\n",
    "\n",
    "    def objective_function(n, beta, gamma) -> float:\n",
    "      n = int(n)\n",
    "      model = BetaVae(\n",
    "          n_latent=n,\n",
    "          beta=beta,\n",
    "          n_chan=N_CHAN,\n",
    "          input_d=INPUT_DIM,\n",
    "          batch=BATCH,\n",
    "          gamma = gamma,\n",
    "          )\n",
    "      model.train_self(\n",
    "          data_path=TRAIN_PATH,\n",
    "          epochs=1,\n",
    "          weights_file=f'bvae_n{n}_b{beta}_{\"bw\" if N_CHAN == 1 else \"\"}_'\n",
    "                      f'{INPUT_DIM[0]}x{INPUT_DIM[1]}.pt')\n",
    "      return model.test(TEST_PATH, iters=1)\n",
    "\n",
    "    parameters_dict = {'n': (5, 200), 'beta': (0.1,30), 'gamma': (0.001, 30)}\n",
    "\n",
    "    def Bayesian_Optimization(objective_function, parameters_dict, n_iter = 50):\n",
    "      optimizer = BayesianOptimization(\n",
    "          f=objective_function,\n",
    "          pbounds= parameters_dict,\n",
    "          verbose=2,\n",
    "          random_state=1)\n",
    "    \n",
    "    best_hypers = Bayesian_Optimization(objective_function, parameters_dict, n_iter = 50)\n",
    "    \"\"\"\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective_function, #define before this function\n",
    "        pbounds=parameters_dict, \n",
    "        random_state=1)\n",
    "    optimizer.maximize(init_points=n_init_random_explorations, n_iter=n_iter)\n",
    "    print('#################################################################')\n",
    "    print(f'Found Network with Optimal target result of {optimizer.max[\"target\"]}')\n",
    "    print(f'Parameters: {optimizer.max[\"params\"]}')\n",
    "    print('#################################################################')\n",
    "    return optimizer.max[\"params\"]\n",
    "\n",
    "\n",
    "def createData(df:DataFrame=None, df_t:DataFrame=None)->DataFrame:\n",
    "    \"\"\"This function transform the given datasets into a suitable dataset for training/testing.\n",
    "    \n",
    "    algorithm\n",
    "        0. (optional) Subset the data based on card_ids in train/test (improve performance)\n",
    "        1. PA Miracle\n",
    "        2. Impute with Mode\n",
    "        3. Feature Engineering\n",
    "        4. Merge df_transactions from 1 - 3 to train/test (IMPORTANT)\n",
    "        5. ZJ Miracle\n",
    "        6. Remove unnecessary columns\n",
    "    endalgorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "    This specifies the dataframe containing transactions details. Ideally, this should be a combination of \n",
    "    new_historical and historical transactions datagframes.\n",
    "    \n",
    "    df_t: DataFrame\n",
    "    This specifies the train or test dataframe.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame: Valid DataFrame after the preprocessing and imputations\n",
    "    \"\"\"\n",
    "    # Run Pengaik's Miracle\n",
    "    df_transactions_t = pengaik_miracle(df)\n",
    "    print(\"Card_ID Uniqueness (PA Miracle)\", len(df_transactions_t['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Impute with Mode\n",
    "    df_impute_mode = preprocess_data(df=df_transactions_t)\n",
    "    print(\"Card_ID Uniqueness (Impute Mode)\", len(df_impute_mode['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Store PA's Ratios\n",
    "    df_ratios = df_impute_mode[['card_id',\n",
    "       'monthly_average_purchase_amount_raw_for_month_lag_le_0',\n",
    "       'monthly_average_purchase_amount_raw_for_month_lag_gt_0',\n",
    "       'ratio_between_ave_monthly_purchase_raw_for_positive_and_negative',\n",
    "       'ratio']].drop_duplicates()\n",
    "\n",
    "    # Sanity Check for n_rows and uniqueness of card_id\n",
    "    print(\"Card_ID Uniqueness\", len(df_ratios['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "    \n",
    "    # Sanity Check for n_rows and uniqueness of card_id\n",
    "    print(\"Card_ID Uniqueness (PA Ratio)\", len(df_ratios['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "    \n",
    "    # Feature Engineering\n",
    "    df_aggregated_cols = feature_engineering(df_impute_mode)\n",
    "    print(\"Card_ID Uniqueness (Feature Engineering)\", len(df_aggregated_cols['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Merge PA's Ratio\n",
    "    df_aggregated_cols = merge_data('card_id', [df_ratios, df_aggregated_cols])\n",
    "\n",
    "    # Sanity Check for n_rows and uniqueness of card_id\n",
    "    print(\"Card_ID Uniqueness (Merge PA Ratio)\", len(df_aggregated_cols['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "\n",
    "    # Merge Transactions and T\n",
    "    df_t_merge = merge_data('card_id', [df_t, df_aggregated_cols])\n",
    "    print(\"Card_ID Uniqueness (Merging)\", len(df_t_merge['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Execute Zhen Jie's Miracle\n",
    "    df_t_merge = zhenjie_miracle(df_t_merge)\n",
    "    print(\"Card_ID Uniqueness (ZJ Miracle)\", len(df_t_merge['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Impute Missing Data\n",
    "    df_t_merge.fillna(\"2017-01\", inplace=True)\n",
    "\n",
    "    # Engineered by Zhen Jie so Remove\n",
    "    df_t_merge.drop(columns=['hist_purchase_date_max', 'hist_purchase_date_min'], inplace=True)\n",
    "    print(\"Card_ID Uniqueness (Imputations and Drop)\", len(df_test_merge['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "    \n",
    "    # Final Sanity Check\n",
    "    print(\"***FINAL***\")\n",
    "    print('n_rows:', format(df_t_merge.shape[0], \"_\"), end='\\n\\n')\n",
    "    print(\"Columns:\\n\", \", \".join(df_t_merge.columns), sep='')\n",
    "    \n",
    "    return df_t_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543d30b",
   "metadata": {
    "papermill": {
     "duration": 0.004594,
     "end_time": "2023-04-27T10:45:19.156700",
     "exception": false,
     "start_time": "2023-04-27T10:45:19.152106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Datasets <a id=\"datasets\"></a>\n",
    "\n",
    "1. Customer has a ```card_id``` as uuid.\n",
    "2. Each customer can make at least one transaction to merchants.\n",
    "3. Merchant has ```merchant_id``` as uuid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95dadc2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T10:45:19.167874Z",
     "iopub.status.busy": "2023-04-27T10:45:19.167413Z",
     "iopub.status.idle": "2023-04-27T10:47:28.045932Z",
     "shell.execute_reply": "2023-04-27T10:47:28.040929Z"
    },
    "papermill": {
     "duration": 128.897181,
     "end_time": "2023-04-27T10:47:28.058558",
     "exception": false,
     "start_time": "2023-04-27T10:45:19.161377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Historical Transactions\n",
    "tp = pd.read_csv(f'{INPUT_ELO_DIR}/historical_transactions.csv', iterator=True, chunksize=2_000_000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\n",
    "df_historical_transactions = pd.concat(tp, ignore_index=True) \n",
    "\n",
    "# New Historical Transactions\n",
    "tp = pd.read_csv(f'{INPUT_ELO_DIR}/new_merchant_transactions.csv', iterator=True, chunksize=2_000_000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\n",
    "df_new_historical_transactions = pd.concat(tp, ignore_index=True) \n",
    "\n",
    "# Train Data\n",
    "df_train = pd.read_csv(f'{INPUT_ELO_DIR}/train.csv')\n",
    "\n",
    "# Engineered Train Data \n",
    "tp = pd.read_csv(f'{INPUT_PREPROCESSED_DIR}/output.csv',index_col=0, iterator=True, chunksize=5_000_000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\n",
    "df_train_merge = pd.concat(tp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5486b05",
   "metadata": {
    "papermill": {
     "duration": 0.01657,
     "end_time": "2023-04-27T10:47:28.093717",
     "exception": false,
     "start_time": "2023-04-27T10:47:28.077147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HP Tuning of LGBM Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86df620b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T10:47:28.133848Z",
     "iopub.status.busy": "2023-04-27T10:47:28.132209Z",
     "iopub.status.idle": "2023-04-27T10:47:28.317704Z",
     "shell.execute_reply": "2023-04-27T10:47:28.315788Z"
    },
    "papermill": {
     "duration": 0.209994,
     "end_time": "2023-04-27T10:47:28.321392",
     "exception": false,
     "start_time": "2023-04-27T10:47:28.111398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Define LightGBM cross-validation function\n",
    "features = list(df_train_merge.drop(columns=['target', 'card_id']).columns)\n",
    "\n",
    "def lgbm_objfunc(num_leaves, learning_rate, n_estimators, max_depth, min_split_gain, min_child_weight):\n",
    "    # Define LightGBM model with specified hyperparameters\n",
    "    model = lgb.LGBMRegressor(\n",
    "        num_leaves=int(num_leaves),\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=int(n_estimators),\n",
    "        max_depth=int(max_depth),\n",
    "        min_split_gain=min_split_gain,\n",
    "        min_child_weight=min_child_weight,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    # Cross-validation with LightGBM\n",
    "    rmse = np.sqrt(-cross_val_score(model, df_train_merge[features], df_train_merge['target'], scoring=\"neg_mean_squared_error\", cv=10))\n",
    "    \n",
    "    # Return the mean of RMSE scores\n",
    "    return -rmse.mean()\n",
    "\n",
    "# Define hyperparameter ranges for Bayesian Optimization\n",
    "pbounds = {\n",
    "    'num_leaves': (5, 50),\n",
    "    'learning_rate': (0.01, 0.5),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3, 10),\n",
    "    'min_split_gain': (0.001, 0.1),\n",
    "    'min_child_weight': (5, 50)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc8aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:33:35.171448Z",
     "iopub.status.busy": "2023-04-12T02:33:35.171031Z",
     "iopub.status.idle": "2023-04-12T02:54:36.825310Z",
     "shell.execute_reply": "2023-04-12T02:54:36.824378Z",
     "shell.execute_reply.started": "2023-04-12T02:33:35.171417Z"
    },
    "papermill": {
     "duration": 0.004589,
     "end_time": "2023-04-27T10:47:28.333368",
     "exception": false,
     "start_time": "2023-04-27T10:47:28.328779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This might be the model that generates 3.69 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36690956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T10:47:28.345889Z",
     "iopub.status.busy": "2023-04-27T10:47:28.344494Z",
     "iopub.status.idle": "2023-04-27T18:05:45.005350Z",
     "shell.execute_reply": "2023-04-27T18:05:45.001666Z"
    },
    "papermill": {
     "duration": 26296.672494,
     "end_time": "2023-04-27T18:05:45.010716",
     "exception": false,
     "start_time": "2023-04-27T10:47:28.338222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | min_ch... | min_sp... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-3.69    \u001b[0m | \u001b[0m0.2143   \u001b[0m | \u001b[0m8.042    \u001b[0m | \u001b[0m5.005    \u001b[0m | \u001b[0m0.03093  \u001b[0m | \u001b[0m232.1    \u001b[0m | \u001b[0m9.155    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-3.693   \u001b[0m | \u001b[0m0.1013   \u001b[0m | \u001b[0m5.419    \u001b[0m | \u001b[0m22.85    \u001b[0m | \u001b[0m0.05434  \u001b[0m | \u001b[0m477.3    \u001b[0m | \u001b[0m35.83    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-3.707   \u001b[0m | \u001b[0m0.1102   \u001b[0m | \u001b[0m9.147    \u001b[0m | \u001b[0m6.232    \u001b[0m | \u001b[0m0.06738  \u001b[0m | \u001b[0m475.6    \u001b[0m | \u001b[0m30.14    \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m-3.681   \u001b[0m | \u001b[95m0.07879  \u001b[0m | \u001b[95m4.387    \u001b[0m | \u001b[95m41.03    \u001b[0m | \u001b[95m0.09686  \u001b[0m | \u001b[95m382.1    \u001b[0m | \u001b[95m36.15    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-3.905   \u001b[0m | \u001b[0m0.4394   \u001b[0m | \u001b[0m9.262    \u001b[0m | \u001b[0m8.827    \u001b[0m | \u001b[0m0.004866 \u001b[0m | \u001b[0m252.8    \u001b[0m | \u001b[0m44.52    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-3.683   \u001b[0m | \u001b[0m0.05819  \u001b[0m | \u001b[0m5.948    \u001b[0m | \u001b[0m48.11    \u001b[0m | \u001b[0m0.05378  \u001b[0m | \u001b[0m722.7    \u001b[0m | \u001b[0m19.2     \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-3.965   \u001b[0m | \u001b[0m0.3464   \u001b[0m | \u001b[0m8.842    \u001b[0m | \u001b[0m5.823    \u001b[0m | \u001b[0m0.07526  \u001b[0m | \u001b[0m990.0    \u001b[0m | \u001b[0m38.67    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-3.729   \u001b[0m | \u001b[0m0.1474   \u001b[0m | \u001b[0m8.525    \u001b[0m | \u001b[0m9.645    \u001b[0m | \u001b[0m0.04534  \u001b[0m | \u001b[0m917.7    \u001b[0m | \u001b[0m18.21    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-3.685   \u001b[0m | \u001b[0m0.151    \u001b[0m | \u001b[0m3.91     \u001b[0m | \u001b[0m5.872    \u001b[0m | \u001b[0m0.0682   \u001b[0m | \u001b[0m290.5    \u001b[0m | \u001b[0m16.95    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-3.7     \u001b[0m | \u001b[0m0.2509   \u001b[0m | \u001b[0m3.374    \u001b[0m | \u001b[0m30.84    \u001b[0m | \u001b[0m0.01553  \u001b[0m | \u001b[0m630.4    \u001b[0m | \u001b[0m36.49    \u001b[0m |\n",
      "| \u001b[95m11       \u001b[0m | \u001b[95m-3.678   \u001b[0m | \u001b[95m0.06014  \u001b[0m | \u001b[95m5.898    \u001b[0m | \u001b[95m36.25    \u001b[0m | \u001b[95m0.042    \u001b[0m | \u001b[95m145.0    \u001b[0m | \u001b[95m29.12    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-3.789   \u001b[0m | \u001b[0m0.3353   \u001b[0m | \u001b[0m6.604    \u001b[0m | \u001b[0m47.51    \u001b[0m | \u001b[0m0.05907  \u001b[0m | \u001b[0m913.1    \u001b[0m | \u001b[0m11.19    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-3.696   \u001b[0m | \u001b[0m0.07825  \u001b[0m | \u001b[0m8.652    \u001b[0m | \u001b[0m22.9     \u001b[0m | \u001b[0m0.01737  \u001b[0m | \u001b[0m934.8    \u001b[0m | \u001b[0m20.65    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-3.882   \u001b[0m | \u001b[0m0.3779   \u001b[0m | \u001b[0m8.082    \u001b[0m | \u001b[0m44.75    \u001b[0m | \u001b[0m0.06274  \u001b[0m | \u001b[0m775.8    \u001b[0m | \u001b[0m20.7     \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-3.738   \u001b[0m | \u001b[0m0.1423   \u001b[0m | \u001b[0m9.271    \u001b[0m | \u001b[0m24.26    \u001b[0m | \u001b[0m0.09652  \u001b[0m | \u001b[0m697.1    \u001b[0m | \u001b[0m32.98    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-3.681   \u001b[0m | \u001b[0m0.06623  \u001b[0m | \u001b[0m9.646    \u001b[0m | \u001b[0m25.25    \u001b[0m | \u001b[0m0.05826  \u001b[0m | \u001b[0m467.3    \u001b[0m | \u001b[0m15.67    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-3.909   \u001b[0m | \u001b[0m0.4527   \u001b[0m | \u001b[0m7.016    \u001b[0m | \u001b[0m5.129    \u001b[0m | \u001b[0m0.0621   \u001b[0m | \u001b[0m394.0    \u001b[0m | \u001b[0m28.72    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-3.736   \u001b[0m | \u001b[0m0.4441   \u001b[0m | \u001b[0m5.501    \u001b[0m | \u001b[0m45.88    \u001b[0m | \u001b[0m0.06271  \u001b[0m | \u001b[0m114.2    \u001b[0m | \u001b[0m46.82    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-3.95    \u001b[0m | \u001b[0m0.3485   \u001b[0m | \u001b[0m9.981    \u001b[0m | \u001b[0m12.76    \u001b[0m | \u001b[0m0.01458  \u001b[0m | \u001b[0m939.3    \u001b[0m | \u001b[0m36.36    \u001b[0m |\n",
      "| \u001b[95m20       \u001b[0m | \u001b[95m-3.677   \u001b[0m | \u001b[95m0.04234  \u001b[0m | \u001b[95m8.288    \u001b[0m | \u001b[95m38.92    \u001b[0m | \u001b[95m0.09238  \u001b[0m | \u001b[95m740.4    \u001b[0m | \u001b[95m10.59    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-3.689   \u001b[0m | \u001b[0m0.01974  \u001b[0m | \u001b[0m3.183    \u001b[0m | \u001b[0m6.274    \u001b[0m | \u001b[0m0.02537  \u001b[0m | \u001b[0m874.0    \u001b[0m | \u001b[0m29.25    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-3.873   \u001b[0m | \u001b[0m0.2809   \u001b[0m | \u001b[0m8.894    \u001b[0m | \u001b[0m10.59    \u001b[0m | \u001b[0m0.02864  \u001b[0m | \u001b[0m627.2    \u001b[0m | \u001b[0m48.63    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-3.711   \u001b[0m | \u001b[0m0.2849   \u001b[0m | \u001b[0m3.131    \u001b[0m | \u001b[0m41.03    \u001b[0m | \u001b[0m0.02406  \u001b[0m | \u001b[0m826.4    \u001b[0m | \u001b[0m22.45    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-3.73    \u001b[0m | \u001b[0m0.4331   \u001b[0m | \u001b[0m8.23     \u001b[0m | \u001b[0m30.03    \u001b[0m | \u001b[0m0.01451  \u001b[0m | \u001b[0m153.9    \u001b[0m | \u001b[0m10.46    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-3.691   \u001b[0m | \u001b[0m0.03183  \u001b[0m | \u001b[0m3.752    \u001b[0m | \u001b[0m15.16    \u001b[0m | \u001b[0m0.07159  \u001b[0m | \u001b[0m603.7    \u001b[0m | \u001b[0m5.565    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-3.681   \u001b[0m | \u001b[0m0.04527  \u001b[0m | \u001b[0m9.771    \u001b[0m | \u001b[0m30.56    \u001b[0m | \u001b[0m0.02113  \u001b[0m | \u001b[0m327.1    \u001b[0m | \u001b[0m38.47    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-3.687   \u001b[0m | \u001b[0m0.1058   \u001b[0m | \u001b[0m7.07     \u001b[0m | \u001b[0m48.65    \u001b[0m | \u001b[0m0.08484  \u001b[0m | \u001b[0m315.9    \u001b[0m | \u001b[0m27.22    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-3.745   \u001b[0m | \u001b[0m0.3138   \u001b[0m | \u001b[0m8.803    \u001b[0m | \u001b[0m12.06    \u001b[0m | \u001b[0m0.002839 \u001b[0m | \u001b[0m163.0    \u001b[0m | \u001b[0m26.89    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-3.813   \u001b[0m | \u001b[0m0.3071   \u001b[0m | \u001b[0m6.982    \u001b[0m | \u001b[0m19.28    \u001b[0m | \u001b[0m0.09887  \u001b[0m | \u001b[0m621.8    \u001b[0m | \u001b[0m22.11    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-3.719   \u001b[0m | \u001b[0m0.28     \u001b[0m | \u001b[0m8.217    \u001b[0m | \u001b[0m35.12    \u001b[0m | \u001b[0m0.02723  \u001b[0m | \u001b[0m159.7    \u001b[0m | \u001b[0m21.65    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-3.721   \u001b[0m | \u001b[0m0.3186   \u001b[0m | \u001b[0m4.471    \u001b[0m | \u001b[0m38.87    \u001b[0m | \u001b[0m0.007587 \u001b[0m | \u001b[0m334.3    \u001b[0m | \u001b[0m41.21    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-3.678   \u001b[0m | \u001b[0m0.1048   \u001b[0m | \u001b[0m7.476    \u001b[0m | \u001b[0m28.61    \u001b[0m | \u001b[0m0.09256  \u001b[0m | \u001b[0m337.0    \u001b[0m | \u001b[0m7.968    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m-3.712   \u001b[0m | \u001b[0m0.3702   \u001b[0m | \u001b[0m8.405    \u001b[0m | \u001b[0m45.85    \u001b[0m | \u001b[0m0.09327  \u001b[0m | \u001b[0m112.6    \u001b[0m | \u001b[0m15.55    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-3.895   \u001b[0m | \u001b[0m0.3122   \u001b[0m | \u001b[0m9.643    \u001b[0m | \u001b[0m47.76    \u001b[0m | \u001b[0m0.05611  \u001b[0m | \u001b[0m924.0    \u001b[0m | \u001b[0m33.87    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m-3.808   \u001b[0m | \u001b[0m0.2011   \u001b[0m | \u001b[0m6.402    \u001b[0m | \u001b[0m32.19    \u001b[0m | \u001b[0m0.05541  \u001b[0m | \u001b[0m933.6    \u001b[0m | \u001b[0m46.34    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m-3.719   \u001b[0m | \u001b[0m0.2035   \u001b[0m | \u001b[0m9.743    \u001b[0m | \u001b[0m12.83    \u001b[0m | \u001b[0m0.01351  \u001b[0m | \u001b[0m221.6    \u001b[0m | \u001b[0m27.75    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m-3.682   \u001b[0m | \u001b[0m0.02055  \u001b[0m | \u001b[0m9.636    \u001b[0m | \u001b[0m42.22    \u001b[0m | \u001b[0m0.002487 \u001b[0m | \u001b[0m258.6    \u001b[0m | \u001b[0m19.94    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m-3.703   \u001b[0m | \u001b[0m0.07419  \u001b[0m | \u001b[0m8.666    \u001b[0m | \u001b[0m20.51    \u001b[0m | \u001b[0m0.09407  \u001b[0m | \u001b[0m623.8    \u001b[0m | \u001b[0m44.55    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m-3.919   \u001b[0m | \u001b[0m0.4239   \u001b[0m | \u001b[0m9.338    \u001b[0m | \u001b[0m25.69    \u001b[0m | \u001b[0m0.05509  \u001b[0m | \u001b[0m818.7    \u001b[0m | \u001b[0m17.86    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m-3.806   \u001b[0m | \u001b[0m0.2502   \u001b[0m | \u001b[0m7.194    \u001b[0m | \u001b[0m5.699    \u001b[0m | \u001b[0m0.05975  \u001b[0m | \u001b[0m490.3    \u001b[0m | \u001b[0m41.33    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m-3.762   \u001b[0m | \u001b[0m0.1645   \u001b[0m | \u001b[0m9.25     \u001b[0m | \u001b[0m31.0     \u001b[0m | \u001b[0m0.01922  \u001b[0m | \u001b[0m809.1    \u001b[0m | \u001b[0m32.54    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m-3.686   \u001b[0m | \u001b[0m0.03642  \u001b[0m | \u001b[0m5.941    \u001b[0m | \u001b[0m35.56    \u001b[0m | \u001b[0m0.09194  \u001b[0m | \u001b[0m100.4    \u001b[0m | \u001b[0m48.95    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m-3.769   \u001b[0m | \u001b[0m0.1945   \u001b[0m | \u001b[0m9.816    \u001b[0m | \u001b[0m32.21    \u001b[0m | \u001b[0m0.08306  \u001b[0m | \u001b[0m617.2    \u001b[0m | \u001b[0m33.26    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m-3.746   \u001b[0m | \u001b[0m0.1499   \u001b[0m | \u001b[0m7.108    \u001b[0m | \u001b[0m38.75    \u001b[0m | \u001b[0m0.08597  \u001b[0m | \u001b[0m779.6    \u001b[0m | \u001b[0m36.41    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m-3.835   \u001b[0m | \u001b[0m0.4336   \u001b[0m | \u001b[0m5.259    \u001b[0m | \u001b[0m35.19    \u001b[0m | \u001b[0m0.04564  \u001b[0m | \u001b[0m443.9    \u001b[0m | \u001b[0m23.49    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m-3.778   \u001b[0m | \u001b[0m0.2067   \u001b[0m | \u001b[0m5.222    \u001b[0m | \u001b[0m32.99    \u001b[0m | \u001b[0m0.04359  \u001b[0m | \u001b[0m976.4    \u001b[0m | \u001b[0m35.5     \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m-3.716   \u001b[0m | \u001b[0m0.1073   \u001b[0m | \u001b[0m5.987    \u001b[0m | \u001b[0m20.45    \u001b[0m | \u001b[0m0.07997  \u001b[0m | \u001b[0m892.0    \u001b[0m | \u001b[0m45.67    \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m-3.762   \u001b[0m | \u001b[0m0.3347   \u001b[0m | \u001b[0m4.891    \u001b[0m | \u001b[0m16.36    \u001b[0m | \u001b[0m0.08563  \u001b[0m | \u001b[0m574.9    \u001b[0m | \u001b[0m41.1     \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m-3.819   \u001b[0m | \u001b[0m0.2905   \u001b[0m | \u001b[0m8.132    \u001b[0m | \u001b[0m28.36    \u001b[0m | \u001b[0m0.07732  \u001b[0m | \u001b[0m612.0    \u001b[0m | \u001b[0m25.96    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m-3.7     \u001b[0m | \u001b[0m0.1779   \u001b[0m | \u001b[0m3.477    \u001b[0m | \u001b[0m22.01    \u001b[0m | \u001b[0m0.008883 \u001b[0m | \u001b[0m984.5    \u001b[0m | \u001b[0m13.17    \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m-3.85    \u001b[0m | \u001b[0m0.4783   \u001b[0m | \u001b[0m6.777    \u001b[0m | \u001b[0m48.13    \u001b[0m | \u001b[0m0.07426  \u001b[0m | \u001b[0m314.8    \u001b[0m | \u001b[0m27.23    \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m-3.725   \u001b[0m | \u001b[0m0.3029   \u001b[0m | \u001b[0m5.229    \u001b[0m | \u001b[0m34.84    \u001b[0m | \u001b[0m0.04539  \u001b[0m | \u001b[0m255.0    \u001b[0m | \u001b[0m18.63    \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m-3.787   \u001b[0m | \u001b[0m0.4496   \u001b[0m | \u001b[0m6.707    \u001b[0m | \u001b[0m7.655    \u001b[0m | \u001b[0m0.08389  \u001b[0m | \u001b[0m304.2    \u001b[0m | \u001b[0m13.33    \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m-3.697   \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m3.856    \u001b[0m | \u001b[0m37.85    \u001b[0m | \u001b[0m0.08694  \u001b[0m | \u001b[0m696.6    \u001b[0m | \u001b[0m22.77    \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m-3.694   \u001b[0m | \u001b[0m0.4592   \u001b[0m | \u001b[0m3.662    \u001b[0m | \u001b[0m49.4     \u001b[0m | \u001b[0m0.09475  \u001b[0m | \u001b[0m112.0    \u001b[0m | \u001b[0m25.43    \u001b[0m |\n",
      "| \u001b[0m56       \u001b[0m | \u001b[0m-3.746   \u001b[0m | \u001b[0m0.3501   \u001b[0m | \u001b[0m7.015    \u001b[0m | \u001b[0m19.81    \u001b[0m | \u001b[0m0.07127  \u001b[0m | \u001b[0m817.7    \u001b[0m | \u001b[0m6.367    \u001b[0m |\n",
      "| \u001b[0m57       \u001b[0m | \u001b[0m-3.711   \u001b[0m | \u001b[0m0.1089   \u001b[0m | \u001b[0m5.056    \u001b[0m | \u001b[0m12.17    \u001b[0m | \u001b[0m0.02875  \u001b[0m | \u001b[0m830.4    \u001b[0m | \u001b[0m36.79    \u001b[0m |\n",
      "| \u001b[95m58       \u001b[0m | \u001b[95m-3.677   \u001b[0m | \u001b[95m0.01226  \u001b[0m | \u001b[95m8.747    \u001b[0m | \u001b[95m28.06    \u001b[0m | \u001b[95m0.05413  \u001b[0m | \u001b[95m764.6    \u001b[0m | \u001b[95m47.54    \u001b[0m |\n",
      "| \u001b[0m59       \u001b[0m | \u001b[0m-3.796   \u001b[0m | \u001b[0m0.2293   \u001b[0m | \u001b[0m8.585    \u001b[0m | \u001b[0m23.01    \u001b[0m | \u001b[0m0.01733  \u001b[0m | \u001b[0m934.8    \u001b[0m | \u001b[0m21.16    \u001b[0m |\n",
      "| \u001b[0m60       \u001b[0m | \u001b[0m-3.749   \u001b[0m | \u001b[0m0.4501   \u001b[0m | \u001b[0m3.605    \u001b[0m | \u001b[0m30.18    \u001b[0m | \u001b[0m0.02623  \u001b[0m | \u001b[0m780.0    \u001b[0m | \u001b[0m15.7     \u001b[0m |\n",
      "| \u001b[0m61       \u001b[0m | \u001b[0m-3.962   \u001b[0m | \u001b[0m0.4945   \u001b[0m | \u001b[0m8.222    \u001b[0m | \u001b[0m19.52    \u001b[0m | \u001b[0m0.03805  \u001b[0m | \u001b[0m393.5    \u001b[0m | \u001b[0m31.14    \u001b[0m |\n",
      "| \u001b[0m62       \u001b[0m | \u001b[0m-3.801   \u001b[0m | \u001b[0m0.3816   \u001b[0m | \u001b[0m4.426    \u001b[0m | \u001b[0m15.5     \u001b[0m | \u001b[0m0.05894  \u001b[0m | \u001b[0m790.0    \u001b[0m | \u001b[0m48.46    \u001b[0m |\n",
      "| \u001b[0m63       \u001b[0m | \u001b[0m-4.035   \u001b[0m | \u001b[0m0.4382   \u001b[0m | \u001b[0m9.946    \u001b[0m | \u001b[0m22.48    \u001b[0m | \u001b[0m0.08644  \u001b[0m | \u001b[0m812.4    \u001b[0m | \u001b[0m37.62    \u001b[0m |\n",
      "| \u001b[0m64       \u001b[0m | \u001b[0m-3.677   \u001b[0m | \u001b[0m0.01853  \u001b[0m | \u001b[0m5.319    \u001b[0m | \u001b[0m25.96    \u001b[0m | \u001b[0m0.02942  \u001b[0m | \u001b[0m673.2    \u001b[0m | \u001b[0m22.88    \u001b[0m |\n",
      "| \u001b[0m65       \u001b[0m | \u001b[0m-3.803   \u001b[0m | \u001b[0m0.2844   \u001b[0m | \u001b[0m7.127    \u001b[0m | \u001b[0m37.62    \u001b[0m | \u001b[0m0.0761   \u001b[0m | \u001b[0m493.0    \u001b[0m | \u001b[0m34.72    \u001b[0m |\n",
      "| \u001b[0m66       \u001b[0m | \u001b[0m-3.679   \u001b[0m | \u001b[0m0.04743  \u001b[0m | \u001b[0m7.694    \u001b[0m | \u001b[0m36.09    \u001b[0m | \u001b[0m0.06889  \u001b[0m | \u001b[0m351.9    \u001b[0m | \u001b[0m32.77    \u001b[0m |\n",
      "| \u001b[0m67       \u001b[0m | \u001b[0m-3.7     \u001b[0m | \u001b[0m0.4175   \u001b[0m | \u001b[0m3.052    \u001b[0m | \u001b[0m6.763    \u001b[0m | \u001b[0m0.08097  \u001b[0m | \u001b[0m181.3    \u001b[0m | \u001b[0m39.04    \u001b[0m |\n",
      "| \u001b[0m68       \u001b[0m | \u001b[0m-3.678   \u001b[0m | \u001b[0m0.05534  \u001b[0m | \u001b[0m9.956    \u001b[0m | \u001b[0m25.82    \u001b[0m | \u001b[0m0.01479  \u001b[0m | \u001b[0m366.1    \u001b[0m | \u001b[0m10.24    \u001b[0m |\n",
      "| \u001b[0m69       \u001b[0m | \u001b[0m-3.685   \u001b[0m | \u001b[0m0.0864   \u001b[0m | \u001b[0m3.465    \u001b[0m | \u001b[0m46.12    \u001b[0m | \u001b[0m0.01072  \u001b[0m | \u001b[0m380.5    \u001b[0m | \u001b[0m33.82    \u001b[0m |\n",
      "| \u001b[0m70       \u001b[0m | \u001b[0m-3.751   \u001b[0m | \u001b[0m0.2088   \u001b[0m | \u001b[0m8.132    \u001b[0m | \u001b[0m43.55    \u001b[0m | \u001b[0m0.09909  \u001b[0m | \u001b[0m869.1    \u001b[0m | \u001b[0m16.24    \u001b[0m |\n",
      "| \u001b[0m71       \u001b[0m | \u001b[0m-3.678   \u001b[0m | \u001b[0m0.01709  \u001b[0m | \u001b[0m7.485    \u001b[0m | \u001b[0m5.744    \u001b[0m | \u001b[0m0.08891  \u001b[0m | \u001b[0m945.1    \u001b[0m | \u001b[0m15.22    \u001b[0m |\n",
      "| \u001b[95m72       \u001b[0m | \u001b[95m-3.676   \u001b[0m | \u001b[95m0.03614  \u001b[0m | \u001b[95m6.816    \u001b[0m | \u001b[95m28.9     \u001b[0m | \u001b[95m0.004311 \u001b[0m | \u001b[95m353.5    \u001b[0m | \u001b[95m27.85    \u001b[0m |\n",
      "| \u001b[0m73       \u001b[0m | \u001b[0m-3.836   \u001b[0m | \u001b[0m0.2469   \u001b[0m | \u001b[0m8.323    \u001b[0m | \u001b[0m41.5     \u001b[0m | \u001b[0m0.07832  \u001b[0m | \u001b[0m888.4    \u001b[0m | \u001b[0m37.04    \u001b[0m |\n",
      "| \u001b[0m74       \u001b[0m | \u001b[0m-3.686   \u001b[0m | \u001b[0m0.1955   \u001b[0m | \u001b[0m3.579    \u001b[0m | \u001b[0m27.82    \u001b[0m | \u001b[0m0.04302  \u001b[0m | \u001b[0m126.1    \u001b[0m | \u001b[0m12.02    \u001b[0m |\n",
      "| \u001b[0m75       \u001b[0m | \u001b[0m-3.72    \u001b[0m | \u001b[0m0.3552   \u001b[0m | \u001b[0m3.937    \u001b[0m | \u001b[0m40.92    \u001b[0m | \u001b[0m0.06398  \u001b[0m | \u001b[0m655.4    \u001b[0m | \u001b[0m29.01    \u001b[0m |\n",
      "| \u001b[0m76       \u001b[0m | \u001b[0m-4.012   \u001b[0m | \u001b[0m0.4119   \u001b[0m | \u001b[0m8.994    \u001b[0m | \u001b[0m23.61    \u001b[0m | \u001b[0m0.08187  \u001b[0m | \u001b[0m995.9    \u001b[0m | \u001b[0m36.37    \u001b[0m |\n",
      "| \u001b[0m77       \u001b[0m | \u001b[0m-3.678   \u001b[0m | \u001b[0m0.01517  \u001b[0m | \u001b[0m8.183    \u001b[0m | \u001b[0m36.1     \u001b[0m | \u001b[0m0.06662  \u001b[0m | \u001b[0m954.3    \u001b[0m | \u001b[0m11.97    \u001b[0m |\n",
      "| \u001b[0m78       \u001b[0m | \u001b[0m-3.856   \u001b[0m | \u001b[0m0.4493   \u001b[0m | \u001b[0m5.067    \u001b[0m | \u001b[0m44.96    \u001b[0m | \u001b[0m0.01647  \u001b[0m | \u001b[0m686.3    \u001b[0m | \u001b[0m17.73    \u001b[0m |\n",
      "| \u001b[0m79       \u001b[0m | \u001b[0m-3.676   \u001b[0m | \u001b[0m0.03965  \u001b[0m | \u001b[0m6.061    \u001b[0m | \u001b[0m24.71    \u001b[0m | \u001b[0m0.07396  \u001b[0m | \u001b[0m259.3    \u001b[0m | \u001b[0m34.82    \u001b[0m |\n",
      "| \u001b[0m80       \u001b[0m | \u001b[0m-3.902   \u001b[0m | \u001b[0m0.4821   \u001b[0m | \u001b[0m9.698    \u001b[0m | \u001b[0m10.44    \u001b[0m | \u001b[0m0.02566  \u001b[0m | \u001b[0m209.9    \u001b[0m | \u001b[0m36.91    \u001b[0m |\n",
      "| \u001b[0m81       \u001b[0m | \u001b[0m-3.683   \u001b[0m | \u001b[0m0.1179   \u001b[0m | \u001b[0m6.917    \u001b[0m | \u001b[0m14.12    \u001b[0m | \u001b[0m0.0842   \u001b[0m | \u001b[0m370.9    \u001b[0m | \u001b[0m10.02    \u001b[0m |\n",
      "| \u001b[0m82       \u001b[0m | \u001b[0m-3.738   \u001b[0m | \u001b[0m0.1858   \u001b[0m | \u001b[0m6.179    \u001b[0m | \u001b[0m9.422    \u001b[0m | \u001b[0m0.04885  \u001b[0m | \u001b[0m370.8    \u001b[0m | \u001b[0m45.73    \u001b[0m |\n",
      "| \u001b[0m83       \u001b[0m | \u001b[0m-3.697   \u001b[0m | \u001b[0m0.1693   \u001b[0m | \u001b[0m3.298    \u001b[0m | \u001b[0m16.53    \u001b[0m | \u001b[0m0.02891  \u001b[0m | \u001b[0m898.8    \u001b[0m | \u001b[0m22.06    \u001b[0m |\n",
      "| \u001b[0m84       \u001b[0m | \u001b[0m-3.68    \u001b[0m | \u001b[0m0.07047  \u001b[0m | \u001b[0m4.146    \u001b[0m | \u001b[0m30.55    \u001b[0m | \u001b[0m0.01548  \u001b[0m | \u001b[0m214.4    \u001b[0m | \u001b[0m48.67    \u001b[0m |\n",
      "| \u001b[0m85       \u001b[0m | \u001b[0m-4.042   \u001b[0m | \u001b[0m0.4463   \u001b[0m | \u001b[0m9.834    \u001b[0m | \u001b[0m25.27    \u001b[0m | \u001b[0m0.03944  \u001b[0m | \u001b[0m954.5    \u001b[0m | \u001b[0m31.19    \u001b[0m |\n",
      "| \u001b[0m86       \u001b[0m | \u001b[0m-3.87    \u001b[0m | \u001b[0m0.4949   \u001b[0m | \u001b[0m4.348    \u001b[0m | \u001b[0m19.98    \u001b[0m | \u001b[0m0.09604  \u001b[0m | \u001b[0m798.7    \u001b[0m | \u001b[0m20.26    \u001b[0m |\n",
      "| \u001b[0m87       \u001b[0m | \u001b[0m-3.868   \u001b[0m | \u001b[0m0.4949   \u001b[0m | \u001b[0m4.162    \u001b[0m | \u001b[0m6.743    \u001b[0m | \u001b[0m0.01946  \u001b[0m | \u001b[0m856.1    \u001b[0m | \u001b[0m28.36    \u001b[0m |\n",
      "| \u001b[0m88       \u001b[0m | \u001b[0m-3.783   \u001b[0m | \u001b[0m0.4028   \u001b[0m | \u001b[0m4.599    \u001b[0m | \u001b[0m31.68    \u001b[0m | \u001b[0m0.07006  \u001b[0m | \u001b[0m568.3    \u001b[0m | \u001b[0m21.69    \u001b[0m |\n",
      "| \u001b[0m89       \u001b[0m | \u001b[0m-3.69    \u001b[0m | \u001b[0m0.1015   \u001b[0m | \u001b[0m8.009    \u001b[0m | \u001b[0m36.85    \u001b[0m | \u001b[0m0.07827  \u001b[0m | \u001b[0m570.0    \u001b[0m | \u001b[0m17.2     \u001b[0m |\n",
      "| \u001b[0m90       \u001b[0m | \u001b[0m-3.877   \u001b[0m | \u001b[0m0.4966   \u001b[0m | \u001b[0m4.998    \u001b[0m | \u001b[0m9.13     \u001b[0m | \u001b[0m0.07342  \u001b[0m | \u001b[0m905.1    \u001b[0m | \u001b[0m45.23    \u001b[0m |\n",
      "| \u001b[0m91       \u001b[0m | \u001b[0m-3.794   \u001b[0m | \u001b[0m0.2478   \u001b[0m | \u001b[0m7.297    \u001b[0m | \u001b[0m27.79    \u001b[0m | \u001b[0m0.04046  \u001b[0m | \u001b[0m475.5    \u001b[0m | \u001b[0m41.53    \u001b[0m |\n",
      "| \u001b[0m92       \u001b[0m | \u001b[0m-4.093   \u001b[0m | \u001b[0m0.4554   \u001b[0m | \u001b[0m7.034    \u001b[0m | \u001b[0m6.595    \u001b[0m | \u001b[0m0.004998 \u001b[0m | \u001b[0m939.3    \u001b[0m | \u001b[0m47.95    \u001b[0m |\n",
      "| \u001b[0m93       \u001b[0m | \u001b[0m-3.721   \u001b[0m | \u001b[0m0.2188   \u001b[0m | \u001b[0m6.339    \u001b[0m | \u001b[0m34.61    \u001b[0m | \u001b[0m0.08637  \u001b[0m | \u001b[0m225.5    \u001b[0m | \u001b[0m45.25    \u001b[0m |\n",
      "| \u001b[0m94       \u001b[0m | \u001b[0m-3.742   \u001b[0m | \u001b[0m0.1992   \u001b[0m | \u001b[0m9.33     \u001b[0m | \u001b[0m40.65    \u001b[0m | \u001b[0m0.06997  \u001b[0m | \u001b[0m494.7    \u001b[0m | \u001b[0m25.4     \u001b[0m |\n",
      "| \u001b[0m95       \u001b[0m | \u001b[0m-3.903   \u001b[0m | \u001b[0m0.4672   \u001b[0m | \u001b[0m8.439    \u001b[0m | \u001b[0m23.01    \u001b[0m | \u001b[0m0.002462 \u001b[0m | \u001b[0m553.5    \u001b[0m | \u001b[0m16.21    \u001b[0m |\n",
      "| \u001b[0m96       \u001b[0m | \u001b[0m-3.699   \u001b[0m | \u001b[0m0.2001   \u001b[0m | \u001b[0m3.062    \u001b[0m | \u001b[0m38.86    \u001b[0m | \u001b[0m0.02215  \u001b[0m | \u001b[0m942.7    \u001b[0m | \u001b[0m17.83    \u001b[0m |\n",
      "| \u001b[0m97       \u001b[0m | \u001b[0m-3.684   \u001b[0m | \u001b[0m0.04935  \u001b[0m | \u001b[0m8.982    \u001b[0m | \u001b[0m15.53    \u001b[0m | \u001b[0m0.02772  \u001b[0m | \u001b[0m701.1    \u001b[0m | \u001b[0m26.6     \u001b[0m |\n",
      "| \u001b[0m98       \u001b[0m | \u001b[0m-3.686   \u001b[0m | \u001b[0m0.09596  \u001b[0m | \u001b[0m6.275    \u001b[0m | \u001b[0m26.44    \u001b[0m | \u001b[0m0.003224 \u001b[0m | \u001b[0m550.4    \u001b[0m | \u001b[0m14.98    \u001b[0m |\n",
      "| \u001b[0m99       \u001b[0m | \u001b[0m-3.694   \u001b[0m | \u001b[0m0.06397  \u001b[0m | \u001b[0m5.342    \u001b[0m | \u001b[0m38.57    \u001b[0m | \u001b[0m0.09438  \u001b[0m | \u001b[0m921.3    \u001b[0m | \u001b[0m38.39    \u001b[0m |\n",
      "| \u001b[0m100      \u001b[0m | \u001b[0m-3.677   \u001b[0m | \u001b[0m0.02662  \u001b[0m | \u001b[0m5.135    \u001b[0m | \u001b[0m7.452    \u001b[0m | \u001b[0m0.06014  \u001b[0m | \u001b[0m805.7    \u001b[0m | \u001b[0m16.97    \u001b[0m |\n",
      "| \u001b[0m101      \u001b[0m | \u001b[0m-3.738   \u001b[0m | \u001b[0m0.2309   \u001b[0m | \u001b[0m7.485    \u001b[0m | \u001b[0m37.48    \u001b[0m | \u001b[0m0.01637  \u001b[0m | \u001b[0m269.9    \u001b[0m | \u001b[0m35.65    \u001b[0m |\n",
      "| \u001b[0m102      \u001b[0m | \u001b[0m-3.681   \u001b[0m | \u001b[0m0.1157   \u001b[0m | \u001b[0m4.038    \u001b[0m | \u001b[0m38.05    \u001b[0m | \u001b[0m0.04117  \u001b[0m | \u001b[0m120.8    \u001b[0m | \u001b[0m47.43    \u001b[0m |\n",
      "| \u001b[0m103      \u001b[0m | \u001b[0m-3.881   \u001b[0m | \u001b[0m0.4393   \u001b[0m | \u001b[0m6.08     \u001b[0m | \u001b[0m30.48    \u001b[0m | \u001b[0m0.08781  \u001b[0m | \u001b[0m342.0    \u001b[0m | \u001b[0m34.63    \u001b[0m |\n",
      "| \u001b[0m104      \u001b[0m | \u001b[0m-3.682   \u001b[0m | \u001b[0m0.02027  \u001b[0m | \u001b[0m5.673    \u001b[0m | \u001b[0m15.68    \u001b[0m | \u001b[0m0.0831   \u001b[0m | \u001b[0m230.5    \u001b[0m | \u001b[0m39.87    \u001b[0m |\n",
      "| \u001b[0m105      \u001b[0m | \u001b[0m-3.691   \u001b[0m | \u001b[0m0.1376   \u001b[0m | \u001b[0m7.753    \u001b[0m | \u001b[0m47.53    \u001b[0m | \u001b[0m0.09951  \u001b[0m | \u001b[0m203.8    \u001b[0m | \u001b[0m33.56    \u001b[0m |\n",
      "| \u001b[0m106      \u001b[0m | \u001b[0m-3.68    \u001b[0m | \u001b[0m0.05126  \u001b[0m | \u001b[0m5.747    \u001b[0m | \u001b[0m33.75    \u001b[0m | \u001b[0m0.06753  \u001b[0m | \u001b[0m122.9    \u001b[0m | \u001b[0m27.5     \u001b[0m |\n",
      "| \u001b[0m107      \u001b[0m | \u001b[0m-3.86    \u001b[0m | \u001b[0m0.4247   \u001b[0m | \u001b[0m8.572    \u001b[0m | \u001b[0m33.23    \u001b[0m | \u001b[0m0.08819  \u001b[0m | \u001b[0m602.8    \u001b[0m | \u001b[0m16.95    \u001b[0m |\n",
      "| \u001b[0m108      \u001b[0m | \u001b[0m-3.71    \u001b[0m | \u001b[0m0.2142   \u001b[0m | \u001b[0m5.88     \u001b[0m | \u001b[0m18.95    \u001b[0m | \u001b[0m0.006503 \u001b[0m | \u001b[0m460.4    \u001b[0m | \u001b[0m13.46    \u001b[0m |\n",
      "| \u001b[0m109      \u001b[0m | \u001b[0m-3.717   \u001b[0m | \u001b[0m0.1069   \u001b[0m | \u001b[0m5.493    \u001b[0m | \u001b[0m10.13    \u001b[0m | \u001b[0m0.05503  \u001b[0m | \u001b[0m961.6    \u001b[0m | \u001b[0m26.04    \u001b[0m |\n",
      "| \u001b[0m110      \u001b[0m | \u001b[0m-3.68    \u001b[0m | \u001b[0m0.116    \u001b[0m | \u001b[0m9.654    \u001b[0m | \u001b[0m32.41    \u001b[0m | \u001b[0m0.09328  \u001b[0m | \u001b[0m249.5    \u001b[0m | \u001b[0m7.955    \u001b[0m |\n",
      "| \u001b[0m111      \u001b[0m | \u001b[0m-3.928   \u001b[0m | \u001b[0m0.3049   \u001b[0m | \u001b[0m9.106    \u001b[0m | \u001b[0m6.115    \u001b[0m | \u001b[0m0.02346  \u001b[0m | \u001b[0m892.8    \u001b[0m | \u001b[0m47.87    \u001b[0m |\n",
      "| \u001b[0m112      \u001b[0m | \u001b[0m-3.682   \u001b[0m | \u001b[0m0.1345   \u001b[0m | \u001b[0m9.611    \u001b[0m | \u001b[0m11.3     \u001b[0m | \u001b[0m0.02292  \u001b[0m | \u001b[0m220.1    \u001b[0m | \u001b[0m5.631    \u001b[0m |\n",
      "| \u001b[0m113      \u001b[0m | \u001b[0m-3.868   \u001b[0m | \u001b[0m0.4779   \u001b[0m | \u001b[0m7.169    \u001b[0m | \u001b[0m49.81    \u001b[0m | \u001b[0m0.02947  \u001b[0m | \u001b[0m560.7    \u001b[0m | \u001b[0m14.51    \u001b[0m |\n",
      "| \u001b[0m114      \u001b[0m | \u001b[0m-3.824   \u001b[0m | \u001b[0m0.4161   \u001b[0m | \u001b[0m5.119    \u001b[0m | \u001b[0m5.943    \u001b[0m | \u001b[0m0.06035  \u001b[0m | \u001b[0m364.0    \u001b[0m | \u001b[0m29.43    \u001b[0m |\n",
      "| \u001b[0m115      \u001b[0m | \u001b[0m-3.718   \u001b[0m | \u001b[0m0.3547   \u001b[0m | \u001b[0m3.13     \u001b[0m | \u001b[0m31.43    \u001b[0m | \u001b[0m0.04211  \u001b[0m | \u001b[0m634.5    \u001b[0m | \u001b[0m17.84    \u001b[0m |\n",
      "| \u001b[0m116      \u001b[0m | \u001b[0m-3.686   \u001b[0m | \u001b[0m0.07495  \u001b[0m | \u001b[0m5.195    \u001b[0m | \u001b[0m13.49    \u001b[0m | \u001b[0m0.03403  \u001b[0m | \u001b[0m430.1    \u001b[0m | \u001b[0m37.06    \u001b[0m |\n",
      "| \u001b[0m117      \u001b[0m | \u001b[0m-3.681   \u001b[0m | \u001b[0m0.1555   \u001b[0m | \u001b[0m6.176    \u001b[0m | \u001b[0m39.34    \u001b[0m | \u001b[0m0.09294  \u001b[0m | \u001b[0m163.3    \u001b[0m | \u001b[0m14.42    \u001b[0m |\n",
      "| \u001b[0m118      \u001b[0m | \u001b[0m-3.683   \u001b[0m | \u001b[0m0.0389   \u001b[0m | \u001b[0m6.33     \u001b[0m | \u001b[0m9.65     \u001b[0m | \u001b[0m0.01244  \u001b[0m | \u001b[0m866.6    \u001b[0m | \u001b[0m24.91    \u001b[0m |\n",
      "| \u001b[0m119      \u001b[0m | \u001b[0m-3.687   \u001b[0m | \u001b[0m0.02726  \u001b[0m | \u001b[0m3.142    \u001b[0m | \u001b[0m16.32    \u001b[0m | \u001b[0m0.05985  \u001b[0m | \u001b[0m801.8    \u001b[0m | \u001b[0m41.51    \u001b[0m |\n",
      "| \u001b[0m120      \u001b[0m | \u001b[0m-3.754   \u001b[0m | \u001b[0m0.2512   \u001b[0m | \u001b[0m6.671    \u001b[0m | \u001b[0m27.99    \u001b[0m | \u001b[0m0.01752  \u001b[0m | \u001b[0m286.0    \u001b[0m | \u001b[0m40.98    \u001b[0m |\n",
      "| \u001b[0m121      \u001b[0m | \u001b[0m-3.783   \u001b[0m | \u001b[0m0.2545   \u001b[0m | \u001b[0m9.859    \u001b[0m | \u001b[0m19.12    \u001b[0m | \u001b[0m0.0747   \u001b[0m | \u001b[0m279.4    \u001b[0m | \u001b[0m42.67    \u001b[0m |\n",
      "| \u001b[0m122      \u001b[0m | \u001b[0m-3.736   \u001b[0m | \u001b[0m0.3524   \u001b[0m | \u001b[0m6.377    \u001b[0m | \u001b[0m25.58    \u001b[0m | \u001b[0m0.04467  \u001b[0m | \u001b[0m725.5    \u001b[0m | \u001b[0m6.752    \u001b[0m |\n",
      "| \u001b[0m123      \u001b[0m | \u001b[0m-3.699   \u001b[0m | \u001b[0m0.09656  \u001b[0m | \u001b[0m7.406    \u001b[0m | \u001b[0m35.27    \u001b[0m | \u001b[0m0.01483  \u001b[0m | \u001b[0m427.3    \u001b[0m | \u001b[0m44.17    \u001b[0m |\n",
      "| \u001b[0m124      \u001b[0m | \u001b[0m-3.749   \u001b[0m | \u001b[0m0.1537   \u001b[0m | \u001b[0m9.732    \u001b[0m | \u001b[0m44.42    \u001b[0m | \u001b[0m0.01534  \u001b[0m | \u001b[0m908.0    \u001b[0m | \u001b[0m30.54    \u001b[0m |\n",
      "| \u001b[0m125      \u001b[0m | \u001b[0m-3.721   \u001b[0m | \u001b[0m0.3725   \u001b[0m | \u001b[0m5.351    \u001b[0m | \u001b[0m5.9      \u001b[0m | \u001b[0m0.04585  \u001b[0m | \u001b[0m635.1    \u001b[0m | \u001b[0m5.077    \u001b[0m |\n",
      "| \u001b[0m126      \u001b[0m | \u001b[0m-3.746   \u001b[0m | \u001b[0m0.1644   \u001b[0m | \u001b[0m8.215    \u001b[0m | \u001b[0m43.2     \u001b[0m | \u001b[0m0.09527  \u001b[0m | \u001b[0m519.4    \u001b[0m | \u001b[0m41.92    \u001b[0m |\n",
      "| \u001b[0m127      \u001b[0m | \u001b[0m-3.679   \u001b[0m | \u001b[0m0.03632  \u001b[0m | \u001b[0m9.293    \u001b[0m | \u001b[0m47.15    \u001b[0m | \u001b[0m0.04518  \u001b[0m | \u001b[0m505.7    \u001b[0m | \u001b[0m30.19    \u001b[0m |\n",
      "| \u001b[0m128      \u001b[0m | \u001b[0m-3.822   \u001b[0m | \u001b[0m0.331    \u001b[0m | \u001b[0m8.069    \u001b[0m | \u001b[0m21.75    \u001b[0m | \u001b[0m0.0611   \u001b[0m | \u001b[0m294.2    \u001b[0m | \u001b[0m43.45    \u001b[0m |\n",
      "| \u001b[0m129      \u001b[0m | \u001b[0m-3.694   \u001b[0m | \u001b[0m0.1509   \u001b[0m | \u001b[0m4.114    \u001b[0m | \u001b[0m26.49    \u001b[0m | \u001b[0m0.02876  \u001b[0m | \u001b[0m460.1    \u001b[0m | \u001b[0m47.15    \u001b[0m |\n",
      "| \u001b[0m130      \u001b[0m | \u001b[0m-3.728   \u001b[0m | \u001b[0m0.4308   \u001b[0m | \u001b[0m3.506    \u001b[0m | \u001b[0m23.81    \u001b[0m | \u001b[0m0.0746   \u001b[0m | \u001b[0m551.8    \u001b[0m | \u001b[0m44.2     \u001b[0m |\n",
      "| \u001b[0m131      \u001b[0m | \u001b[0m-3.872   \u001b[0m | \u001b[0m0.3371   \u001b[0m | \u001b[0m7.249    \u001b[0m | \u001b[0m30.33    \u001b[0m | \u001b[0m0.08269  \u001b[0m | \u001b[0m922.2    \u001b[0m | \u001b[0m25.31    \u001b[0m |\n",
      "| \u001b[0m132      \u001b[0m | \u001b[0m-3.681   \u001b[0m | \u001b[0m0.09435  \u001b[0m | \u001b[0m5.343    \u001b[0m | \u001b[0m12.61    \u001b[0m | \u001b[0m0.04084  \u001b[0m | \u001b[0m218.2    \u001b[0m | \u001b[0m41.26    \u001b[0m |\n",
      "| \u001b[0m133      \u001b[0m | \u001b[0m-3.749   \u001b[0m | \u001b[0m0.1201   \u001b[0m | \u001b[0m9.629    \u001b[0m | \u001b[0m9.551    \u001b[0m | \u001b[0m0.05087  \u001b[0m | \u001b[0m902.1    \u001b[0m | \u001b[0m47.29    \u001b[0m |\n",
      "| \u001b[0m134      \u001b[0m | \u001b[0m-3.928   \u001b[0m | \u001b[0m0.4277   \u001b[0m | \u001b[0m9.05     \u001b[0m | \u001b[0m43.78    \u001b[0m | \u001b[0m0.0772   \u001b[0m | \u001b[0m836.9    \u001b[0m | \u001b[0m20.26    \u001b[0m |\n",
      "| \u001b[0m135      \u001b[0m | \u001b[0m-3.807   \u001b[0m | \u001b[0m0.2667   \u001b[0m | \u001b[0m9.007    \u001b[0m | \u001b[0m25.68    \u001b[0m | \u001b[0m0.09815  \u001b[0m | \u001b[0m444.1    \u001b[0m | \u001b[0m36.36    \u001b[0m |\n",
      "| \u001b[0m136      \u001b[0m | \u001b[0m-3.677   \u001b[0m | \u001b[0m0.1103   \u001b[0m | \u001b[0m6.948    \u001b[0m | \u001b[0m11.47    \u001b[0m | \u001b[0m0.05371  \u001b[0m | \u001b[0m150.5    \u001b[0m | \u001b[0m19.52    \u001b[0m |\n",
      "| \u001b[0m137      \u001b[0m | \u001b[0m-3.72    \u001b[0m | \u001b[0m0.327    \u001b[0m | \u001b[0m4.541    \u001b[0m | \u001b[0m27.41    \u001b[0m | \u001b[0m0.03814  \u001b[0m | \u001b[0m323.8    \u001b[0m | \u001b[0m44.75    \u001b[0m |\n",
      "| \u001b[0m138      \u001b[0m | \u001b[0m-3.786   \u001b[0m | \u001b[0m0.3094   \u001b[0m | \u001b[0m4.339    \u001b[0m | \u001b[0m13.42    \u001b[0m | \u001b[0m0.02361  \u001b[0m | \u001b[0m913.3    \u001b[0m | \u001b[0m22.32    \u001b[0m |\n",
      "| \u001b[0m139      \u001b[0m | \u001b[0m-3.699   \u001b[0m | \u001b[0m0.2139   \u001b[0m | \u001b[0m4.1      \u001b[0m | \u001b[0m9.462    \u001b[0m | \u001b[0m0.01686  \u001b[0m | \u001b[0m345.4    \u001b[0m | \u001b[0m32.45    \u001b[0m |\n",
      "| \u001b[0m140      \u001b[0m | \u001b[0m-3.699   \u001b[0m | \u001b[0m0.1877   \u001b[0m | \u001b[0m3.857    \u001b[0m | \u001b[0m48.31    \u001b[0m | \u001b[0m0.05706  \u001b[0m | \u001b[0m943.4    \u001b[0m | \u001b[0m35.74    \u001b[0m |\n",
      "| \u001b[0m141      \u001b[0m | \u001b[0m-3.71    \u001b[0m | \u001b[0m0.06958  \u001b[0m | \u001b[0m6.993    \u001b[0m | \u001b[0m37.23    \u001b[0m | \u001b[0m0.02522  \u001b[0m | \u001b[0m981.6    \u001b[0m | \u001b[0m44.92    \u001b[0m |\n",
      "| \u001b[0m142      \u001b[0m | \u001b[0m-3.712   \u001b[0m | \u001b[0m0.273    \u001b[0m | \u001b[0m7.502    \u001b[0m | \u001b[0m31.68    \u001b[0m | \u001b[0m0.03597  \u001b[0m | \u001b[0m295.1    \u001b[0m | \u001b[0m12.86    \u001b[0m |\n",
      "| \u001b[0m143      \u001b[0m | \u001b[0m-3.814   \u001b[0m | \u001b[0m0.4113   \u001b[0m | \u001b[0m6.385    \u001b[0m | \u001b[0m42.21    \u001b[0m | \u001b[0m0.01693  \u001b[0m | \u001b[0m286.6    \u001b[0m | \u001b[0m28.15    \u001b[0m |\n",
      "| \u001b[0m144      \u001b[0m | \u001b[0m-3.715   \u001b[0m | \u001b[0m0.2051   \u001b[0m | \u001b[0m7.534    \u001b[0m | \u001b[0m38.35    \u001b[0m | \u001b[0m0.09083  \u001b[0m | \u001b[0m171.9    \u001b[0m | \u001b[0m48.43    \u001b[0m |\n",
      "| \u001b[0m145      \u001b[0m | \u001b[0m-3.682   \u001b[0m | \u001b[0m0.03544  \u001b[0m | \u001b[0m5.121    \u001b[0m | \u001b[0m34.16    \u001b[0m | \u001b[0m0.08191  \u001b[0m | \u001b[0m936.7    \u001b[0m | \u001b[0m23.99    \u001b[0m |\n",
      "| \u001b[0m146      \u001b[0m | \u001b[0m-3.7     \u001b[0m | \u001b[0m0.2836   \u001b[0m | \u001b[0m9.491    \u001b[0m | \u001b[0m27.98    \u001b[0m | \u001b[0m0.05067  \u001b[0m | \u001b[0m119.2    \u001b[0m | \u001b[0m14.09    \u001b[0m |\n",
      "| \u001b[0m147      \u001b[0m | \u001b[0m-3.697   \u001b[0m | \u001b[0m0.08389  \u001b[0m | \u001b[0m5.173    \u001b[0m | \u001b[0m39.74    \u001b[0m | \u001b[0m0.02632  \u001b[0m | \u001b[0m767.0    \u001b[0m | \u001b[0m36.37    \u001b[0m |\n",
      "| \u001b[0m148      \u001b[0m | \u001b[0m-3.679   \u001b[0m | \u001b[0m0.04116  \u001b[0m | \u001b[0m7.891    \u001b[0m | \u001b[0m5.988    \u001b[0m | \u001b[0m0.03725  \u001b[0m | \u001b[0m476.3    \u001b[0m | \u001b[0m17.19    \u001b[0m |\n",
      "| \u001b[0m149      \u001b[0m | \u001b[0m-3.699   \u001b[0m | \u001b[0m0.1303   \u001b[0m | \u001b[0m5.269    \u001b[0m | \u001b[0m22.95    \u001b[0m | \u001b[0m0.0869   \u001b[0m | \u001b[0m790.7    \u001b[0m | \u001b[0m13.45    \u001b[0m |\n",
      "| \u001b[0m150      \u001b[0m | \u001b[0m-3.68    \u001b[0m | \u001b[0m0.0334   \u001b[0m | \u001b[0m6.804    \u001b[0m | \u001b[0m10.55    \u001b[0m | \u001b[0m0.05431  \u001b[0m | \u001b[0m826.0    \u001b[0m | \u001b[0m24.86    \u001b[0m |\n",
      "| \u001b[0m151      \u001b[0m | \u001b[0m-3.686   \u001b[0m | \u001b[0m0.08107  \u001b[0m | \u001b[0m6.499    \u001b[0m | \u001b[0m41.47    \u001b[0m | \u001b[0m0.07915  \u001b[0m | \u001b[0m432.2    \u001b[0m | \u001b[0m27.98    \u001b[0m |\n",
      "| \u001b[0m152      \u001b[0m | \u001b[0m-3.803   \u001b[0m | \u001b[0m0.4041   \u001b[0m | \u001b[0m4.695    \u001b[0m | \u001b[0m34.61    \u001b[0m | \u001b[0m0.02291  \u001b[0m | \u001b[0m721.2    \u001b[0m | \u001b[0m44.32    \u001b[0m |\n",
      "| \u001b[0m153      \u001b[0m | \u001b[0m-3.792   \u001b[0m | \u001b[0m0.3984   \u001b[0m | \u001b[0m8.134    \u001b[0m | \u001b[0m13.95    \u001b[0m | \u001b[0m0.02243  \u001b[0m | \u001b[0m250.5    \u001b[0m | \u001b[0m18.51    \u001b[0m |\n",
      "| \u001b[0m154      \u001b[0m | \u001b[0m-3.709   \u001b[0m | \u001b[0m0.1285   \u001b[0m | \u001b[0m7.976    \u001b[0m | \u001b[0m16.76    \u001b[0m | \u001b[0m0.03552  \u001b[0m | \u001b[0m994.9    \u001b[0m | \u001b[0m12.43    \u001b[0m |\n",
      "| \u001b[0m155      \u001b[0m | \u001b[0m-3.757   \u001b[0m | \u001b[0m0.291    \u001b[0m | \u001b[0m5.33     \u001b[0m | \u001b[0m31.95    \u001b[0m | \u001b[0m0.033    \u001b[0m | \u001b[0m393.9    \u001b[0m | \u001b[0m40.74    \u001b[0m |\n",
      "| \u001b[0m156      \u001b[0m | \u001b[0m-3.909   \u001b[0m | \u001b[0m0.4957   \u001b[0m | \u001b[0m7.017    \u001b[0m | \u001b[0m35.54    \u001b[0m | \u001b[0m0.06828  \u001b[0m | \u001b[0m631.3    \u001b[0m | \u001b[0m16.57    \u001b[0m |\n",
      "| \u001b[0m157      \u001b[0m | \u001b[0m-3.679   \u001b[0m | \u001b[0m0.08447  \u001b[0m | \u001b[0m8.31     \u001b[0m | \u001b[0m34.21    \u001b[0m | \u001b[0m0.01423  \u001b[0m | \u001b[0m285.1    \u001b[0m | \u001b[0m10.92    \u001b[0m |\n",
      "| \u001b[0m158      \u001b[0m | \u001b[0m-3.752   \u001b[0m | \u001b[0m0.4063   \u001b[0m | \u001b[0m7.947    \u001b[0m | \u001b[0m6.706    \u001b[0m | \u001b[0m0.04668  \u001b[0m | \u001b[0m337.9    \u001b[0m | \u001b[0m9.416    \u001b[0m |\n",
      "| \u001b[0m159      \u001b[0m | \u001b[0m-3.687   \u001b[0m | \u001b[0m0.2648   \u001b[0m | \u001b[0m4.796    \u001b[0m | \u001b[0m39.78    \u001b[0m | \u001b[0m0.01958  \u001b[0m | \u001b[0m212.1    \u001b[0m | \u001b[0m6.879    \u001b[0m |\n",
      "| \u001b[0m160      \u001b[0m | \u001b[0m-3.743   \u001b[0m | \u001b[0m0.29     \u001b[0m | \u001b[0m9.078    \u001b[0m | \u001b[0m25.75    \u001b[0m | \u001b[0m0.0238   \u001b[0m | \u001b[0m154.7    \u001b[0m | \u001b[0m33.15    \u001b[0m |\n",
      "| \u001b[0m161      \u001b[0m | \u001b[0m-3.86    \u001b[0m | \u001b[0m0.3737   \u001b[0m | \u001b[0m5.588    \u001b[0m | \u001b[0m22.36    \u001b[0m | \u001b[0m0.04016  \u001b[0m | \u001b[0m641.5    \u001b[0m | \u001b[0m45.72    \u001b[0m |\n",
      "| \u001b[0m162      \u001b[0m | \u001b[0m-4.075   \u001b[0m | \u001b[0m0.4968   \u001b[0m | \u001b[0m6.805    \u001b[0m | \u001b[0m42.53    \u001b[0m | \u001b[0m0.06007  \u001b[0m | \u001b[0m884.3    \u001b[0m | \u001b[0m47.36    \u001b[0m |\n",
      "| \u001b[0m163      \u001b[0m | \u001b[0m-3.679   \u001b[0m | \u001b[0m0.1043   \u001b[0m | \u001b[0m6.997    \u001b[0m | \u001b[0m12.39    \u001b[0m | \u001b[0m0.09553  \u001b[0m | \u001b[0m150.6    \u001b[0m | \u001b[0m14.66    \u001b[0m |\n",
      "| \u001b[0m164      \u001b[0m | \u001b[0m-3.882   \u001b[0m | \u001b[0m0.4523   \u001b[0m | \u001b[0m6.926    \u001b[0m | \u001b[0m37.15    \u001b[0m | \u001b[0m0.0292   \u001b[0m | \u001b[0m307.0    \u001b[0m | \u001b[0m42.22    \u001b[0m |\n",
      "| \u001b[0m165      \u001b[0m | \u001b[0m-3.683   \u001b[0m | \u001b[0m0.1587   \u001b[0m | \u001b[0m7.634    \u001b[0m | \u001b[0m32.14    \u001b[0m | \u001b[0m0.08198  \u001b[0m | \u001b[0m596.9    \u001b[0m | \u001b[0m5.238    \u001b[0m |\n",
      "| \u001b[0m166      \u001b[0m | \u001b[0m-3.712   \u001b[0m | \u001b[0m0.3918   \u001b[0m | \u001b[0m3.011    \u001b[0m | \u001b[0m8.247    \u001b[0m | \u001b[0m0.02674  \u001b[0m | \u001b[0m404.4    \u001b[0m | \u001b[0m7.405    \u001b[0m |\n",
      "| \u001b[0m167      \u001b[0m | \u001b[0m-3.682   \u001b[0m | \u001b[0m0.06916  \u001b[0m | \u001b[0m9.66     \u001b[0m | \u001b[0m19.35    \u001b[0m | \u001b[0m0.07474  \u001b[0m | \u001b[0m452.2    \u001b[0m | \u001b[0m13.3     \u001b[0m |\n",
      "| \u001b[0m168      \u001b[0m | \u001b[0m-3.739   \u001b[0m | \u001b[0m0.398    \u001b[0m | \u001b[0m4.943    \u001b[0m | \u001b[0m43.34    \u001b[0m | \u001b[0m0.03073  \u001b[0m | \u001b[0m345.0    \u001b[0m | \u001b[0m18.84    \u001b[0m |\n",
      "| \u001b[0m169      \u001b[0m | \u001b[0m-3.687   \u001b[0m | \u001b[0m0.06871  \u001b[0m | \u001b[0m3.376    \u001b[0m | \u001b[0m23.66    \u001b[0m | \u001b[0m0.01387  \u001b[0m | \u001b[0m439.8    \u001b[0m | \u001b[0m5.557    \u001b[0m |\n",
      "| \u001b[0m170      \u001b[0m | \u001b[0m-3.848   \u001b[0m | \u001b[0m0.2729   \u001b[0m | \u001b[0m6.91     \u001b[0m | \u001b[0m41.37    \u001b[0m | \u001b[0m0.04192  \u001b[0m | \u001b[0m764.7    \u001b[0m | \u001b[0m49.99    \u001b[0m |\n",
      "| \u001b[0m171      \u001b[0m | \u001b[0m-3.716   \u001b[0m | \u001b[0m0.3385   \u001b[0m | \u001b[0m3.195    \u001b[0m | \u001b[0m36.46    \u001b[0m | \u001b[0m0.0254   \u001b[0m | \u001b[0m661.3    \u001b[0m | \u001b[0m35.18    \u001b[0m |\n",
      "| \u001b[0m172      \u001b[0m | \u001b[0m-3.713   \u001b[0m | \u001b[0m0.2789   \u001b[0m | \u001b[0m9.223    \u001b[0m | \u001b[0m15.22    \u001b[0m | \u001b[0m0.09755  \u001b[0m | \u001b[0m455.2    \u001b[0m | \u001b[0m8.555    \u001b[0m |\n",
      "| \u001b[0m173      \u001b[0m | \u001b[0m-3.825   \u001b[0m | \u001b[0m0.3472   \u001b[0m | \u001b[0m5.515    \u001b[0m | \u001b[0m20.32    \u001b[0m | \u001b[0m0.0249   \u001b[0m | \u001b[0m572.7    \u001b[0m | \u001b[0m28.79    \u001b[0m |\n",
      "| \u001b[0m174      \u001b[0m | \u001b[0m-3.835   \u001b[0m | \u001b[0m0.2485   \u001b[0m | \u001b[0m9.262    \u001b[0m | \u001b[0m7.056    \u001b[0m | \u001b[0m0.06674  \u001b[0m | \u001b[0m905.7    \u001b[0m | \u001b[0m27.71    \u001b[0m |\n",
      "| \u001b[0m175      \u001b[0m | \u001b[0m-3.681   \u001b[0m | \u001b[0m0.1201   \u001b[0m | \u001b[0m4.177    \u001b[0m | \u001b[0m7.23     \u001b[0m | \u001b[0m0.06703  \u001b[0m | \u001b[0m257.9    \u001b[0m | \u001b[0m42.39    \u001b[0m |\n",
      "| \u001b[0m176      \u001b[0m | \u001b[0m-3.864   \u001b[0m | \u001b[0m0.2925   \u001b[0m | \u001b[0m6.645    \u001b[0m | \u001b[0m31.88    \u001b[0m | \u001b[0m0.04197  \u001b[0m | \u001b[0m843.8    \u001b[0m | \u001b[0m35.02    \u001b[0m |\n",
      "| \u001b[0m177      \u001b[0m | \u001b[0m-3.904   \u001b[0m | \u001b[0m0.4296   \u001b[0m | \u001b[0m8.931    \u001b[0m | \u001b[0m8.031    \u001b[0m | \u001b[0m0.07901  \u001b[0m | \u001b[0m254.5    \u001b[0m | \u001b[0m49.96    \u001b[0m |\n",
      "| \u001b[0m178      \u001b[0m | \u001b[0m-3.775   \u001b[0m | \u001b[0m0.3115   \u001b[0m | \u001b[0m5.309    \u001b[0m | \u001b[0m23.58    \u001b[0m | \u001b[0m0.01714  \u001b[0m | \u001b[0m477.1    \u001b[0m | \u001b[0m22.5     \u001b[0m |\n",
      "| \u001b[0m179      \u001b[0m | \u001b[0m-3.711   \u001b[0m | \u001b[0m0.1402   \u001b[0m | \u001b[0m7.869    \u001b[0m | \u001b[0m9.224    \u001b[0m | \u001b[0m0.09633  \u001b[0m | \u001b[0m486.8    \u001b[0m | \u001b[0m24.0     \u001b[0m |\n",
      "| \u001b[95m180      \u001b[0m | \u001b[95m-3.676   \u001b[0m | \u001b[95m0.04994  \u001b[0m | \u001b[95m6.363    \u001b[0m | \u001b[95m47.21    \u001b[0m | \u001b[95m0.08751  \u001b[0m | \u001b[95m532.9    \u001b[0m | \u001b[95m13.09    \u001b[0m |\n",
      "| \u001b[0m181      \u001b[0m | \u001b[0m-3.694   \u001b[0m | \u001b[0m0.3155   \u001b[0m | \u001b[0m3.826    \u001b[0m | \u001b[0m13.97    \u001b[0m | \u001b[0m0.01719  \u001b[0m | \u001b[0m238.3    \u001b[0m | \u001b[0m29.22    \u001b[0m |\n",
      "| \u001b[0m182      \u001b[0m | \u001b[0m-3.678   \u001b[0m | \u001b[0m0.03206  \u001b[0m | \u001b[0m9.146    \u001b[0m | \u001b[0m29.45    \u001b[0m | \u001b[0m0.02852  \u001b[0m | \u001b[0m405.4    \u001b[0m | \u001b[0m37.9     \u001b[0m |\n",
      "| \u001b[0m183      \u001b[0m | \u001b[0m-3.755   \u001b[0m | \u001b[0m0.4855   \u001b[0m | \u001b[0m4.58     \u001b[0m | \u001b[0m8.794    \u001b[0m | \u001b[0m0.09049  \u001b[0m | \u001b[0m518.3    \u001b[0m | \u001b[0m7.402    \u001b[0m |\n",
      "| \u001b[0m184      \u001b[0m | \u001b[0m-3.786   \u001b[0m | \u001b[0m0.4382   \u001b[0m | \u001b[0m8.412    \u001b[0m | \u001b[0m21.59    \u001b[0m | \u001b[0m0.007255 \u001b[0m | \u001b[0m427.5    \u001b[0m | \u001b[0m9.682    \u001b[0m |\n",
      "| \u001b[0m185      \u001b[0m | \u001b[0m-3.953   \u001b[0m | \u001b[0m0.4906   \u001b[0m | \u001b[0m7.217    \u001b[0m | \u001b[0m26.76    \u001b[0m | \u001b[0m0.02874  \u001b[0m | \u001b[0m944.9    \u001b[0m | \u001b[0m14.51    \u001b[0m |\n",
      "| \u001b[0m186      \u001b[0m | \u001b[0m-3.731   \u001b[0m | \u001b[0m0.4873   \u001b[0m | \u001b[0m6.314    \u001b[0m | \u001b[0m22.03    \u001b[0m | \u001b[0m0.05973  \u001b[0m | \u001b[0m125.1    \u001b[0m | \u001b[0m12.86    \u001b[0m |\n",
      "| \u001b[0m187      \u001b[0m | \u001b[0m-3.766   \u001b[0m | \u001b[0m0.328    \u001b[0m | \u001b[0m5.958    \u001b[0m | \u001b[0m21.55    \u001b[0m | \u001b[0m0.09365  \u001b[0m | \u001b[0m388.3    \u001b[0m | \u001b[0m19.07    \u001b[0m |\n",
      "| \u001b[0m188      \u001b[0m | \u001b[0m-3.823   \u001b[0m | \u001b[0m0.2572   \u001b[0m | \u001b[0m7.931    \u001b[0m | \u001b[0m7.572    \u001b[0m | \u001b[0m0.09861  \u001b[0m | \u001b[0m927.9    \u001b[0m | \u001b[0m27.39    \u001b[0m |\n",
      "| \u001b[0m189      \u001b[0m | \u001b[0m-3.702   \u001b[0m | \u001b[0m0.2312   \u001b[0m | \u001b[0m9.132    \u001b[0m | \u001b[0m33.4     \u001b[0m | \u001b[0m0.007783 \u001b[0m | \u001b[0m188.6    \u001b[0m | \u001b[0m14.03    \u001b[0m |\n",
      "| \u001b[0m190      \u001b[0m | \u001b[0m-3.755   \u001b[0m | \u001b[0m0.478    \u001b[0m | \u001b[0m6.161    \u001b[0m | \u001b[0m44.03    \u001b[0m | \u001b[0m0.05613  \u001b[0m | \u001b[0m110.8    \u001b[0m | \u001b[0m25.47    \u001b[0m |\n",
      "| \u001b[0m191      \u001b[0m | \u001b[0m-3.68    \u001b[0m | \u001b[0m0.1113   \u001b[0m | \u001b[0m5.124    \u001b[0m | \u001b[0m23.37    \u001b[0m | \u001b[0m0.06628  \u001b[0m | \u001b[0m319.3    \u001b[0m | \u001b[0m7.872    \u001b[0m |\n",
      "| \u001b[0m192      \u001b[0m | \u001b[0m-3.886   \u001b[0m | \u001b[0m0.3837   \u001b[0m | \u001b[0m8.444    \u001b[0m | \u001b[0m25.08    \u001b[0m | \u001b[0m0.006254 \u001b[0m | \u001b[0m952.1    \u001b[0m | \u001b[0m14.09    \u001b[0m |\n",
      "| \u001b[0m193      \u001b[0m | \u001b[0m-3.854   \u001b[0m | \u001b[0m0.4583   \u001b[0m | \u001b[0m7.854    \u001b[0m | \u001b[0m46.49    \u001b[0m | \u001b[0m0.0923   \u001b[0m | \u001b[0m296.5    \u001b[0m | \u001b[0m27.33    \u001b[0m |\n",
      "| \u001b[0m194      \u001b[0m | \u001b[0m-4.013   \u001b[0m | \u001b[0m0.4463   \u001b[0m | \u001b[0m8.31     \u001b[0m | \u001b[0m21.24    \u001b[0m | \u001b[0m0.03608  \u001b[0m | \u001b[0m620.4    \u001b[0m | \u001b[0m39.32    \u001b[0m |\n",
      "| \u001b[0m195      \u001b[0m | \u001b[0m-3.762   \u001b[0m | \u001b[0m0.4998   \u001b[0m | \u001b[0m3.27     \u001b[0m | \u001b[0m16.42    \u001b[0m | \u001b[0m0.0773   \u001b[0m | \u001b[0m725.6    \u001b[0m | \u001b[0m22.06    \u001b[0m |\n",
      "| \u001b[0m196      \u001b[0m | \u001b[0m-3.759   \u001b[0m | \u001b[0m0.1806   \u001b[0m | \u001b[0m8.047    \u001b[0m | \u001b[0m34.21    \u001b[0m | \u001b[0m0.08144  \u001b[0m | \u001b[0m894.1    \u001b[0m | \u001b[0m21.91    \u001b[0m |\n",
      "| \u001b[0m197      \u001b[0m | \u001b[0m-3.684   \u001b[0m | \u001b[0m0.06187  \u001b[0m | \u001b[0m6.893    \u001b[0m | \u001b[0m19.74    \u001b[0m | \u001b[0m0.09021  \u001b[0m | \u001b[0m484.7    \u001b[0m | \u001b[0m26.92    \u001b[0m |\n",
      "| \u001b[0m198      \u001b[0m | \u001b[0m-3.854   \u001b[0m | \u001b[0m0.2581   \u001b[0m | \u001b[0m6.925    \u001b[0m | \u001b[0m31.91    \u001b[0m | \u001b[0m0.04869  \u001b[0m | \u001b[0m878.9    \u001b[0m | \u001b[0m46.08    \u001b[0m |\n",
      "| \u001b[0m199      \u001b[0m | \u001b[0m-3.84    \u001b[0m | \u001b[0m0.3791   \u001b[0m | \u001b[0m5.123    \u001b[0m | \u001b[0m36.99    \u001b[0m | \u001b[0m0.03983  \u001b[0m | \u001b[0m585.3    \u001b[0m | \u001b[0m32.69    \u001b[0m |\n",
      "| \u001b[0m200      \u001b[0m | \u001b[0m-3.81    \u001b[0m | \u001b[0m0.2525   \u001b[0m | \u001b[0m6.26     \u001b[0m | \u001b[0m32.85    \u001b[0m | \u001b[0m0.05942  \u001b[0m | \u001b[0m714.0    \u001b[0m | \u001b[0m32.79    \u001b[0m |\n",
      "| \u001b[0m201      \u001b[0m | \u001b[0m-3.687   \u001b[0m | \u001b[0m0.1566   \u001b[0m | \u001b[0m4.979    \u001b[0m | \u001b[0m8.0      \u001b[0m | \u001b[0m0.03967  \u001b[0m | \u001b[0m290.5    \u001b[0m | \u001b[0m16.02    \u001b[0m |\n",
      "| \u001b[0m202      \u001b[0m | \u001b[0m-3.724   \u001b[0m | \u001b[0m0.4828   \u001b[0m | \u001b[0m3.032    \u001b[0m | \u001b[0m13.09    \u001b[0m | \u001b[0m0.06557  \u001b[0m | \u001b[0m387.9    \u001b[0m | \u001b[0m9.646    \u001b[0m |\n",
      "| \u001b[0m203      \u001b[0m | \u001b[0m-3.698   \u001b[0m | \u001b[0m0.1057   \u001b[0m | \u001b[0m7.315    \u001b[0m | \u001b[0m18.23    \u001b[0m | \u001b[0m0.02874  \u001b[0m | \u001b[0m480.6    \u001b[0m | \u001b[0m28.44    \u001b[0m |\n",
      "| \u001b[0m204      \u001b[0m | \u001b[0m-3.872   \u001b[0m | \u001b[0m0.3101   \u001b[0m | \u001b[0m9.325    \u001b[0m | \u001b[0m38.38    \u001b[0m | \u001b[0m0.008879 \u001b[0m | \u001b[0m899.0    \u001b[0m | \u001b[0m29.64    \u001b[0m |\n",
      "| \u001b[0m205      \u001b[0m | \u001b[0m-3.688   \u001b[0m | \u001b[0m0.1279   \u001b[0m | \u001b[0m3.125    \u001b[0m | \u001b[0m41.71    \u001b[0m | \u001b[0m0.06702  \u001b[0m | \u001b[0m744.6    \u001b[0m | \u001b[0m22.38    \u001b[0m |\n",
      "| \u001b[0m206      \u001b[0m | \u001b[0m-3.727   \u001b[0m | \u001b[0m0.2209   \u001b[0m | \u001b[0m8.425    \u001b[0m | \u001b[0m6.882    \u001b[0m | \u001b[0m0.02782  \u001b[0m | \u001b[0m386.2    \u001b[0m | \u001b[0m16.01    \u001b[0m |\n",
      "| \u001b[0m207      \u001b[0m | \u001b[0m-3.717   \u001b[0m | \u001b[0m0.3619   \u001b[0m | \u001b[0m6.778    \u001b[0m | \u001b[0m36.36    \u001b[0m | \u001b[0m0.07606  \u001b[0m | \u001b[0m225.7    \u001b[0m | \u001b[0m10.82    \u001b[0m |\n",
      "| \u001b[0m208      \u001b[0m | \u001b[0m-3.856   \u001b[0m | \u001b[0m0.3287   \u001b[0m | \u001b[0m8.272    \u001b[0m | \u001b[0m10.57    \u001b[0m | \u001b[0m0.01886  \u001b[0m | \u001b[0m381.8    \u001b[0m | \u001b[0m40.85    \u001b[0m |\n",
      "| \u001b[0m209      \u001b[0m | \u001b[0m-3.759   \u001b[0m | \u001b[0m0.4775   \u001b[0m | \u001b[0m5.199    \u001b[0m | \u001b[0m11.67    \u001b[0m | \u001b[0m0.07145  \u001b[0m | \u001b[0m810.2    \u001b[0m | \u001b[0m5.808    \u001b[0m |\n",
      "| \u001b[0m210      \u001b[0m | \u001b[0m-3.701   \u001b[0m | \u001b[0m0.1553   \u001b[0m | \u001b[0m6.989    \u001b[0m | \u001b[0m34.06    \u001b[0m | \u001b[0m0.003283 \u001b[0m | \u001b[0m213.6    \u001b[0m | \u001b[0m40.91    \u001b[0m |\n",
      "=================================================================================================\n",
      "#################################################################\n",
      "Found Network with Optimal target result of -3.67587348422473\n",
      "Parameters: {'learning_rate': 0.04994015617051042, 'max_depth': 6.3629067353851525, 'min_child_weight': 47.210572780597815, 'min_split_gain': 0.08751279891580666, 'n_estimators': 532.873512954938, 'num_leaves': 13.091290105138171}\n",
      "#################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(learning_rate=0.04994015617051042, max_depth=6,\n",
       "              min_child_weight=47.210572780597815,\n",
       "              min_split_gain=0.08751279891580666, n_estimators=532,\n",
       "              num_leaves=13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = Bayesian_Optimization(lgbm_objfunc, pbounds, n_init_random_explorations=50, n_iter = 160)\n",
    "\n",
    "\n",
    "final_model = lgb.LGBMRegressor(**best_params)\n",
    "final_model.num_leaves = int(best_params['num_leaves'])\n",
    "final_model.max_depth = int(best_params['max_depth'])\n",
    "final_model.n_estimators = int(best_params['n_estimators'])\n",
    "features = df_train_merge.drop(columns=['target', 'card_id']).columns\n",
    "final_model.fit(df_train_merge[features], df_train_merge['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419d924b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T18:05:45.055820Z",
     "iopub.status.busy": "2023-04-27T18:05:45.055127Z",
     "iopub.status.idle": "2023-04-27T18:05:45.065166Z",
     "shell.execute_reply": "2023-04-27T18:05:45.063893Z"
    },
    "papermill": {
     "duration": 0.037143,
     "end_time": "2023-04-27T18:05:45.067911",
     "exception": false,
     "start_time": "2023-04-27T18:05:45.030768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_for_production(dataset):\n",
    "    features = dataset.drop(columns=['card_id']).columns\n",
    "    return final_model.predict(dataset[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7dc8a",
   "metadata": {
    "papermill": {
     "duration": 0.018692,
     "end_time": "2023-04-27T18:05:45.105223",
     "exception": false,
     "start_time": "2023-04-27T18:05:45.086531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate Output for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c1a1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T18:05:45.144667Z",
     "iopub.status.busy": "2023-04-27T18:05:45.144215Z",
     "iopub.status.idle": "2023-04-27T18:05:48.501344Z",
     "shell.execute_reply": "2023-04-27T18:05:48.500309Z"
    },
    "papermill": {
     "duration": 3.38026,
     "end_time": "2023-04-27T18:05:48.504260",
     "exception": false,
     "start_time": "2023-04-27T18:05:45.124000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare Test Dataset\n",
    "df_test_merge = pd.read_csv(f'{INPUT_PREPROCESSED_DIR}/test_merged.csv', index_col=0)\n",
    "df_test_merge.drop(['first_active_month'], axis=1, inplace=True)\n",
    "\n",
    "card_id = df_test_merge.card_id.unique()\n",
    "\n",
    "# Predict\n",
    "y_test_predict = model_for_production(df_test_merge)\n",
    "df_test_predict = pd.DataFrame({'card_id': card_id, 'target': y_test_predict})\n",
    "\n",
    "# Save to csv\n",
    "df_test_predict.to_csv('best_of_pure_lgbm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26446.294223,
   "end_time": "2023-04-27T18:05:51.642916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-27T10:45:05.348693",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
