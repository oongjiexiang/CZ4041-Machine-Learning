{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55d5848",
   "metadata": {
    "papermill": {
     "duration": 0.005759,
     "end_time": "2023-04-27T10:45:15.762503",
     "exception": false,
     "start_time": "2023-04-27T10:45:15.756744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "Source: https://www.kaggle.com/competitions/elo-merchant-category-recommendation/data\n",
    "\n",
    "### Table of Contents\n",
    "- [Libraries](#libraries)\n",
    "- [Utils](#utils)\n",
    "- [Datasets](#datasets)\n",
    "- [Data Engineering](#data-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836af62",
   "metadata": {
    "papermill": {
     "duration": 0.004112,
     "end_time": "2023-04-27T10:45:15.771328",
     "exception": false,
     "start_time": "2023-04-27T10:45:15.767216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Libraries <a id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f53a323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T10:45:15.782716Z",
     "iopub.status.busy": "2023-04-27T10:45:15.781535Z",
     "iopub.status.idle": "2023-04-27T10:45:19.048451Z",
     "shell.execute_reply": "2023-04-27T10:45:19.047026Z"
    },
    "papermill": {
     "duration": 3.276188,
     "end_time": "2023-04-27T10:45:19.051943",
     "exception": false,
     "start_time": "2023-04-27T10:45:15.775755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "# Tools\n",
    "import math\n",
    "import datetime\n",
    "from typing import List, Union\n",
    "\n",
    "# ML Tools\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# CONSTANTS\n",
    "SEED = 123\n",
    "TEST_PERC = 0.05\n",
    "INPUT_ELO_DIR = '/kaggle/input/elo-merchant-category-recommendation'\n",
    "INPUT_PREPROCESSED_DIR = '/kaggle/input/cz4041-preprocessed'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import unittest\n",
    "import threading\n",
    "\n",
    "np.random.seed(400)\n",
    "random.seed(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f35e5",
   "metadata": {
    "papermill": {
     "duration": 0.004465,
     "end_time": "2023-04-27T10:45:19.061356",
     "exception": false,
     "start_time": "2023-04-27T10:45:19.056891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utils <a id=\"utils\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2f68f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T10:45:19.073706Z",
     "iopub.status.busy": "2023-04-27T10:45:19.073245Z",
     "iopub.status.idle": "2023-04-27T10:45:19.144405Z",
     "shell.execute_reply": "2023-04-27T10:45:19.143321Z"
    },
    "papermill": {
     "duration": 0.080969,
     "end_time": "2023-04-27T10:45:19.147156",
     "exception": false,
     "start_time": "2023-04-27T10:45:19.066187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarizeDF(df:DataFrame)->DataFrame:\n",
    "    \"\"\"This function shows a basic summary of the given dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas DataFrame\n",
    "    This specifies the dataframe to be summarized.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame: This is a table of summary of the given dataset.\n",
    "    \"\"\"    \n",
    "    variables, dtypes, count, unique, missing, pc_missing = [], [], [], [], [], []\n",
    "    \n",
    "    for item in df.columns:\n",
    "        variables.append(item)\n",
    "        dtypes.append(df[item].dtype)\n",
    "        count.append(len(df[item]))\n",
    "        unique.append(len(df[item].unique()))\n",
    "        missing.append(df[item].isna().sum())\n",
    "        pc_missing.append(round((df[item].isna().sum() / len(df[item])) * 100, 2))\n",
    "\n",
    "    output = pd.DataFrame({\n",
    "        'column_name': variables, \n",
    "        'dtype': dtypes,\n",
    "        'count': count,\n",
    "        'unique': unique,\n",
    "        'missing': missing, \n",
    "        'percentage_missing_data': pc_missing\n",
    "    })    \n",
    "        \n",
    "    return output\n",
    "\n",
    "def preprocess_data(df:DataFrame=None)->DataFrame:\n",
    "    \"\"\"This function preprocess the data into a specific form for the computation.\n",
    "    Given a DataFrame (df), impute with mode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas DataFrame\n",
    "    This specifies the data to be preprocessed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame: This specifies the preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise Exception(\"Expected a DataFrame, no DataFrame supplied.\")\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    for col in df.columns[df.isnull().any()]:\n",
    "        df_copy[col].fillna(df_copy['card_id'].map(df_copy.groupby('card_id')[col].apply(lambda x: x.mode().iloc[0] if not x.isnull().all() else np.nan)).fillna(df_copy[col].mode().iloc[0]), inplace=True)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "def feature_engineering(df:DataFrame=None)->DataFrame:\n",
    "    \"\"\"This function perform feature engineering on the input Data\"\"\"\n",
    "    \n",
    "    def get_new_columns(name:str, aggs:list)->list: # Nested function for feature engineering\n",
    "        \"\"\"This function creates new column names for the aggregation of the features.\"\"\"\n",
    "        return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    \n",
    "    # Make copy of df\n",
    "    df_historical_transactions_copy = df.copy()\n",
    " \n",
    "    # Convert DT columns to Pandas DT\n",
    "    df_historical_transactions_copy['purchase_date'] = pd.to_datetime(df_historical_transactions_copy['purchase_date'])\n",
    " \n",
    "    # Feature Engineer columns from purchase_date\n",
    "    df_historical_transactions_copy['year'] = df_historical_transactions_copy['purchase_date'].dt.year\n",
    "    df_historical_transactions_copy['weekofyear'] = df_historical_transactions_copy['purchase_date'].dt.isocalendar().week\n",
    "    df_historical_transactions_copy['month'] = df_historical_transactions_copy['purchase_date'].dt.month\n",
    "    df_historical_transactions_copy['dayofweek'] = df_historical_transactions_copy['purchase_date'].dt.dayofweek\n",
    "    df_historical_transactions_copy['weekend'] = (df_historical_transactions_copy.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df_historical_transactions_copy['hour'] = df_historical_transactions_copy['purchase_date'].dt.hour\n",
    " \n",
    "    # Encode Binary Features\n",
    "    df_historical_transactions_copy['authorized_flag'] = df_historical_transactions_copy['authorized_flag'].map({\"Y\":1, 'N':0})\n",
    "    df_historical_transactions_copy['category_1'] = df_historical_transactions_copy['category_1'].map({'Y':1, 'N':0})\n",
    " \n",
    "    # Feature Engineer Month Diff/Lag\n",
    "    df_historical_transactions_copy['month_diff'] = ((datetime.datetime.today() - df_historical_transactions_copy['purchase_date']).dt.days)//30\n",
    "    df_historical_transactions_copy['month_diff'] += df_historical_transactions_copy['month_lag']\n",
    "    \n",
    "    # Getting Centrality of the Data\n",
    "    aggs = {}\n",
    "    for col in ['month','hour','weekofyear','dayofweek','year', 'state_id','subsector_id']:\n",
    "        aggs[col] = ['nunique']\n",
    " \n",
    "    # Feature Engineering using Univariate Analysis\n",
    "    aggs['authorized_flag'] = ['sum', 'mean']\n",
    "    aggs['card_id'] = ['size']\n",
    "    aggs['category_1'] = ['sum', 'mean']\n",
    "    aggs['installments'] = ['sum','max','min','mean','var']\n",
    "    aggs['month_lag'] = ['max','min','mean','var']\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_diff'] = ['mean']\n",
    "    aggs['weekend'] = ['sum', 'mean']\n",
    " \n",
    "    for col in ['category_2','category_3']:\n",
    "        df_historical_transactions_copy[col+'_mean'] = df_historical_transactions_copy.groupby([col])['purchase_amount'].transform('mean')\n",
    "        aggs[col+'_mean'] = ['mean']    \n",
    " \n",
    "    new_columns = get_new_columns('hist',aggs)\n",
    "    \n",
    "    # Group Aggregations by card_id\n",
    "    df_historical_transactions_copy_group = df_historical_transactions_copy.groupby('card_id').agg(aggs)\n",
    " \n",
    "    # Remove Multilevel Indexing with New Column Names\n",
    "    df_historical_transactions_copy_group.columns = new_columns\n",
    "    \n",
    "    # Reset Index\n",
    "    df_historical_transactions_copy_group.reset_index(drop=False,inplace=True)\n",
    "    \n",
    "    # Cast variable to pandas Datetime\n",
    "    df_historical_transactions_copy_group['hist_purchase_date_max'] = pd.to_datetime(df_historical_transactions_copy_group['hist_purchase_date_max'])\n",
    "    df_historical_transactions_copy_group['hist_purchase_date_min'] = pd.to_datetime(df_historical_transactions_copy_group['hist_purchase_date_min'])\n",
    "\n",
    "    return df_historical_transactions_copy_group\n",
    "\n",
    "def merge_data(key:str=None, dfs:List[DataFrame]=None)->DataFrame:\n",
    "    \"\"\"This function takes in multiple dataframes and performs a left outer join on a key.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    key: str\n",
    "    This species the joining key.\n",
    "    \n",
    "    dfs: list of pandas DataFrame\n",
    "    This specifies the list of DataFrames to perform left outer join based on a key.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame: This specifies the resultant DataFrame from the merging operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanity Check\n",
    "    if key is None:\n",
    "        raise Exception(\"Expected a key, no key supplied.\")\n",
    "        \n",
    "    if not isinstance(key, str):\n",
    "        raise Exception(f\"Expected type str for key, {type(key)} was supplied.\")\n",
    "    \n",
    "    if dfs is None or not len(dfs) == 2:\n",
    "        raise Exception(\"Expected at least two DataFrame.\")\n",
    "        \n",
    "    if any(type(x) != DataFrame for x in dfs):\n",
    "        raise Exception(\"At least one DataFrame is not the correct DataType.\")\n",
    "        \n",
    "    # Iterate through DataFrames to perform merge operation\n",
    "    df_res = dfs[0]\n",
    "    \n",
    "    for df in dfs[1:]:\n",
    "        df_res = pd.merge(left=df_res, right=df, how='left', left_on=key, right_on=key)\n",
    "    \n",
    "    return df_res\n",
    "\n",
    "def zhenjie_miracle(df:DataFrame)->DataFrame:\n",
    "    \"\"\"This function perform feature engineering on purchase_max and purchase_min and one-hot encoding on the \n",
    "    input Data which must be a merged dataframe of train dataset and trans (hist and new) dataset after running \n",
    "    feature_engineering function\"\"\"\n",
    "    \n",
    "    # Make copy of df\n",
    "    df_historical_transactions_copy_group = df.copy()\n",
    "\n",
    "    brazil_holiday_list=[ \n",
    "            '01-01-17', '14-02-17', '28-08-17', '14-04-17', '16-04-17', '21-04-17',\n",
    "            '01-05-17', '15-06-17', '07-09-17', '12-10-17', '02-11-17', '15-11-17', \n",
    "            '24-12-17', '25-12-17', '31-12-17',\n",
    "            '01-01-18', '14-02-18', '28-08-18', '14-04-18', '16-04-18', '21-04-18',\n",
    "            '01-05-18', '15-06-18', '07-09-18', '12-10-18', '02-11-18', '15-11-18', \n",
    "            '24-12-18', '25-12-18', '31-12-18'\n",
    "      ]\n",
    "    df_historical_transactions_copy_group['purchase_max_is_holiday'] = df_historical_transactions_copy_group['hist_purchase_date_max'].isin(brazil_holiday_list).astype(int)\n",
    "    df_historical_transactions_copy_group['purchase_min_is_holiday'] = df_historical_transactions_copy_group['hist_purchase_date_min'].isin(brazil_holiday_list).astype(int)\n",
    "    \n",
    "    df_historical_transactions_copy_group_dummies = pd.get_dummies(df_historical_transactions_copy_group['feature_1'], prefix='feature_1', drop_first=True)\n",
    "    df_historical_transactions_copy_group = pd.concat([df_historical_transactions_copy_group, df_historical_transactions_copy_group_dummies], axis=1)\n",
    "    df_historical_transactions_copy_group_dummies = pd.get_dummies(df_historical_transactions_copy_group['feature_2'], prefix='feature_2', drop_first=True)\n",
    "    df_historical_transactions_copy_group = pd.concat([df_historical_transactions_copy_group, df_historical_transactions_copy_group_dummies], axis=1)\n",
    "\n",
    "    return df_historical_transactions_copy_group\n",
    "\n",
    "\n",
    "def pengaik_miracle(df:DataFrame=None)->DataFrame:\n",
    "    \n",
    "    \"\"\"This function perform feature engineering on average monthly purchase amount raw of positive month lags \n",
    "    over that of negative. It also performs feature engineering on the ratio of purchase amount raw of \n",
    "    month_lag=i/month_lag=i-1 for each card_id and returns the average ratio as a column\n",
    "    input Data which must be a concat dataframe of trans (hist and new) dataset \"\"\"\n",
    "    \n",
    "    transactions_copy = df.copy()\n",
    "    \n",
    "    # Reverse purchase_amount\n",
    "    transactions_copy['purchase_amount_raw'] = np.round(transactions_copy['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
    "\n",
    "    # Group transactions_copy by card_id and month_lag\n",
    "    grouped_transactions_copy = transactions_copy.groupby(['card_id', 'month_lag']).agg({'purchase_amount_raw': 'mean'}).reset_index()\n",
    "\n",
    "    # Separate transactions_copy into two groups based on month_lag\n",
    "    lag_le_0 = grouped_transactions_copy[grouped_transactions_copy['month_lag'] <= 0]\n",
    "    lag_gt_0 = grouped_transactions_copy[grouped_transactions_copy['month_lag'] > 0]\n",
    "\n",
    "    # Calculate the monthly average purchase amount for each group\n",
    "    lag_le_0_monthly_average_raw = lag_le_0.groupby('card_id')['purchase_amount_raw'].mean().reset_index().rename(columns={'purchase_amount_raw': 'monthly_average_purchase_amount_raw_for_month_lag_le_0'})\n",
    "    lag_gt_0_monthly_average_raw = lag_gt_0.groupby('card_id')['purchase_amount_raw'].mean().reset_index().rename(columns={'purchase_amount_raw': 'monthly_average_purchase_amount_raw_for_month_lag_gt_0'})\n",
    "\n",
    "    # Merge the new columns with the original transactions_copy dataframe\n",
    "    transactions_copy = transactions_copy.merge(lag_le_0_monthly_average_raw, on='card_id', how='left')\n",
    "    transactions_copy = transactions_copy.merge(lag_gt_0_monthly_average_raw, on='card_id', how='left')\n",
    "\n",
    "    transactions_copy['ratio_between_ave_monthly_purchase_raw_for_positive_and_negative'] = transactions_copy['monthly_average_purchase_amount_raw_for_month_lag_gt_0'] / transactions_copy['monthly_average_purchase_amount_raw_for_month_lag_le_0']\n",
    "\n",
    "    # Find the minimum month_lag for each card_id and set the index to 'card_id'\n",
    "    min_month_lag_per_card = transactions_copy.groupby('card_id', as_index=False)['month_lag'].min().set_index('card_id')\n",
    "\n",
    "    # Fill in missing month_lag values for each card_id\n",
    "    unique_card_ids = transactions_copy['card_id'].unique()\n",
    "    min_month_lag = transactions_copy['month_lag'].min()\n",
    "    max_month_lag = transactions_copy['month_lag'].max()\n",
    "\n",
    "    complete_data = []\n",
    "\n",
    "    for card_id in unique_card_ids:\n",
    "        # Use .loc[] accessor to look up the minimum month_lag for each card_id\n",
    "        card_min_month_lag = min_month_lag_per_card.loc[card_id]['month_lag']\n",
    "        for month_lag in range(card_min_month_lag, max_month_lag + 1):\n",
    "            complete_data.append([card_id, month_lag, 0])\n",
    "\n",
    "    complete_transactions_copy = pd.DataFrame(complete_data, columns=['card_id', 'month_lag', 'purchase_amount_raw'])\n",
    "\n",
    "    # Compute the purchase_amount_raw sum for each card_id and month_lag combination\n",
    "    grouped_transactions_copy = transactions_copy.groupby(['card_id', 'month_lag'], as_index=False)['purchase_amount_raw'].sum()\n",
    "\n",
    "    # Merge the complete_transactions_copy dataframe with the grouped_transactions_copy dataframe\n",
    "    merged_transactions_copy = pd.merge(complete_transactions_copy, grouped_transactions_copy, on=['card_id', 'month_lag'], how='left', suffixes=('', '_y'))\n",
    "    merged_transactions_copy['purchase_amount_raw'] = merged_transactions_copy['purchase_amount_raw_y'].fillna(merged_transactions_copy['purchase_amount_raw'])\n",
    "\n",
    "    # Calculate the ratio of purchase_amount_raw for each month_lag=i/month_lag=i-1\n",
    "    merged_transactions_copy['prev_month_purchase_amount'] = merged_transactions_copy.groupby('card_id')['purchase_amount_raw'].shift(1)\n",
    "    merged_transactions_copy['ratio'] = np.where(merged_transactions_copy['prev_month_purchase_amount'] != 0, merged_transactions_copy['purchase_amount_raw'] / merged_transactions_copy['prev_month_purchase_amount'], np.nan)\n",
    "\n",
    "    # Compute the average of these ratios for each card_id\n",
    "    average_ratios = merged_transactions_copy.groupby('card_id', as_index=False)['ratio'].mean()\n",
    "\n",
    "    # Handling division by zero cases by replacing np.inf with np.nan and then replacing np.nan with a suitable value (e.g., 1)\n",
    "    average_ratios['ratio'] = average_ratios['ratio'].replace([np.inf, -np.inf], np.nan).fillna(1)\n",
    "\n",
    "    # Merge average_ratios with transactions_copy DataFrame\n",
    "    feature_engineered_transactions_copy = transactions_copy.merge(average_ratios, on='card_id', how='left')\n",
    "\n",
    "    return feature_engineered_transactions_copy\n",
    "\n",
    "\n",
    "def createData(df:DataFrame=None, df_t:DataFrame=None)->DataFrame:\n",
    "    \"\"\"This function transform the given datasets into a suitable dataset for training/testing.\n",
    "    \n",
    "    algorithm\n",
    "        0. (optional) Subset the data based on card_ids in train/test (improve performance)\n",
    "        1. PA Miracle\n",
    "        2. Impute with Mode\n",
    "        3. Feature Engineering\n",
    "        4. Merge df_transactions from 1 - 3 to train/test (IMPORTANT)\n",
    "        5. ZJ Miracle\n",
    "        6. Remove unnecessary columns\n",
    "    endalgorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "    This specifies the dataframe containing transactions details. Ideally, this should be a combination of \n",
    "    new_historical and historical transactions datagframes.\n",
    "    \n",
    "    df_t: DataFrame\n",
    "    This specifies the train or test dataframe.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame: Valid DataFrame after the preprocessing and imputations\n",
    "    \"\"\"\n",
    "    # Run Pengaik's Miracle\n",
    "    df_transactions_t = pengaik_miracle(df)\n",
    "    print(\"Card_ID Uniqueness (PA Miracle)\", len(df_transactions_t['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Impute with Mode\n",
    "    df_impute_mode = preprocess_data(df=df_transactions_t)\n",
    "    print(\"Card_ID Uniqueness (Impute Mode)\", len(df_impute_mode['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Store PA's Ratios\n",
    "    df_ratios = df_impute_mode[['card_id',\n",
    "       'monthly_average_purchase_amount_raw_for_month_lag_le_0',\n",
    "       'monthly_average_purchase_amount_raw_for_month_lag_gt_0',\n",
    "       'ratio_between_ave_monthly_purchase_raw_for_positive_and_negative',\n",
    "       'ratio']].drop_duplicates()\n",
    "\n",
    "    # Sanity Check for n_rows and uniqueness of card_id\n",
    "    print(\"Card_ID Uniqueness\", len(df_ratios['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "    \n",
    "    # Sanity Check for n_rows and uniqueness of card_id\n",
    "    print(\"Card_ID Uniqueness (PA Ratio)\", len(df_ratios['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "    \n",
    "    # Feature Engineering\n",
    "    df_aggregated_cols = feature_engineering(df_impute_mode)\n",
    "    print(\"Card_ID Uniqueness (Feature Engineering)\", len(df_aggregated_cols['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Merge PA's Ratio\n",
    "    df_aggregated_cols = merge_data('card_id', [df_ratios, df_aggregated_cols])\n",
    "\n",
    "    # Sanity Check for n_rows and uniqueness of card_id\n",
    "    print(\"Card_ID Uniqueness (Merge PA Ratio)\", len(df_aggregated_cols['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "\n",
    "    # Merge Transactions and T\n",
    "    df_t_merge = merge_data('card_id', [df_t, df_aggregated_cols])\n",
    "    print(\"Card_ID Uniqueness (Merging)\", len(df_t_merge['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Execute Zhen Jie's Miracle\n",
    "    df_t_merge = zhenjie_miracle(df_t_merge)\n",
    "    print(\"Card_ID Uniqueness (ZJ Miracle)\", len(df_t_merge['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Impute Missing Data\n",
    "    df_t_merge.fillna(\"2017-01\", inplace=True)\n",
    "\n",
    "    # Engineered by Zhen Jie so Remove\n",
    "    df_t_merge.drop(columns=['hist_purchase_date_max', 'hist_purchase_date_min'], inplace=True)\n",
    "    print(\"Card_ID Uniqueness (Imputations and Drop)\", len(df_test_merge['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "    \n",
    "    # Final Sanity Check\n",
    "    print(\"***FINAL***\")\n",
    "    print('n_rows:', format(df_t_merge.shape[0], \"_\"), end='\\n\\n')\n",
    "    print(\"Columns:\\n\", \", \".join(df_t_merge.columns), sep='')\n",
    "    \n",
    "    return df_t_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543d30b",
   "metadata": {
    "papermill": {
     "duration": 0.004594,
     "end_time": "2023-04-27T10:45:19.156700",
     "exception": false,
     "start_time": "2023-04-27T10:45:19.152106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Datasets <a id=\"datasets\"></a>\n",
    "\n",
    "1. Customer has a ```card_id``` as uuid.\n",
    "2. Each customer can make at least one transaction to merchants.\n",
    "3. Merchant has ```merchant_id``` as uuid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95dadc2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T10:45:19.167874Z",
     "iopub.status.busy": "2023-04-27T10:45:19.167413Z",
     "iopub.status.idle": "2023-04-27T10:47:28.045932Z",
     "shell.execute_reply": "2023-04-27T10:47:28.040929Z"
    },
    "papermill": {
     "duration": 128.897181,
     "end_time": "2023-04-27T10:47:28.058558",
     "exception": false,
     "start_time": "2023-04-27T10:45:19.161377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Historical Transactions\n",
    "tp = pd.read_csv(f'{INPUT_ELO_DIR}/historical_transactions.csv', iterator=True, chunksize=2_000_000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\n",
    "df_historical_transactions = pd.concat(tp, ignore_index=True) \n",
    "\n",
    "# New Historical Transactions\n",
    "tp = pd.read_csv(f'{INPUT_ELO_DIR}/new_merchant_transactions.csv', iterator=True, chunksize=2_000_000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\n",
    "df_new_historical_transactions = pd.concat(tp, ignore_index=True) \n",
    "\n",
    "# Train Data\n",
    "df_train = pd.read_csv(f'{INPUT_ELO_DIR}/train.csv')\n",
    "\n",
    "# Test Data\n",
    "df_test = pd.read_csv(f'{INPUT_ELO_DIR}/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d358c3",
   "metadata": {},
   "source": [
    "# Data Engineering <a id=\"data-engineering\"></a>\n",
    "The preprocessing and feature engineering steps are memory-intensive and time-consuming, so this notebook was run to generate the engineered data for `train.csv` and `test.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f349ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineered Train Data \n",
    "df_full_transactions = pd.concat([df_historical_transactions, df_new_historical_transactions], axis=0)\n",
    "df_train_engineered = createData(df_full_transactions, df_train)\n",
    "\n",
    "# Engineered Test Data\n",
    "df_test_engineered = createData(df_full_transactions, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "df_train_engineered.to_csv('output.csv')\n",
    "df_test_engineered.to_csv('test_merged.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26446.294223,
   "end_time": "2023-04-27T18:05:51.642916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-27T10:45:05.348693",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
