{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e74b7b3",
   "metadata": {
    "papermill": {
     "duration": 0.010835,
     "end_time": "2023-04-28T14:16:13.201217",
     "exception": false,
     "start_time": "2023-04-28T14:16:13.190382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "Source: https://www.kaggle.com/competitions/elo-merchant-category-recommendation/data\n",
    "\n",
    "### Table of Contents\n",
    "- [Libraries](#libraries)\n",
    "- [Utils](#utils)\n",
    "- [Datasets](#datasets)\n",
    "- [Machine Learning](#custom-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09556102",
   "metadata": {
    "papermill": {
     "duration": 0.009139,
     "end_time": "2023-04-28T14:16:13.220061",
     "exception": false,
     "start_time": "2023-04-28T14:16:13.210922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Libraries <a id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041d5ed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:16:13.241431Z",
     "iopub.status.busy": "2023-04-28T14:16:13.240216Z",
     "iopub.status.idle": "2023-04-28T14:16:15.334581Z",
     "shell.execute_reply": "2023-04-28T14:16:15.333306Z"
    },
    "papermill": {
     "duration": 2.108067,
     "end_time": "2023-04-28T14:16:15.337533",
     "exception": false,
     "start_time": "2023-04-28T14:16:13.229466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "# Tools\n",
    "import math\n",
    "import datetime\n",
    "from typing import List, Union\n",
    "\n",
    "# ML Tools\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# CONSTANTS\n",
    "SEED = 123\n",
    "TEST_PERC = 0.05\n",
    "INPUT_ELO_DIR = '/kaggle/input/elo-merchant-category-recommendation'\n",
    "INPUT_PREPROCESSED_DIR = '/kaggle/input/cz4041-preprocessed'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import unittest\n",
    "import threading\n",
    "\n",
    "np.random.seed(400)\n",
    "random.seed(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b21ad7",
   "metadata": {
    "papermill": {
     "duration": 0.009232,
     "end_time": "2023-04-28T14:16:15.356892",
     "exception": false,
     "start_time": "2023-04-28T14:16:15.347660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utilts <a id=\"utils\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2428523f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:16:15.378579Z",
     "iopub.status.busy": "2023-04-28T14:16:15.378168Z",
     "iopub.status.idle": "2023-04-28T14:16:15.450905Z",
     "shell.execute_reply": "2023-04-28T14:16:15.449723Z"
    },
    "papermill": {
     "duration": 0.08693,
     "end_time": "2023-04-28T14:16:15.453520",
     "exception": false,
     "start_time": "2023-04-28T14:16:15.366590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarizeDF(df:DataFrame)->DataFrame:\n",
    "    \"\"\"This function shows a basic summary of the given dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas DataFrame\n",
    "    This specifies the dataframe to be summarized.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame: This is a table of summary of the given dataset.\n",
    "    \"\"\"    \n",
    "    variables, dtypes, count, unique, missing, pc_missing = [], [], [], [], [], []\n",
    "    \n",
    "    for item in df.columns:\n",
    "        variables.append(item)\n",
    "        dtypes.append(df[item].dtype)\n",
    "        count.append(len(df[item]))\n",
    "        unique.append(len(df[item].unique()))\n",
    "        missing.append(df[item].isna().sum())\n",
    "        pc_missing.append(round((df[item].isna().sum() / len(df[item])) * 100, 2))\n",
    "\n",
    "    output = pd.DataFrame({\n",
    "        'column_name': variables, \n",
    "        'dtype': dtypes,\n",
    "        'count': count,\n",
    "        'unique': unique,\n",
    "        'missing': missing, \n",
    "        'percentage_missing_data': pc_missing\n",
    "    })    \n",
    "        \n",
    "    return output\n",
    "\n",
    "def preprocess_data(df:DataFrame=None)->DataFrame:\n",
    "    \"\"\"This function preprocess the data into a specific form for the computation.\n",
    "    Given a DataFrame (df), impute with mode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas DataFrame\n",
    "    This specifies the data to be preprocessed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame: This specifies the preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise Exception(\"Expected a DataFrame, no DataFrame supplied.\")\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    for col in df.columns[df.isnull().any()]:\n",
    "        df_copy[col].fillna(df_copy['card_id'].map(df_copy.groupby('card_id')[col].apply(lambda x: x.mode().iloc[0] if not x.isnull().all() else np.nan)).fillna(df_copy[col].mode().iloc[0]), inplace=True)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "def feature_engineering(df:DataFrame=None)->DataFrame:\n",
    "    \"\"\"This function perform feature engineering on the input Data\"\"\"\n",
    "    \n",
    "    def get_new_columns(name:str, aggs:list)->list: # Nested function for feature engineering\n",
    "        \"\"\"This function creates new column names for the aggregation of the features.\"\"\"\n",
    "        return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    \n",
    "    # Make copy of df\n",
    "    df_historical_transactions_copy = df.copy()\n",
    " \n",
    "    # Convert DT columns to Pandas DT\n",
    "    df_historical_transactions_copy['purchase_date'] = pd.to_datetime(df_historical_transactions_copy['purchase_date'])\n",
    " \n",
    "    # Feature Engineer columns from purchase_date\n",
    "    df_historical_transactions_copy['year'] = df_historical_transactions_copy['purchase_date'].dt.year\n",
    "    df_historical_transactions_copy['weekofyear'] = df_historical_transactions_copy['purchase_date'].dt.isocalendar().week\n",
    "    df_historical_transactions_copy['month'] = df_historical_transactions_copy['purchase_date'].dt.month\n",
    "    df_historical_transactions_copy['dayofweek'] = df_historical_transactions_copy['purchase_date'].dt.dayofweek\n",
    "    df_historical_transactions_copy['weekend'] = (df_historical_transactions_copy.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df_historical_transactions_copy['hour'] = df_historical_transactions_copy['purchase_date'].dt.hour\n",
    " \n",
    "    # Encode Binary Features\n",
    "    df_historical_transactions_copy['authorized_flag'] = df_historical_transactions_copy['authorized_flag'].map({\"Y\":1, 'N':0})\n",
    "    df_historical_transactions_copy['category_1'] = df_historical_transactions_copy['category_1'].map({'Y':1, 'N':0})\n",
    " \n",
    "    # Feature Engineer Month Diff/Lag\n",
    "    df_historical_transactions_copy['month_diff'] = ((datetime.datetime.today() - df_historical_transactions_copy['purchase_date']).dt.days)//30\n",
    "    df_historical_transactions_copy['month_diff'] += df_historical_transactions_copy['month_lag']\n",
    "    \n",
    "    # Getting Centrality of the Data\n",
    "    aggs = {}\n",
    "    for col in ['month','hour','weekofyear','dayofweek','year', 'state_id','subsector_id']:\n",
    "        aggs[col] = ['nunique']\n",
    " \n",
    "    # Feature Engineering using Univariate Analysis\n",
    "    aggs['authorized_flag'] = ['sum', 'mean']\n",
    "    aggs['card_id'] = ['size']\n",
    "    aggs['category_1'] = ['sum', 'mean']\n",
    "    aggs['installments'] = ['sum','max','min','mean','var']\n",
    "    aggs['month_lag'] = ['max','min','mean','var']\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_diff'] = ['mean']\n",
    "    aggs['weekend'] = ['sum', 'mean']\n",
    " \n",
    "    for col in ['category_2','category_3']:\n",
    "        df_historical_transactions_copy[col+'_mean'] = df_historical_transactions_copy.groupby([col])['purchase_amount'].transform('mean')\n",
    "        aggs[col+'_mean'] = ['mean']    \n",
    " \n",
    "    new_columns = get_new_columns('hist',aggs)\n",
    "    \n",
    "    # Group Aggregations by card_id\n",
    "    df_historical_transactions_copy_group = df_historical_transactions_copy.groupby('card_id').agg(aggs)\n",
    " \n",
    "    # Remove Multilevel Indexing with New Column Names\n",
    "    df_historical_transactions_copy_group.columns = new_columns\n",
    "    \n",
    "    # Reset Index\n",
    "    df_historical_transactions_copy_group.reset_index(drop=False,inplace=True)\n",
    "    \n",
    "    # Cast variable to pandas Datetime\n",
    "    df_historical_transactions_copy_group['hist_purchase_date_max'] = pd.to_datetime(df_historical_transactions_copy_group['hist_purchase_date_max'])\n",
    "    df_historical_transactions_copy_group['hist_purchase_date_min'] = pd.to_datetime(df_historical_transactions_copy_group['hist_purchase_date_min'])\n",
    "\n",
    "    return df_historical_transactions_copy_group\n",
    "\n",
    "def merge_data(key:str=None, dfs:List[DataFrame]=None)->DataFrame:\n",
    "    \"\"\"This function takes in multiple dataframes and performs a left outer join on a key.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    key: str\n",
    "    This species the joining key.\n",
    "    \n",
    "    dfs: list of pandas DataFrame\n",
    "    This specifies the list of DataFrames to perform left outer join based on a key.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame: This specifies the resultant DataFrame from the merging operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanity Check\n",
    "    if key is None:\n",
    "        raise Exception(\"Expected a key, no key supplied.\")\n",
    "        \n",
    "    if not isinstance(key, str):\n",
    "        raise Exception(f\"Expected type str for key, {type(key)} was supplied.\")\n",
    "    \n",
    "    if dfs is None or not len(dfs) == 2:\n",
    "        raise Exception(\"Expected at least two DataFrame.\")\n",
    "        \n",
    "    if any(type(x) != DataFrame for x in dfs):\n",
    "        raise Exception(\"At least one DataFrame is not the correct DataType.\")\n",
    "        \n",
    "    # Iterate through DataFrames to perform merge operation\n",
    "    df_res = dfs[0]\n",
    "    \n",
    "    for df in dfs[1:]:\n",
    "        df_res = pd.merge(left=df_res, right=df, how='left', left_on=key, right_on=key)\n",
    "    \n",
    "    return df_res\n",
    "\n",
    "def zhenjie_miracle(df:DataFrame)->DataFrame:\n",
    "    \"\"\"This function perform feature engineering on purchase_max and purchase_min and one-hot encoding on the \n",
    "    input Data which must be a merged dataframe of train dataset and trans (hist and new) dataset after running \n",
    "    feature_engineering function\"\"\"\n",
    "    \n",
    "    # Make copy of df\n",
    "    df_historical_transactions_copy_group = df.copy()\n",
    "\n",
    "    brazil_holiday_list=[ \n",
    "            '01-01-17', '14-02-17', '28-08-17', '14-04-17', '16-04-17', '21-04-17',\n",
    "            '01-05-17', '15-06-17', '07-09-17', '12-10-17', '02-11-17', '15-11-17', \n",
    "            '24-12-17', '25-12-17', '31-12-17',\n",
    "            '01-01-18', '14-02-18', '28-08-18', '14-04-18', '16-04-18', '21-04-18',\n",
    "            '01-05-18', '15-06-18', '07-09-18', '12-10-18', '02-11-18', '15-11-18', \n",
    "            '24-12-18', '25-12-18', '31-12-18'\n",
    "      ]\n",
    "    df_historical_transactions_copy_group['purchase_max_is_holiday'] = df_historical_transactions_copy_group['hist_purchase_date_max'].isin(brazil_holiday_list).astype(int)\n",
    "    df_historical_transactions_copy_group['purchase_min_is_holiday'] = df_historical_transactions_copy_group['hist_purchase_date_min'].isin(brazil_holiday_list).astype(int)\n",
    "    \n",
    "    df_historical_transactions_copy_group_dummies = pd.get_dummies(df_historical_transactions_copy_group['feature_1'], prefix='feature_1', drop_first=True)\n",
    "    df_historical_transactions_copy_group = pd.concat([df_historical_transactions_copy_group, df_historical_transactions_copy_group_dummies], axis=1)\n",
    "    df_historical_transactions_copy_group_dummies = pd.get_dummies(df_historical_transactions_copy_group['feature_2'], prefix='feature_2', drop_first=True)\n",
    "    df_historical_transactions_copy_group = pd.concat([df_historical_transactions_copy_group, df_historical_transactions_copy_group_dummies], axis=1)\n",
    "\n",
    "    return df_historical_transactions_copy_group\n",
    "\n",
    "\n",
    "def pengaik_miracle(df:DataFrame=None)->DataFrame:\n",
    "    \n",
    "    \"\"\"This function perform feature engineering on average monthly purchase amount raw of positive month lags \n",
    "    over that of negative. It also performs feature engineering on the ratio of purchase amount raw of \n",
    "    month_lag=i/month_lag=i-1 for each card_id and returns the average ratio as a column\n",
    "    input Data which must be a concat dataframe of trans (hist and new) dataset \"\"\"\n",
    "    \n",
    "    transactions_copy = df.copy()\n",
    "    \n",
    "    # Reverse purchase_amount\n",
    "    transactions_copy['purchase_amount_raw'] = np.round(transactions_copy['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
    "\n",
    "    # Group transactions_copy by card_id and month_lag\n",
    "    grouped_transactions_copy = transactions_copy.groupby(['card_id', 'month_lag']).agg({'purchase_amount_raw': 'mean'}).reset_index()\n",
    "\n",
    "    # Separate transactions_copy into two groups based on month_lag\n",
    "    lag_le_0 = grouped_transactions_copy[grouped_transactions_copy['month_lag'] <= 0]\n",
    "    lag_gt_0 = grouped_transactions_copy[grouped_transactions_copy['month_lag'] > 0]\n",
    "\n",
    "    # Calculate the monthly average purchase amount for each group\n",
    "    lag_le_0_monthly_average_raw = lag_le_0.groupby('card_id')['purchase_amount_raw'].mean().reset_index().rename(columns={'purchase_amount_raw': 'monthly_average_purchase_amount_raw_for_month_lag_le_0'})\n",
    "    lag_gt_0_monthly_average_raw = lag_gt_0.groupby('card_id')['purchase_amount_raw'].mean().reset_index().rename(columns={'purchase_amount_raw': 'monthly_average_purchase_amount_raw_for_month_lag_gt_0'})\n",
    "\n",
    "    # Merge the new columns with the original transactions_copy dataframe\n",
    "    transactions_copy = transactions_copy.merge(lag_le_0_monthly_average_raw, on='card_id', how='left')\n",
    "    transactions_copy = transactions_copy.merge(lag_gt_0_monthly_average_raw, on='card_id', how='left')\n",
    "\n",
    "    transactions_copy['ratio_between_ave_monthly_purchase_raw_for_positive_and_negative'] = transactions_copy['monthly_average_purchase_amount_raw_for_month_lag_gt_0'] / transactions_copy['monthly_average_purchase_amount_raw_for_month_lag_le_0']\n",
    "\n",
    "    # Find the minimum month_lag for each card_id and set the index to 'card_id'\n",
    "    min_month_lag_per_card = transactions_copy.groupby('card_id', as_index=False)['month_lag'].min().set_index('card_id')\n",
    "\n",
    "    # Fill in missing month_lag values for each card_id\n",
    "    unique_card_ids = transactions_copy['card_id'].unique()\n",
    "    min_month_lag = transactions_copy['month_lag'].min()\n",
    "    max_month_lag = transactions_copy['month_lag'].max()\n",
    "\n",
    "    complete_data = []\n",
    "\n",
    "    for card_id in unique_card_ids:\n",
    "        # Use .loc[] accessor to look up the minimum month_lag for each card_id\n",
    "        card_min_month_lag = min_month_lag_per_card.loc[card_id]['month_lag']\n",
    "        for month_lag in range(card_min_month_lag, max_month_lag + 1):\n",
    "            complete_data.append([card_id, month_lag, 0])\n",
    "\n",
    "    complete_transactions_copy = pd.DataFrame(complete_data, columns=['card_id', 'month_lag', 'purchase_amount_raw'])\n",
    "\n",
    "    # Compute the purchase_amount_raw sum for each card_id and month_lag combination\n",
    "    grouped_transactions_copy = transactions_copy.groupby(['card_id', 'month_lag'], as_index=False)['purchase_amount_raw'].sum()\n",
    "\n",
    "    # Merge the complete_transactions_copy dataframe with the grouped_transactions_copy dataframe\n",
    "    merged_transactions_copy = pd.merge(complete_transactions_copy, grouped_transactions_copy, on=['card_id', 'month_lag'], how='left', suffixes=('', '_y'))\n",
    "    merged_transactions_copy['purchase_amount_raw'] = merged_transactions_copy['purchase_amount_raw_y'].fillna(merged_transactions_copy['purchase_amount_raw'])\n",
    "\n",
    "    # Calculate the ratio of purchase_amount_raw for each month_lag=i/month_lag=i-1\n",
    "    merged_transactions_copy['prev_month_purchase_amount'] = merged_transactions_copy.groupby('card_id')['purchase_amount_raw'].shift(1)\n",
    "    merged_transactions_copy['ratio'] = np.where(merged_transactions_copy['prev_month_purchase_amount'] != 0, merged_transactions_copy['purchase_amount_raw'] / merged_transactions_copy['prev_month_purchase_amount'], np.nan)\n",
    "\n",
    "    # Compute the average of these ratios for each card_id\n",
    "    average_ratios = merged_transactions_copy.groupby('card_id', as_index=False)['ratio'].mean()\n",
    "\n",
    "    # Handling division by zero cases by replacing np.inf with np.nan and then replacing np.nan with a suitable value (e.g., 1)\n",
    "    average_ratios['ratio'] = average_ratios['ratio'].replace([np.inf, -np.inf], np.nan).fillna(1)\n",
    "\n",
    "    # Merge average_ratios with transactions_copy DataFrame\n",
    "    feature_engineered_transactions_copy = transactions_copy.merge(average_ratios, on='card_id', how='left')\n",
    "\n",
    "    return feature_engineered_transactions_copy\n",
    "\n",
    "\n",
    "\n",
    "ModelRegressor = Union[LinearRegression, DecisionTreeRegressor, RandomForestRegressor]\n",
    "def feature_selection(approach:str=\"RFE\", \n",
    "                      k:int=10, \n",
    "                      train:DataFrame=None, \n",
    "                      test:DataFrame=None,\n",
    "                      model:ModelRegressor=None)->List[str]:\n",
    "    \"\"\"This function performs feature selection based on the user's choice of approach.\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "    >> features = feature_selection(approach=\"LGBM\", train=X_train, test=y_train)\n",
    "    \"\"\"\n",
    "    if approach == 'LGBM':\n",
    "        # LGTM Regressor to pick out important features\n",
    "        gbm = lgb.LGBMRegressor()\n",
    "        gbm.fit(train, test)\n",
    "\n",
    "        # Feature Important Viz\n",
    "        fea_imp_ = pd.DataFrame({'variable':train.columns, 'feature_importance':gbm.feature_importances_})\n",
    "        fea_imp_sorted = fea_imp_.sort_values(by='feature_importance', ascending=False)\n",
    "        return list(fea_imp_sorted[:k]['variable'])\n",
    "    if approach == 'RFE':\n",
    "        rfe = RFE(estimator=model, n_features_to_select=k)\n",
    "        rfe = rfe.fit(train, test)\n",
    "\n",
    "        # summarize the ranking of the attributes\n",
    "        fea_rank_ = pd.DataFrame({'variable': train.columns, 'feature_importance':rfe.ranking_})\n",
    "        fea_rank_sorted = fea_rank_.sort_values(by='feature_importance', ascending=False)\n",
    "        return list(fea_rank_sorted[:k]['variable'])\n",
    "\n",
    "    \n",
    "def build_train_test_sets(df:DataFrame=None, \n",
    "                          features:List[str]=None, \n",
    "                          target:str=None, \n",
    "                          verbose:int=0, \n",
    "                          **kwargs:dict)->List[DataFrame]:\n",
    "    \"\"\"This function splits the given dataframe into train and test data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        This specifies the source DataFrame.\n",
    "\n",
    "        features: list of str\n",
    "        This list containing a str-type elements specifies the name of the features.\n",
    "\n",
    "        target: str\n",
    "        This specifies the target variable.\n",
    "        \n",
    "        verbose: int-type\n",
    "            This species the verbosity of the function.\n",
    "    \n",
    "    Kwargs\n",
    "        A dict mapping the corresponding parameters for scikit learn model selection. \n",
    "        \n",
    "        {\"test_size\": 0.05, \"seed\": None}\n",
    "    \n",
    "        If a key from the keys argument is missing from the settings, the default will be used.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list: This species the list containing X_train, X_test, y_test and y_train DataFrame.\n",
    "    \"\"\"\n",
    "    # Default \n",
    "    model_params = {\n",
    "        'test_size': 0.05,\n",
    "        \"seed\": None\n",
    "    }\n",
    "    \n",
    "    # Sanity Check\n",
    "    if df is None:\n",
    "        raise ValueError(\"Expected a DataFrame, no DataFrame supplied.\")\n",
    "        \n",
    "    if features is None:\n",
    "        raise Exception(\"Expected a features list, no features list supplied.\")\n",
    "    \n",
    "    if not isinstance(features, list):\n",
    "        raise Exception(f\"Expected list datatype for features, {type(features)} was supplied.\")\n",
    "        \n",
    "    if not isinstance(target, str):\n",
    "        raise Exception(f\"Expected str datatype for target, {type(target)} was supplied.\")\n",
    "        \n",
    "    # Check for Kwargs\n",
    "    if \"test_size\" in kwargs:\n",
    "        model_params['test_size'] = kwargs[\"test_size\"]\n",
    "    if \"seed\" in kwargs:\n",
    "        model_params['seed'] = kwargs[\"seed\"]\n",
    "        \n",
    "    seed = \"No Seed\" if model_params[\"seed\"] is None else model_params['seed']\n",
    "    if verbose != 0:\n",
    "        print(f\"***Parameters for Model Selection***\\ntest_size {model_params['test_size']}\\nseed: {seed}\\n\")\n",
    "    \n",
    "    if seed != \"No Seed\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[features], df[[target]], test_size=model_params['test_size'])\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[features], df[[target]], test_size=model_params['test_size'], random_state=model_params['seed'])\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def train_eval_model(model:ModelRegressor,  X_train, X_test, y_train, y_test, name)->DataFrame:\n",
    "    \"\"\"This function trains and evaluates the model.\n",
    "    By default, the score used it RMSE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Args:\n",
    "        model: model\n",
    "        This species the model to use for training and evaluation.\n",
    "       \n",
    "    Kwargs\n",
    "        A dict mapping the corresponding parameters for training and test data. \n",
    "        \n",
    "        {\"X_train\": ..., \"X_test\": ..., \"y_train\": ..., \"y_test\": ...}\n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    dataframe: This specifies resets from testing the model.\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Pred\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Eval\n",
    "    print(f\"{name} Score:\", math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def Bayesian_Optimization(objective_function, parameters_dict, n_init_random_explorations=10, n_iter = 50):\n",
    "    \"\"\"Find the hyperparameters that maximizes a given objective (e.g test result)\n",
    "    Parameters\n",
    "    ----------\n",
    "    Args:\n",
    "        objective_function: function\n",
    "        function that outputs a value, which BO will try to maximize. \n",
    "\n",
    "        parameters_dict: Dictionary\n",
    "        Contain hyperparameters that you want to optimize. Key is hyperparameter name, value is (min, max) value of that hyperparameters\n",
    "        \n",
    "        n_init_random_explorations: int\n",
    "        Number of random sets of hyperparameters to try. <n_init_random_explorations> random hp sets are explored before <n_iter> systematic explorations are run\n",
    "        \n",
    "        n_iter: int\n",
    "        Number of iterations to run\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict: Best set of hyperparameters\n",
    "\n",
    "\n",
    "    Usage\n",
    "    -------\n",
    "    The key in pbounds must match the parameters in objective_function!!!\n",
    "\n",
    "    def objective_function(n, beta, gamma) -> float:\n",
    "      n = int(n)\n",
    "      model = BetaVae(\n",
    "          n_latent=n,\n",
    "          beta=beta,\n",
    "          n_chan=N_CHAN,\n",
    "          input_d=INPUT_DIM,\n",
    "          batch=BATCH,\n",
    "          gamma = gamma,\n",
    "          )\n",
    "      model.train_self(\n",
    "          data_path=TRAIN_PATH,\n",
    "          epochs=1,\n",
    "          weights_file=f'bvae_n{n}_b{beta}_{\"bw\" if N_CHAN == 1 else \"\"}_'\n",
    "                      f'{INPUT_DIM[0]}x{INPUT_DIM[1]}.pt')\n",
    "      return model.test(TEST_PATH, iters=1)\n",
    "\n",
    "    parameters_dict = {'n': (5, 200), 'beta': (0.1,30), 'gamma': (0.001, 30)}\n",
    "\n",
    "    def Bayesian_Optimization(objective_function, parameters_dict, n_iter = 50):\n",
    "      optimizer = BayesianOptimization(\n",
    "          f=objective_function,\n",
    "          pbounds= parameters_dict,\n",
    "          verbose=2,\n",
    "          random_state=1)\n",
    "    \n",
    "    best_hypers = Bayesian_Optimization(objective_function, parameters_dict, n_iter = 50)\n",
    "    \"\"\"\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective_function, #define before this function\n",
    "        pbounds=parameters_dict, \n",
    "        random_state=1)\n",
    "    optimizer.maximize(init_points=n_init_random_explorations, n_iter=n_iter)\n",
    "    print('#################################################################')\n",
    "    print(f'Found Network with Optimal target result of {optimizer.max[\"target\"]}')\n",
    "    print(f'Parameters: {optimizer.max[\"params\"]}')\n",
    "    print('#################################################################')\n",
    "    return optimizer.max[\"params\"]\n",
    "\n",
    "\n",
    "def createData(df:DataFrame=None, df_t:DataFrame=None)->DataFrame:\n",
    "    \"\"\"This function transform the given datasets into a suitable dataset for training/testing.\n",
    "    \n",
    "    algorithm\n",
    "        0. (optional) Subset the data based on card_ids in train/test (improve performance)\n",
    "        1. PA Miracle\n",
    "        2. Impute with Mode\n",
    "        3. Feature Engineering\n",
    "        4. Merge df_transactions from 1 - 3 to train/test (IMPORTANT)\n",
    "        5. ZJ Miracle\n",
    "        6. Remove unnecessary columns\n",
    "    endalgorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "    This specifies the dataframe containing transactions details. Ideally, this should be a combination of \n",
    "    new_historical and historical transactions datagframes.\n",
    "    \n",
    "    df_t: DataFrame\n",
    "    This specifies the train or test dataframe.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame: Valid DataFrame after the preprocessing and imputations\n",
    "    \"\"\"\n",
    "    # Run Pengaik's Miracle\n",
    "    df_transactions_t = pengaik_miracle(df)\n",
    "    print(\"Card_ID Uniqueness (PA Miracle)\", len(df_transactions_t['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Impute with Mode\n",
    "    df_impute_mode = preprocess_data(df=df_transactions_t)\n",
    "    print(\"Card_ID Uniqueness (Impute Mode)\", len(df_impute_mode['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Store PA's Ratios\n",
    "    df_ratios = df_impute_mode[['card_id',\n",
    "       'monthly_average_purchase_amount_raw_for_month_lag_le_0',\n",
    "       'monthly_average_purchase_amount_raw_for_month_lag_gt_0',\n",
    "       'ratio_between_ave_monthly_purchase_raw_for_positive_and_negative',\n",
    "       'ratio']].drop_duplicates()\n",
    "\n",
    "    # Sanity Check for n_rows and uniqueness of card_id\n",
    "    print(\"Card_ID Uniqueness\", len(df_ratios['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "    \n",
    "    # Sanity Check for n_rows and uniqueness of card_id\n",
    "    print(\"Card_ID Uniqueness (PA Ratio)\", len(df_ratios['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "    \n",
    "    # Feature Engineering\n",
    "    df_aggregated_cols = feature_engineering(df_impute_mode)\n",
    "    print(\"Card_ID Uniqueness (Feature Engineering)\", len(df_aggregated_cols['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Merge PA's Ratio\n",
    "    df_aggregated_cols = merge_data('card_id', [df_ratios, df_aggregated_cols])\n",
    "\n",
    "    # Sanity Check for n_rows and uniqueness of card_id\n",
    "    print(\"Card_ID Uniqueness (Merge PA Ratio)\", len(df_aggregated_cols['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "\n",
    "    # Merge Transactions and T\n",
    "    df_t_merge = merge_data('card_id', [df_t, df_aggregated_cols])\n",
    "    print(\"Card_ID Uniqueness (Merging)\", len(df_t_merge['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Execute Zhen Jie's Miracle\n",
    "    df_t_merge = zhenjie_miracle(df_t_merge)\n",
    "    print(\"Card_ID Uniqueness (ZJ Miracle)\", len(df_t_merge['card_id'].unique()) == len(df_t['card_id'].unique()))\n",
    "    \n",
    "    # Impute Missing Data\n",
    "    df_t_merge.fillna(\"2017-01\", inplace=True)\n",
    "\n",
    "    # Engineered by Zhen Jie so Remove\n",
    "    df_t_merge.drop(columns=['hist_purchase_date_max', 'hist_purchase_date_min'], inplace=True)\n",
    "    print(\"Card_ID Uniqueness (Imputations and Drop)\", len(df_test_merge['card_id'].unique()) == len(df_test['card_id'].unique()))\n",
    "    \n",
    "    # Final Sanity Check\n",
    "    print(\"***FINAL***\")\n",
    "    print('n_rows:', format(df_t_merge.shape[0], \"_\"), end='\\n\\n')\n",
    "    print(\"Columns:\\n\", \", \".join(df_t_merge.columns), sep='')\n",
    "    \n",
    "    return df_t_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f2b6b",
   "metadata": {
    "papermill": {
     "duration": 0.009742,
     "end_time": "2023-04-28T14:16:15.473005",
     "exception": false,
     "start_time": "2023-04-28T14:16:15.463263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Datasets <a id=\"datasets\"></a>\n",
    "\n",
    "1. Customer has a ```card_id``` as uuid.\n",
    "2. Each customer can make at least one transaction to merchants.\n",
    "3. Merchant has ```merchant_id``` as uuid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86c991b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:16:15.494156Z",
     "iopub.status.busy": "2023-04-28T14:16:15.493731Z",
     "iopub.status.idle": "2023-04-28T14:18:32.534172Z",
     "shell.execute_reply": "2023-04-28T14:18:32.530164Z"
    },
    "papermill": {
     "duration": 137.059466,
     "end_time": "2023-04-28T14:18:32.542120",
     "exception": false,
     "start_time": "2023-04-28T14:16:15.482654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Historical Transactions\n",
    "tp = pd.read_csv(f'{INPUT_ELO_DIR}/historical_transactions.csv', iterator=True, chunksize=2_000_000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\n",
    "df_historical_transactions = pd.concat(tp, ignore_index=True) \n",
    "\n",
    "# New Historical Transactions\n",
    "tp = pd.read_csv(f'{INPUT_ELO_DIR}/new_merchant_transactions.csv', iterator=True, chunksize=2_000_000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\n",
    "df_new_historical_transactions = pd.concat(tp, ignore_index=True) \n",
    "\n",
    "# Train Data\n",
    "df_train = pd.read_csv(f'{INPUT_ELO_DIR}/train.csv')\n",
    "\n",
    "# Engineered Train Data \n",
    "tp = pd.read_csv(f'{INPUT_PREPROCESSED_DIR}/output.csv',index_col=0, iterator=True, chunksize=5_000_000)  # gives TextFileReader, which is iterable with chunks of 1000 rows.\n",
    "df_train_merge = pd.concat(tp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c504405",
   "metadata": {
    "papermill": {
     "duration": 0.040687,
     "end_time": "2023-04-28T14:18:32.623859",
     "exception": false,
     "start_time": "2023-04-28T14:18:32.583172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Machine Learning - Build Custom-Model <a id=\"custom-model\"></a>\n",
    "From the previous notebook, experiments of binary classifier yield the following results:\n",
    "- Best sampling strategy: SMOTE,R-Under(1:4),R-Over\n",
    "- Best binary classifier: Bag-LGBM\n",
    "\n",
    "As a recap of the previous notebook, the model architecture is depicted as follows:\n",
    "\n",
    "<img src=\"./model_architecture.png\"/>\n",
    "\n",
    "There are 6 steps to achieve this:\n",
    "1. Try sampling methods for binary classifier\n",
    "2. Explore binary classification models\n",
    "3. Explore regression models\n",
    "4. Hyperparameter tuning for the selected binary classification model\n",
    "5. Hyperparameter tuning for the regression models\n",
    "6. Explore and hyperparameter-tune meta-model\n",
    "\n",
    "Some setup before running Step 4 and beyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718379d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:18:32.646242Z",
     "iopub.status.busy": "2023-04-28T14:18:32.645811Z",
     "iopub.status.idle": "2023-04-28T14:18:32.922617Z",
     "shell.execute_reply": "2023-04-28T14:18:32.921522Z"
    },
    "papermill": {
     "duration": 0.291004,
     "end_time": "2023-04-28T14:18:32.925212",
     "exception": false,
     "start_time": "2023-04-28T14:18:32.634208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries used in this case\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Imbalanced Learning (Sampling)\n",
    "import imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter\n",
    "\n",
    "df_train_merge['outlier'] = df_train_merge['target'] < -20\n",
    "features = list(df_train_merge.drop(columns=['target', 'card_id', 'outlier']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6aeb208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:18:32.946783Z",
     "iopub.status.busy": "2023-04-28T14:18:32.946333Z",
     "iopub.status.idle": "2023-04-28T14:18:34.207325Z",
     "shell.execute_reply": "2023-04-28T14:18:34.205817Z"
    },
    "papermill": {
     "duration": 1.276925,
     "end_time": "2023-04-28T14:18:34.211865",
     "exception": false,
     "start_time": "2023-04-28T14:18:32.934940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* Top 40 features for bin clf *********\n",
      "['hist_purchase_amount_var', 'monthly_average_purchase_amount_raw_for_month_lag_le_0', 'ratio', 'hist_installments_var', 'hist_purchase_amount_sum', 'hist_purchase_amount_max', 'hist_weekend_mean', 'hist_month_lag_mean', 'hist_month_lag_var', 'hist_purchase_amount_min', 'hist_purchase_amount_mean', 'hist_month_diff_mean', 'hist_weekend_sum', 'hist_category_2_mean_mean', 'hist_month_lag_max', 'hist_category_3_mean_mean', 'purchase_max_is_holiday', 'purchase_min_is_holiday', 'feature_1_2', 'feature_1_3', 'feature_1_4', 'feature_1_5', 'feature_2_2', 'hist_month_lag_min', 'feature_1', 'feature_2', 'hist_state_id_nunique', 'feature_3', 'monthly_average_purchase_amount_raw_for_month_lag_gt_0', 'ratio_between_ave_monthly_purchase_raw_for_positive_and_negative', 'hist_month_nunique', 'hist_hour_nunique', 'hist_weekofyear_nunique', 'hist_dayofweek_nunique', 'hist_year_nunique', 'hist_subsector_id_nunique', 'hist_installments_mean', 'hist_authorized_flag_sum', 'hist_authorized_flag_mean', 'hist_card_id_size']\n"
     ]
    }
   ],
   "source": [
    "features_impt_clf = feature_selection(k=40, train=df_train_merge[features], test=df_train_merge['target'], model=Ridge(random_state=SEED))\n",
    "print(\"********* Top 40 features for bin clf *********\")\n",
    "print(features_impt_clf)\n",
    "\n",
    "def custom_classifier_train(df, bin_classifier, samplers, rare_threshold, show_matrix=False, cv=True, verbose=True):\n",
    "    # Prepare data\n",
    "    features = list(df.drop(columns=['target', 'card_id', 'outlier']).columns)\n",
    "    target = 'outlier'\n",
    "    scores = []\n",
    "    \n",
    "    def train_and_score(X_train, y_train, X_test, y_test):\n",
    "        X_res, y_res = X_train, y_train\n",
    "        for sampler in samplers:\n",
    "            X_res, y_res = sampler.fit_resample(X_res, y_res)\n",
    "        bin_classifier.fit(X_res, y_res)\n",
    "\n",
    "        # Visualise the performance of the Classifier x Sampler\n",
    "        y_pred = bin_classifier.predict_proba(X_test)[:, 1] > rare_threshold\n",
    "\n",
    "        if show_matrix:\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            ConfusionMatrixDisplay(cm).plot()\n",
    "        return f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Model that predicts a data point is rare    \n",
    "    X, y = df[features_impt_clf], df[[target]]\n",
    "    if not cv:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)\n",
    "        score = train_and_score(X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        for i, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            if verbose:\n",
    "                print(f\"Fold {i}\")\n",
    "            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            X_test, y_test = X.iloc[val_idx], y.iloc[val_idx]\n",
    "            score = train_and_score(X_train, y_train, X_test, y_test)\n",
    "            scores.append(score)\n",
    "        score = np.mean(scores)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74c3044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:18:34.266052Z",
     "iopub.status.busy": "2023-04-28T14:18:34.265163Z",
     "iopub.status.idle": "2023-04-28T14:18:34.281803Z",
     "shell.execute_reply": "2023-04-28T14:18:34.280156Z"
    },
    "papermill": {
     "duration": 0.048366,
     "end_time": "2023-04-28T14:18:34.285914",
     "exception": false,
     "start_time": "2023-04-28T14:18:34.237548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sor = RandomOverSampler(random_state=SEED)\n",
    "sur_random_half = RandomUnderSampler(random_state=SEED)   # Set rare:non-rare as 1:1\n",
    "sur_random_fifth = RandomUnderSampler(sampling_strategy={0: df_train_merge.outlier.sum()*7}, random_state=SEED)   # Set rare:non-rare as 1:4\n",
    "su_tl = TomekLinks()\n",
    "so_smote = SMOTE()\n",
    "su_nm = NearMiss()\n",
    "\n",
    "samplers = [[sor], [sur_random_half], [sur_random_fifth], [su_tl], [so_smote], [su_nm], [so_smote, sur_random_fifth, sor], [so_smote, sur_random_half, sor],\n",
    "           [so_smote, su_tl]]\n",
    "sampler_names = ['R-Over', 'R-Under(1:1)', 'R-Under(1:4)', 'Tomek', 'SMOTE', 'NearMiss', 'SMOTE,R-Under(1:4),R-Over', \n",
    "                 'SMOTE,R-Under(1:1),R-Over', 'SMOTE,Tomek']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507626d",
   "metadata": {
    "papermill": {
     "duration": 0.009481,
     "end_time": "2023-04-28T14:18:34.371372",
     "exception": false,
     "start_time": "2023-04-28T14:18:34.361891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. HP Tuning for Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9819d3c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:18:34.392733Z",
     "iopub.status.busy": "2023-04-28T14:18:34.392283Z",
     "iopub.status.idle": "2023-04-28T14:18:34.401486Z",
     "shell.execute_reply": "2023-04-28T14:18:34.400341Z"
    },
    "papermill": {
     "duration": 0.022473,
     "end_time": "2023-04-28T14:18:34.403686",
     "exception": false,
     "start_time": "2023-04-28T14:18:34.381213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_classifier_objfunc(rare_threshold, n_estimators, max_depth, num_leaves):\n",
    "    # RF, SMOTE oversampler performs the best over any combination of BinClassifier x Sampler x Weights (upweighted or not)\n",
    "    clf_baglgb = BaggingClassifier(LGBMClassifier(max_depth=int(max_depth), num_leaves=int(num_leaves), random_state=SEED),\n",
    "                                   n_estimators=int(n_estimators),\n",
    "                                   random_state=SEED)\n",
    "    \n",
    "    sor = RandomOverSampler(random_state=SEED)\n",
    "    sur_random_fifth = RandomUnderSampler(sampling_strategy={0: df_train_merge.outlier.sum()*7}, random_state=SEED)   # Set rare:non-rare as 1:4\n",
    "    so_smote = SMOTE()\n",
    "    samplers = [so_smote, sur_random_fifth, sor]\n",
    "    \n",
    "    # Arguments\n",
    "    df = df_train_merge\n",
    "    bin_classifier = clf_baglgb\n",
    "    \n",
    "    score = custom_classifier_train(df, bin_classifier, samplers, rare_threshold, verbose=False)\n",
    "    return score\n",
    "\n",
    "\n",
    "pbounds = { \n",
    "    'rare_threshold': (0.05, 0.85),\n",
    "    'n_estimators': (10, 100),\n",
    "    'max_depth': (2, 10),\n",
    "    'num_leaves': (2, 50)\n",
    "}\n",
    "\n",
    "# best_params_clf = Bayesian_Optimization(custom_classifier_objfunc, pbounds, n_init_random_explorations=5, n_iter=10)  # COMPLETED\n",
    "best_params_clf = {'max_depth': 5.1741397938453595, 'n_estimators': 58.493506060302124, 'num_leaves': 22.121336691358152, 'rare_threshold': 0.5981756003174076}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aad6c18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:18:34.426767Z",
     "iopub.status.busy": "2023-04-28T14:18:34.425988Z",
     "iopub.status.idle": "2023-04-28T14:24:40.230744Z",
     "shell.execute_reply": "2023-04-28T14:24:40.228833Z"
    },
    "papermill": {
     "duration": 365.836857,
     "end_time": "2023-04-28T14:24:40.250520",
     "exception": false,
     "start_time": "2023-04-28T14:18:34.413663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LGBMClassifier(max_depth=5, num_leaves=22,\n",
       "                                                random_state=123),\n",
       "                  n_estimators=58, random_state=123)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Classifier Model\n",
    "bin_classifier = BaggingClassifier(LGBMClassifier(max_depth=int(best_params_clf['max_depth']), num_leaves=int(best_params_clf['num_leaves']), random_state=SEED),\n",
    "                                   n_estimators=int(best_params_clf['n_estimators']), random_state=SEED)\n",
    "\n",
    "# Results from previous version: best_samplers\n",
    "best_samplers = [so_smote, sur_random_fifth, sor]\n",
    "\n",
    "X_res, y_res = df_train_merge[features_impt_clf], df_train_merge['outlier']\n",
    "for sampler in best_samplers:\n",
    "    X_res, y_res = sampler.fit_resample(X_res, y_res)\n",
    "bin_classifier.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29831f",
   "metadata": {
    "papermill": {
     "duration": 0.009506,
     "end_time": "2023-04-28T14:24:40.269710",
     "exception": false,
     "start_time": "2023-04-28T14:24:40.260204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. HP Tuning of the First-level Regression Models\n",
    "The first cell is similar to the first two cells in Step 3 to have the necessary setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca184f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:24:40.291944Z",
     "iopub.status.busy": "2023-04-28T14:24:40.291454Z",
     "iopub.status.idle": "2023-04-28T14:24:44.163631Z",
     "shell.execute_reply": "2023-04-28T14:24:44.162323Z"
    },
    "papermill": {
     "duration": 3.886716,
     "end_time": "2023-04-28T14:24:44.166340",
     "exception": false,
     "start_time": "2023-04-28T14:24:40.279624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** Top 35 features for Regressors **************************\n",
      "['hist_month_diff_mean', 'hist_month_lag_mean', 'hist_month_lag_var', 'ratio_between_ave_monthly_purchase_raw_for_positive_and_negative', 'hist_authorized_flag_mean', 'monthly_average_purchase_amount_raw_for_month_lag_gt_0', 'hist_category_1_sum', 'hist_category_1_mean', 'hist_weekend_mean', 'ratio', 'monthly_average_purchase_amount_raw_for_month_lag_le_0', 'hist_purchase_amount_max', 'hist_purchase_amount_min', 'hist_weekofyear_nunique', 'hist_installments_sum', 'hist_purchase_amount_var', 'hist_month_lag_max', 'hist_category_2_mean_mean', 'hist_purchase_amount_sum', 'hist_subsector_id_nunique', 'hist_category_3_mean_mean', 'hist_installments_mean', 'hist_installments_var', 'hist_card_id_size', 'hist_month_nunique', 'hist_purchase_amount_mean', 'hist_authorized_flag_sum', 'hist_hour_nunique', 'hist_weekend_sum', 'hist_month_lag_min', 'feature_2', 'feature_1', 'hist_state_id_nunique', 'feature_1_3', 'hist_installments_min']\n"
     ]
    }
   ],
   "source": [
    "# Create three datasets: to train regression on full dataset; concentrated outlier dataset; less concentrated\n",
    "global df_rare\n",
    "global df_non_rare\n",
    "df_rare, df_non_rare = df_train_merge[df_train_merge['outlier'] == 1], df_train_merge[df_train_merge['outlier'] == 0]\n",
    "n = int(0.2 * len(df_rare))\n",
    "\n",
    "df_outlier_more = pd.concat([df_rare, df_non_rare.sample(n)])\n",
    "df_outlier_less = pd.concat([df_non_rare, df_rare.sample(n)])\n",
    "\n",
    "global features_impt_full\n",
    "features_impt_full = feature_selection(approach='LGBM', k=35, train=df_train_merge[features], test=df_train_merge['target'])\n",
    "print(\"*********************** Top 35 features for Regressors **************************\")\n",
    "print(features_impt_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b8c751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:24:44.188183Z",
     "iopub.status.busy": "2023-04-28T14:24:44.187774Z",
     "iopub.status.idle": "2023-04-28T14:24:44.195799Z",
     "shell.execute_reply": "2023-04-28T14:24:44.194713Z"
    },
    "papermill": {
     "duration": 0.021699,
     "end_time": "2023-04-28T14:24:44.198102",
     "exception": false,
     "start_time": "2023-04-28T14:24:44.176403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_reg_train(df, reg, features, cv=True, verbose=True):\n",
    "    # Prepare data\n",
    "#     features = list(df.drop(columns=['target', 'card_id', 'outlier']).columns)\n",
    "    target = 'target'\n",
    "    costs = []\n",
    "    \n",
    "    # Model that predicts a data point is rare    \n",
    "    X, y = df[features], df[[target]]\n",
    "    if not cv:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)\n",
    "        reg.fit(X_train, y_train)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        cost = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    else:\n",
    "        costs = np.sqrt(-cross_val_score(reg, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "        cost = costs.mean()\n",
    "        \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791615de",
   "metadata": {
    "papermill": {
     "duration": 0.009958,
     "end_time": "2023-04-28T14:24:44.365762",
     "exception": false,
     "start_time": "2023-04-28T14:24:44.355804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As all 3 regressors should be LGBM, the same `custom_reg_objfunc()` is used\n",
    "\n",
    "As the total tuning time of each regressor exceeds 12 hours, the hyperparams were obtained by having 3 notebooks that each tune a regressor. The line with `# COMPLETE` indicates the original line that is executed when that specific notebook is tuning that regressor, whereas the next line below that is the exact hyperparams that yield the best performance. For instance, the code to tune the regressor on concentrated rare points ends with the following two lines:\n",
    "```py\n",
    "# best_params_reg_rare = Bayesian_Optimization(custom_reg_objfunc, pbounds) # COMPLETED\n",
    "best_params_reg_rare = {...REDACTED...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c974eb2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:24:44.388248Z",
     "iopub.status.busy": "2023-04-28T14:24:44.387439Z",
     "iopub.status.idle": "2023-04-28T14:24:44.399014Z",
     "shell.execute_reply": "2023-04-28T14:24:44.397877Z"
    },
    "papermill": {
     "duration": 0.025348,
     "end_time": "2023-04-28T14:24:44.401222",
     "exception": false,
     "start_time": "2023-04-28T14:24:44.375874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_reg_objfunc(num_leaves, learning_rate, n_estimators, max_depth, min_split_gain, min_child_weight):\n",
    "    \n",
    "    def train_and_eval(df_rare, df_non_rare):\n",
    "        cost = np.NaN\n",
    "        # Prepare data\n",
    "        features = features_impt_full\n",
    "        target = 'target'\n",
    "        \n",
    "        for i in range(5):\n",
    "            df = pd.concat([df_rare, df_non_rare.sample(n)])\n",
    "            df_outlier_less = pd.concat([df_non_rare, df_rare.sample(n)])\n",
    "            \n",
    "            X, y = df[features], df[[target]]\n",
    "            costs = np.sqrt(-cross_val_score(reg, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "            cost = costs.mean()\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(\n",
    "        num_leaves=int(num_leaves),\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=int(n_estimators),\n",
    "        max_depth=int(max_depth),\n",
    "        min_split_gain=min_split_gain,\n",
    "        min_child_weight=min_child_weight,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Arguments\n",
    "    df = df_train_merge\n",
    "    \n",
    "    cost = train_and_eval(df_rare, df_non_rare)\n",
    "    return -cost\n",
    "\n",
    "\n",
    "pbounds = {\n",
    "    'num_leaves': (5, 50),\n",
    "    'learning_rate': (0.01, 0.5),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3, 10),\n",
    "    'min_split_gain': (0.001, 0.1),\n",
    "    'min_child_weight': (5, 50)\n",
    "}\n",
    "\n",
    "# best_params_reg_rare = Bayesian_Optimization(custom_reg_objfunc, pbounds) # COMPLETED\n",
    "best_params_reg_rare = {'learning_rate': 0.015170978234846356, 'max_depth': 8.182998613590625, 'min_child_weight': 36.101012674105675, 'min_split_gain': 0.0666169341089749, 'n_estimators': 954.3342602764994, 'num_leaves': 11.971532436721898}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8890660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:24:44.423587Z",
     "iopub.status.busy": "2023-04-28T14:24:44.422405Z",
     "iopub.status.idle": "2023-04-28T14:24:44.433899Z",
     "shell.execute_reply": "2023-04-28T14:24:44.433034Z"
    },
    "papermill": {
     "duration": 0.024953,
     "end_time": "2023-04-28T14:24:44.436210",
     "exception": false,
     "start_time": "2023-04-28T14:24:44.411257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_reg_objfunc(num_leaves, learning_rate, n_estimators, max_depth, min_split_gain, min_child_weight):\n",
    "    \n",
    "    def train_and_eval(df_rare, df_non_rare):\n",
    "        cost = np.NaN\n",
    "        # Prepare data\n",
    "        features = features_impt_full\n",
    "        target = 'target'\n",
    "        \n",
    "        for i in range(5):\n",
    "            df = pd.concat([df_non_rare, df_rare.sample(n)])\n",
    "            \n",
    "            X, y = df[features], df[[target]]\n",
    "        \n",
    "            costs = np.sqrt(-cross_val_score(reg, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "            cost = costs.mean()\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(\n",
    "        num_leaves=int(num_leaves),\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=int(n_estimators),\n",
    "        max_depth=int(max_depth),\n",
    "        min_split_gain=min_split_gain,\n",
    "        min_child_weight=min_child_weight,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Arguments\n",
    "    df = df_train_merge\n",
    "    \n",
    "    cost = train_and_eval(df_rare, df_non_rare)\n",
    "    return -cost\n",
    "\n",
    "\n",
    "pbounds = {\n",
    "    'num_leaves': (5, 50),\n",
    "    'learning_rate': (0.01, 0.5),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3, 10),\n",
    "    'min_split_gain': (0.001, 0.1),\n",
    "    'min_child_weight': (5, 50)\n",
    "}\n",
    "\n",
    "# best_params_reg_non_rare = Bayesian_Optimization(custom_reg_objfunc, pbounds) # COMPLETE\n",
    "best_params_reg_non_rare = {'learning_rate': 0.10180087793228383, 'max_depth': 3.240145102154466, 'min_child_weight': 46.87061090154842, 'min_split_gain': 0.008862420216352685, 'n_estimators': 592.576399719306, 'num_leaves': 48.80270066231908}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35ef275d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:24:44.457852Z",
     "iopub.status.busy": "2023-04-28T14:24:44.457241Z",
     "iopub.status.idle": "2023-04-28T14:24:44.467464Z",
     "shell.execute_reply": "2023-04-28T14:24:44.466508Z"
    },
    "papermill": {
     "duration": 0.023366,
     "end_time": "2023-04-28T14:24:44.469638",
     "exception": false,
     "start_time": "2023-04-28T14:24:44.446272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_reg_objfunc(num_leaves, learning_rate, n_estimators, max_depth, min_split_gain, min_child_weight):\n",
    "    \n",
    "    def train_and_eval(df):\n",
    "        cost = np.NaN\n",
    "        # Prepare data\n",
    "        features = features_impt_full\n",
    "        target = 'target'\n",
    "            \n",
    "        X, y = df[features], df[[target]]\n",
    "        \n",
    "        costs = np.sqrt(-cross_val_score(reg, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "        cost = costs.mean()\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(\n",
    "        num_leaves=int(num_leaves),\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=int(n_estimators),\n",
    "        max_depth=int(max_depth),\n",
    "        min_split_gain=min_split_gain,\n",
    "        min_child_weight=min_child_weight,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Arguments\n",
    "    df = df_train_merge\n",
    "    \n",
    "    cost = train_and_eval(df)\n",
    "    return -cost\n",
    "\n",
    "\n",
    "pbounds = {\n",
    "    'num_leaves': (5, 50),\n",
    "    'learning_rate': (0.01, 0.5),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3, 10),\n",
    "    'min_split_gain': (0.001, 0.1),\n",
    "    'min_child_weight': (5, 50)\n",
    "}\n",
    "\n",
    "# best_params_reg_full = Bayesian_Optimization(custom_reg_objfunc, pbounds) # COMPLETE\n",
    "best_params_reg_full = {'learning_rate': 0.01, 'max_depth': 10.0, 'min_child_weight': 32.80371387644972, 'min_split_gain': 0.001, 'n_estimators': 757.8955675847295, 'num_leaves': 23.642715185230898}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178894c",
   "metadata": {
    "papermill": {
     "duration": 0.009757,
     "end_time": "2023-04-28T14:24:44.489302",
     "exception": false,
     "start_time": "2023-04-28T14:24:44.479545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Train all regressors with their best hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05af3f92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:24:44.511302Z",
     "iopub.status.busy": "2023-04-28T14:24:44.510588Z",
     "iopub.status.idle": "2023-04-28T14:25:17.818532Z",
     "shell.execute_reply": "2023-04-28T14:25:17.817740Z"
    },
    "papermill": {
     "duration": 33.331229,
     "end_time": "2023-04-28T14:25:17.830379",
     "exception": false,
     "start_time": "2023-04-28T14:24:44.499150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(learning_rate=0.10180087793228383, max_depth=3,\n",
       "              min_child_weight=46.87061090154842,\n",
       "              min_split_gain=0.008862420216352685, n_estimators=592,\n",
       "              num_leaves=48, random_state=123)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_full = lgb.LGBMRegressor(\n",
    "    num_leaves=int(best_params_reg_full['num_leaves']),\n",
    "    learning_rate=best_params_reg_full['learning_rate'],\n",
    "    n_estimators=int(best_params_reg_full['n_estimators']),\n",
    "    max_depth=int(best_params_reg_full['max_depth']),\n",
    "    min_split_gain=best_params_reg_full['min_split_gain'],\n",
    "    min_child_weight=best_params_reg_full['min_child_weight'],\n",
    "    random_state=SEED\n",
    ")\n",
    "reg_rare = lgb.LGBMRegressor(\n",
    "    num_leaves=int(best_params_reg_rare['num_leaves']),\n",
    "    learning_rate=best_params_reg_rare['learning_rate'],\n",
    "    n_estimators=int(best_params_reg_rare['n_estimators']),\n",
    "    max_depth=int(best_params_reg_rare['max_depth']),\n",
    "    min_split_gain=best_params_reg_rare['min_split_gain'],\n",
    "    min_child_weight=best_params_reg_rare['min_child_weight'],\n",
    "    random_state=SEED\n",
    ")\n",
    "reg_non_rare = lgb.LGBMRegressor(\n",
    "    num_leaves=int(best_params_reg_non_rare['num_leaves']),\n",
    "    learning_rate=best_params_reg_non_rare['learning_rate'],\n",
    "    n_estimators=int(best_params_reg_non_rare['n_estimators']),\n",
    "    max_depth=int(best_params_reg_non_rare['max_depth']),\n",
    "    min_split_gain=best_params_reg_non_rare['min_split_gain'],\n",
    "    min_child_weight=best_params_reg_non_rare['min_child_weight'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "reg_full.fit(df_train_merge[features_impt_full], df_train_merge['target'])\n",
    "reg_rare.fit(df_outlier_more[features_impt_full], df_outlier_more['target'])\n",
    "reg_non_rare.fit(df_outlier_less[features_impt_full], df_outlier_less['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08bab1",
   "metadata": {
    "papermill": {
     "duration": 0.009791,
     "end_time": "2023-04-28T14:25:17.850071",
     "exception": false,
     "start_time": "2023-04-28T14:25:17.840280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Explore and HP Tune Meta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1493f9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:25:17.872097Z",
     "iopub.status.busy": "2023-04-28T14:25:17.871451Z",
     "iopub.status.idle": "2023-04-28T14:25:54.460792Z",
     "shell.execute_reply": "2023-04-28T14:25:54.459684Z"
    },
    "papermill": {
     "duration": 36.603684,
     "end_time": "2023-04-28T14:25:54.463727",
     "exception": false,
     "start_time": "2023-04-28T14:25:17.860043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "rare_prob_scaler = StandardScaler()\n",
    "overall_pred_scaler = StandardScaler()\n",
    "low_rare_pred_scaler = StandardScaler()\n",
    "high_rare_pred_scaler = StandardScaler()\n",
    "\n",
    "# Prepare Dataset\n",
    "rare_prob = bin_classifier.predict_proba(df_train_merge[features_impt_clf])[:, 1].reshape(-1,1)\n",
    "overall_pred = reg_full.predict(df_train_merge[features_impt_full]).reshape(-1,1)\n",
    "low_rare_pred = reg_rare.predict(df_train_merge[features_impt_full]).reshape(-1,1)\n",
    "high_rare_pred = reg_non_rare.predict(df_train_merge[features_impt_full]).reshape(-1,1)\n",
    "\n",
    "\n",
    "rare_prob_scaled = rare_prob_scaler.fit_transform(rare_prob).ravel()\n",
    "overall_pred_scaled = overall_pred_scaler.fit_transform(overall_pred).ravel()\n",
    "low_rare_pred_scaled = low_rare_pred_scaler.fit_transform(low_rare_pred).ravel()\n",
    "high_rare_pred_scaled = high_rare_pred_scaler.fit_transform(high_rare_pred).ravel()\n",
    "\n",
    "df_stack = pd.DataFrame({'low_conc_pred': low_rare_pred_scaled, \n",
    "                         'high_conc_pred': high_rare_pred_scaled, \n",
    "                         'overall_pred': overall_pred_scaled, \n",
    "                         'rare_prob': rare_prob_scaled,\n",
    "                         'target': df_train_merge['target']\n",
    "                        })\n",
    "features_stack = df_stack.drop(['target'], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efc7ff6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:25:54.486952Z",
     "iopub.status.busy": "2023-04-28T14:25:54.486316Z",
     "iopub.status.idle": "2023-04-28T14:37:02.671478Z",
     "shell.execute_reply": "2023-04-28T14:37:02.670490Z"
    },
    "papermill": {
     "duration": 668.199483,
     "end_time": "2023-04-28T14:37:02.673956",
     "exception": false,
     "start_time": "2023-04-28T14:25:54.474473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model LinearRegression\n",
      "Using model Ridge\n",
      "Using model Lasso\n",
      "Using model MLPRegressor\n",
      "Using model LGBMRegressor\n",
      "*** Decision: Choose MLP ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>3.521088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>3.521088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>3.666426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>3.481544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>3.541103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RMSE\n",
       "Linear  3.521088\n",
       "Ridge   3.521088\n",
       "Lasso   3.666426\n",
       "MLP     3.481544\n",
       "LGBM    3.541103"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Individual models\n",
    "reg_lr = LinearRegression()\n",
    "reg_ridge = Ridge(random_state=SEED)\n",
    "reg_lasso = Lasso(random_state=SEED)\n",
    "reg_mlp = MLPRegressor(random_state=SEED, max_iter=300)\n",
    "reg_gbm = lgb.LGBMRegressor()\n",
    "\n",
    "# Prepare variables\n",
    "regressors = [reg_lr, reg_ridge, reg_lasso, reg_mlp, reg_gbm]\n",
    "model_names = ['Linear', 'Ridge', 'Lasso', 'MLP', 'LGBM']\n",
    "dict_models = {reg_name: reg_object for reg_name, reg_object in zip(model_names, regressors)}\n",
    "\n",
    "df = df_stack  # Need to change to latest\n",
    "results = []\n",
    "\n",
    "for reg in regressors:\n",
    "    print(f\"Using model {reg.__class__.__name__}\")\n",
    "    cost = custom_reg_train(df, reg, features_stack)\n",
    "    results.append(cost)\n",
    "\n",
    "df_exp_reg = pd.DataFrame({'RMSE': results}, index=model_names)\n",
    "name_best_reg = df_exp_reg['RMSE'].idxmin()\n",
    "best_reg = dict_models[name_best_reg]\n",
    "print(f\"*** Decision: Choose {name_best_reg} ***\")\n",
    "df_exp_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f299a951",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:37:02.698137Z",
     "iopub.status.busy": "2023-04-28T14:37:02.697307Z",
     "iopub.status.idle": "2023-04-28T14:53:03.561874Z",
     "shell.execute_reply": "2023-04-28T14:53:03.560037Z"
    },
    "papermill": {
     "duration": 960.881558,
     "end_time": "2023-04-28T14:53:03.566299",
     "exception": false,
     "start_time": "2023-04-28T14:37:02.684741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | hidden... | learni... | max_iter  |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-3.494   \u001b[0m | \u001b[0m44.62    \u001b[0m | \u001b[0m0.007231 \u001b[0m | \u001b[0m1.001    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-3.502   \u001b[0m | \u001b[0m33.72    \u001b[0m | \u001b[0m0.001553 \u001b[0m | \u001b[0m1.831    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m-3.486   \u001b[0m | \u001b[95m22.69    \u001b[0m | \u001b[95m0.003521 \u001b[0m | \u001b[95m4.571    \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m-3.483   \u001b[0m | \u001b[95m56.19    \u001b[0m | \u001b[95m0.00425  \u001b[0m | \u001b[95m7.167    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-3.493   \u001b[0m | \u001b[0m24.42    \u001b[0m | \u001b[0m0.008793 \u001b[0m | \u001b[0m1.246    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-3.489   \u001b[0m | \u001b[0m68.69    \u001b[0m | \u001b[0m0.004231 \u001b[0m | \u001b[0m6.028    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-3.488   \u001b[0m | \u001b[0m18.34    \u001b[0m | \u001b[0m0.002061 \u001b[0m | \u001b[0m8.207    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-3.495   \u001b[0m | \u001b[0m96.98    \u001b[0m | \u001b[0m0.003203 \u001b[0m | \u001b[0m7.231    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-3.496   \u001b[0m | \u001b[0m88.26    \u001b[0m | \u001b[0m0.008957 \u001b[0m | \u001b[0m1.765    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-3.485   \u001b[0m | \u001b[0m8.71     \u001b[0m | \u001b[0m0.001781 \u001b[0m | \u001b[0m8.903    \u001b[0m |\n",
      "| \u001b[95m11       \u001b[0m | \u001b[95m-3.483   \u001b[0m | \u001b[95m56.16    \u001b[0m | \u001b[95m0.002231 \u001b[0m | \u001b[95m7.29     \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-3.508   \u001b[0m | \u001b[0m56.91    \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m8.928    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-3.484   \u001b[0m | \u001b[0m55.28    \u001b[0m | \u001b[0m0.006656 \u001b[0m | \u001b[0m7.418    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-3.488   \u001b[0m | \u001b[0m9.698    \u001b[0m | \u001b[0m0.006376 \u001b[0m | \u001b[0m8.221    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-3.486   \u001b[0m | \u001b[0m8.002    \u001b[0m | \u001b[0m0.007239 \u001b[0m | \u001b[0m7.799    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-3.488   \u001b[0m | \u001b[0m7.29     \u001b[0m | \u001b[0m0.004974 \u001b[0m | \u001b[0m8.966    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-3.488   \u001b[0m | \u001b[0m22.8     \u001b[0m | \u001b[0m0.001151 \u001b[0m | \u001b[0m5.932    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-3.513   \u001b[0m | \u001b[0m21.32    \u001b[0m | \u001b[0m0.0003701\u001b[0m | \u001b[0m4.851    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-3.483   \u001b[0m | \u001b[0m23.68    \u001b[0m | \u001b[0m0.00751  \u001b[0m | \u001b[0m4.682    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-3.488   \u001b[0m | \u001b[0m23.43    \u001b[0m | \u001b[0m0.002662 \u001b[0m | \u001b[0m3.561    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-3.485   \u001b[0m | \u001b[0m24.3     \u001b[0m | \u001b[0m0.006656 \u001b[0m | \u001b[0m5.807    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-3.486   \u001b[0m | \u001b[0m25.26    \u001b[0m | \u001b[0m0.003816 \u001b[0m | \u001b[0m4.706    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-3.488   \u001b[0m | \u001b[0m25.81    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.21     \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-3.489   \u001b[0m | \u001b[0m24.09    \u001b[0m | \u001b[0m0.007145 \u001b[0m | \u001b[0m7.327    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-3.495   \u001b[0m | \u001b[0m9.791    \u001b[0m | \u001b[0m0.0009458\u001b[0m | \u001b[0m9.923    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-3.507   \u001b[0m | \u001b[0m54.92    \u001b[0m | \u001b[0m0.0001416\u001b[0m | \u001b[0m6.002    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-3.486   \u001b[0m | \u001b[0m9.042    \u001b[0m | \u001b[0m0.007474 \u001b[0m | \u001b[0m6.837    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-3.518   \u001b[0m | \u001b[0m7.729    \u001b[0m | \u001b[0m0.000499 \u001b[0m | \u001b[0m6.249    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-3.487   \u001b[0m | \u001b[0m10.23    \u001b[0m | \u001b[0m0.005375 \u001b[0m | \u001b[0m7.052    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-3.486   \u001b[0m | \u001b[0m8.861    \u001b[0m | \u001b[0m0.006369 \u001b[0m | \u001b[0m7.793    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-3.486   \u001b[0m | \u001b[0m54.31    \u001b[0m | \u001b[0m0.009297 \u001b[0m | \u001b[0m8.451    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-3.487   \u001b[0m | \u001b[0m26.7     \u001b[0m | \u001b[0m0.008158 \u001b[0m | \u001b[0m4.894    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m-3.487   \u001b[0m | \u001b[0m26.4     \u001b[0m | \u001b[0m0.005486 \u001b[0m | \u001b[0m3.426    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-3.587   \u001b[0m | \u001b[0m27.92    \u001b[0m | \u001b[0m0.0001079\u001b[0m | \u001b[0m3.939    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m-3.484   \u001b[0m | \u001b[0m25.22    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m3.082    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m-3.487   \u001b[0m | \u001b[0m52.57    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m9.075    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m-3.485   \u001b[0m | \u001b[0m52.11    \u001b[0m | \u001b[0m0.004902 \u001b[0m | \u001b[0m7.44     \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m-3.483   \u001b[0m | \u001b[0m50.76    \u001b[0m | \u001b[0m0.002584 \u001b[0m | \u001b[0m8.475    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m-3.494   \u001b[0m | \u001b[0m50.44    \u001b[0m | \u001b[0m0.006856 \u001b[0m | \u001b[0m6.986    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m-3.484   \u001b[0m | \u001b[0m50.75    \u001b[0m | \u001b[0m0.004107 \u001b[0m | \u001b[0m9.965    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m-3.487   \u001b[0m | \u001b[0m49.16    \u001b[0m | \u001b[0m0.009896 \u001b[0m | \u001b[0m9.479    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m-3.483   \u001b[0m | \u001b[0m19.03    \u001b[0m | \u001b[0m0.00592  \u001b[0m | \u001b[0m9.94     \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m-3.49    \u001b[0m | \u001b[0m20.11    \u001b[0m | \u001b[0m0.005969 \u001b[0m | \u001b[0m8.914    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m-3.485   \u001b[0m | \u001b[0m17.33    \u001b[0m | \u001b[0m0.00475  \u001b[0m | \u001b[0m9.687    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m-3.488   \u001b[0m | \u001b[0m15.89    \u001b[0m | \u001b[0m0.004325 \u001b[0m | \u001b[0m8.36     \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m-3.486   \u001b[0m | \u001b[0m15.62    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m-3.487   \u001b[0m | \u001b[0m14.06    \u001b[0m | \u001b[0m0.009997 \u001b[0m | \u001b[0m9.283    \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m-3.485   \u001b[0m | \u001b[0m12.38    \u001b[0m | \u001b[0m0.00599  \u001b[0m | \u001b[0m8.04     \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m-3.488   \u001b[0m | \u001b[0m13.89    \u001b[0m | \u001b[0m0.007454 \u001b[0m | \u001b[0m7.532    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m-3.492   \u001b[0m | \u001b[0m12.26    \u001b[0m | \u001b[0m0.001462 \u001b[0m | \u001b[0m6.376    \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m-3.491   \u001b[0m | \u001b[0m12.33    \u001b[0m | \u001b[0m0.001033 \u001b[0m | \u001b[0m9.812    \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m-3.495   \u001b[0m | \u001b[0m26.14    \u001b[0m | \u001b[0m0.009673 \u001b[0m | \u001b[0m1.959    \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m-3.484   \u001b[0m | \u001b[0m53.83    \u001b[0m | \u001b[0m0.003975 \u001b[0m | \u001b[0m9.997    \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m-3.485   \u001b[0m | \u001b[0m16.96    \u001b[0m | \u001b[0m0.007194 \u001b[0m | \u001b[0m6.401    \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m-3.494   \u001b[0m | \u001b[0m15.49    \u001b[0m | \u001b[0m0.007367 \u001b[0m | \u001b[0m6.263    \u001b[0m |\n",
      "| \u001b[0m56       \u001b[0m | \u001b[0m-3.494   \u001b[0m | \u001b[0m18.45    \u001b[0m | \u001b[0m0.009057 \u001b[0m | \u001b[0m6.314    \u001b[0m |\n",
      "| \u001b[0m57       \u001b[0m | \u001b[0m-3.485   \u001b[0m | \u001b[0m16.97    \u001b[0m | \u001b[0m0.004751 \u001b[0m | \u001b[0m4.751    \u001b[0m |\n",
      "| \u001b[0m58       \u001b[0m | \u001b[0m-3.487   \u001b[0m | \u001b[0m16.93    \u001b[0m | \u001b[0m0.007608 \u001b[0m | \u001b[0m3.15     \u001b[0m |\n",
      "| \u001b[0m59       \u001b[0m | \u001b[0m-3.494   \u001b[0m | \u001b[0m15.57    \u001b[0m | \u001b[0m0.001791 \u001b[0m | \u001b[0m3.964    \u001b[0m |\n",
      "| \u001b[0m60       \u001b[0m | \u001b[0m-3.493   \u001b[0m | \u001b[0m18.23    \u001b[0m | \u001b[0m0.003151 \u001b[0m | \u001b[0m3.919    \u001b[0m |\n",
      "=============================================================\n",
      "#################################################################\n",
      "Found Network with Optimal target result of -3.482502492123129\n",
      "Parameters: {'hidden_layer_sizes': 56.157122692493864, 'learning_rate_init': 0.0022311977431408187, 'max_iter': 7.29013913059945}\n",
      "#################################################################\n"
     ]
    }
   ],
   "source": [
    "def custom_reg_objfunc(hidden_layer_sizes, max_iter, learning_rate_init):\n",
    "    \n",
    "    def train_and_eval(df):\n",
    "        cost = np.NaN\n",
    "        \n",
    "        # Prepare data\n",
    "        features = list(df.drop(columns=['target']).columns)\n",
    "        target = 'target'\n",
    "        \n",
    "        X, y = df[features], df[[target]]\n",
    "        costs = np.sqrt(-cross_val_score(reg, X, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "        cost = costs.mean()\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    reg = MLPRegressor(hidden_layer_sizes=int(hidden_layer_sizes), \n",
    "                       max_iter=int(max_iter), \n",
    "                       learning_rate_init=learning_rate_init, \n",
    "                       random_state=SEED)\n",
    "\n",
    "    # Arguments\n",
    "    df = df_stack\n",
    "    \n",
    "    cost = train_and_eval(df)\n",
    "    return -cost\n",
    "\n",
    "\n",
    "pbounds = {\n",
    "    'hidden_layer_sizes': (5, 100),\n",
    "    'max_iter': (1, 10),\n",
    "    'learning_rate_init': (0.0001, 0.01)\n",
    "}\n",
    "\n",
    "# best_params_reg_meta = Bayesian_Optimization(custom_reg_objfunc, pbounds, n_init_random_explorations=10, n_iter=50) # COMPLETE\n",
    "best_params_reg_meta = {'hidden_layer_sizes': 37.17855696044765, 'learning_rate_init': 0.002646738138804039, 'max_iter': 9.9545564395736}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2e1c948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:53:03.647144Z",
     "iopub.status.busy": "2023-04-28T14:53:03.646389Z",
     "iopub.status.idle": "2023-04-28T14:53:12.377957Z",
     "shell.execute_reply": "2023-04-28T14:53:12.376445Z"
    },
    "papermill": {
     "duration": 8.776937,
     "end_time": "2023-04-28T14:53:12.382071",
     "exception": false,
     "start_time": "2023-04-28T14:53:03.605134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=56, learning_rate_init=0.0022311977431408187,\n",
       "             max_iter=7, random_state=123)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_meta = MLPRegressor(hidden_layer_sizes=int(best_params_reg_meta['hidden_layer_sizes']), \n",
    "                        max_iter=int(best_params_reg_meta['max_iter']), \n",
    "                        learning_rate_init=best_params_reg_meta['learning_rate_init'], \n",
    "                        random_state=SEED)\n",
    "reg_meta.fit(df_stack[features_stack], df_stack['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea2c9e6",
   "metadata": {
    "papermill": {
     "duration": 0.01403,
     "end_time": "2023-04-28T14:53:12.514818",
     "exception": false,
     "start_time": "2023-04-28T14:53:12.500788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate Output for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51783fea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:53:12.460037Z",
     "iopub.status.busy": "2023-04-28T14:53:12.459324Z",
     "iopub.status.idle": "2023-04-28T14:53:12.477277Z",
     "shell.execute_reply": "2023-04-28T14:53:12.475702Z"
    },
    "papermill": {
     "duration": 0.062171,
     "end_time": "2023-04-28T14:53:12.482073",
     "exception": false,
     "start_time": "2023-04-28T14:53:12.419902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_for_production(dataset):\n",
    "    features = dataset.drop(columns=['card_id']).columns\n",
    "    \n",
    "    rare_prob = bin_classifier.predict_proba(dataset[features_impt_clf])[:, 1].reshape(-1,1)\n",
    "    overall_pred = reg_full.predict(dataset[features_impt_full]).reshape(-1,1)\n",
    "    low_rare_pred = reg_rare.predict(dataset[features_impt_full]).reshape(-1,1)\n",
    "    high_rare_pred = reg_non_rare.predict(dataset[features_impt_full]).reshape(-1,1)\n",
    "\n",
    "\n",
    "    rare_prob_scaled = rare_prob_scaler.transform(rare_prob).ravel()\n",
    "    overall_pred_scaled = overall_pred_scaler.transform(overall_pred).ravel()\n",
    "    low_rare_pred_scaled = low_rare_pred_scaler.transform(low_rare_pred).ravel()\n",
    "    high_rare_pred_scaled = high_rare_pred_scaler.transform(high_rare_pred).ravel()\n",
    "\n",
    "    df_stack = pd.DataFrame({'low_conc_pred': low_rare_pred_scaled, \n",
    "                             'high_conc_pred': high_rare_pred_scaled, \n",
    "                             'overall_pred': overall_pred_scaled, \n",
    "                             'rare_prob': rare_prob_scaled\n",
    "                            })\n",
    "    return reg_meta.predict(df_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7894404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T14:53:12.546216Z",
     "iopub.status.busy": "2023-04-28T14:53:12.544887Z",
     "iopub.status.idle": "2023-04-28T14:53:37.133590Z",
     "shell.execute_reply": "2023-04-28T14:53:37.132780Z"
    },
    "papermill": {
     "duration": 24.606663,
     "end_time": "2023-04-28T14:53:37.135896",
     "exception": false,
     "start_time": "2023-04-28T14:53:12.529233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare Test Dataset\n",
    "df_test_merge = pd.read_csv(f'{INPUT_PREPROCESSED_DIR}/test_merged.csv', index_col=0)\n",
    "df_test_merge.drop(['first_active_month'], axis=1, inplace=True)\n",
    "\n",
    "card_id = df_test_merge.card_id.unique()\n",
    "\n",
    "# Predict\n",
    "y_test_predict = model_for_production(df_test_merge)\n",
    "df_test_predict = pd.DataFrame({'card_id': card_id, 'target': y_test_predict})\n",
    "\n",
    "# Save to csv\n",
    "df_test_predict.to_csv('best_of_custom_model.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2255.638575,
   "end_time": "2023-04-28T14:53:39.879979",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-28T14:16:04.241404",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
